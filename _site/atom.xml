<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mozillo Blog</title>
    <description>A blog about cognitive science, machine learning, and data science</description>
    <link></link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
    <author>
      <name>mozillo</name>
      <email>motwu412@gmail.com</email>
      <uri>http://mozillo.github.io/</uri>
    </author>
    
      <item>
        <title>Post Doctoral Fellowships Opportunities in Data Science</title>
        <description>&lt;p&gt;Since I’m on the search for new opportunities in data science for my life post PhD, I decided to compile a &lt;strong&gt;list of postdoctoral fellowships&lt;/strong&gt; and share it with everyone.&lt;/p&gt;

&lt;p&gt;The current COVID-19 situation has triggered a hiring freeze on many institutions around the world. Yet, some positions are still available and some may reopen next year, meaning I’ll be adding both currently available positions and positions that may reopen next year.&lt;/p&gt;

&lt;p&gt;If you want me to &lt;em&gt;add a new postdoctoral opportunity&lt;/em&gt; &lt;strong&gt;anywhere in the world!&lt;/strong&gt;, please fill out the form below the table with your message.&lt;/p&gt;

&lt;iframe src=&quot;https://docs.google.com/spreadsheets/d/e/2PACX-1vT0oKNA910VQvSzLtk1Za9K3xLRNFO6Jy1S-yl-grKWoTvlibR1hqHg3VtB89Bfu0r8lzSF0MsS9UdR/pubhtml?widget=true&amp;amp;headers=false&quot; width=&quot;100%&quot; height=&quot;300&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;template-to-add-a-postdoc-to-the-database&quot;&gt;Template to add a postdoc to the database&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Position:
Institution:
City: 
Country:
Link:
Date posted:
Current state:
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;style&gt;
input[type=&quot;text&quot;], input[type=&quot;email&quot;], input[type=&quot;search&quot;], 
input[type=&quot;submit&quot;], button, textarea { 
  padding: 1em 1.5em; 
  border: 1px solid #e5e5e5; 
  border-radius: 30px; 
  margin-bottom: 1em; 
  font-family:  -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, 
                &quot;Roboto&quot;, &quot;Oxygen&quot;, &quot;Ubuntu&quot;, &quot;Cantarell&quot;, 
                &quot;Fira Sans&quot;, &quot;Droid Sans&quot;, &quot;Helvetica Neue&quot;, 
                Arial, sans-serif; 
}

textarea { width: 90%;  resize: none; }
&lt;/style&gt;

&lt;form id=&quot;fs-frm&quot; name=&quot;simple-contact-form&quot; accept-charset=&quot;utf-8&quot; action=&quot;https://formspree.io/xknqyrjn&quot; method=&quot;post&quot;&gt;
  &lt;fieldset id=&quot;fs-frm-inputs&quot;&gt;
    &lt;label for=&quot;full-name&quot;&gt;Full Name&lt;/label&gt;
    &lt;input type=&quot;text&quot; name=&quot;name&quot; id=&quot;full-name&quot; placeholder=&quot;First and Last&quot; required=&quot;&quot; /&gt;
    &lt;label for=&quot;email-address&quot;&gt;Email Address&lt;/label&gt;
    &lt;input type=&quot;email&quot; name=&quot;_replyto&quot; id=&quot;email-address&quot; placeholder=&quot;youremail@domain.com&quot; required=&quot;&quot; /&gt;
    &lt;br /&gt;
    &lt;label for=&quot;message&quot;&gt;Message&lt;/label&gt;
    &lt;br /&gt;
    &lt;textarea rows=&quot;6&quot; name=&quot;message&quot; id=&quot;message&quot; placeholder=&quot;Please use the template above add a postdoc to the database&quot; required=&quot;&quot;&gt;&lt;/textarea&gt;
    &lt;input type=&quot;hidden&quot; name=&quot;_subject&quot; id=&quot;email-subject&quot; value=&quot;Contact Form Submission&quot; /&gt;
  &lt;/fieldset&gt;
  &lt;input type=&quot;submit&quot; value=&quot;Submit&quot; /&gt;
&lt;/form&gt;
</description>
        <pubDate>Sat, 01 Aug 2020 00:00:00 +0800</pubDate>
        <link>//postdocs-ds</link>
        <link href="/postdocs-ds"/>
        <guid isPermaLink="true">/postdocs-ds</guid>
      </item>
    
      <item>
        <title>NumPy Fundamentals for Data Science and Machine Learning</title>
        <description>&lt;hr /&gt;

&lt;iframe src=&quot;https://github.com/sponsors/pabloinsente/card&quot; title=&quot;Sponsor pabloinsente&quot; height=&quot;225&quot; width=&quot;600&quot; style=&quot;border: 0;&quot;&gt;&lt;/iframe&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Note&lt;/em&gt;&lt;/strong&gt;: If you prefer to read with a &lt;strong&gt;white background and black font&lt;/strong&gt;, you can see this article in GitHub &lt;a href=&quot;https://github.com/pabloinsente/intro-sc-python/blob/master/notebooks/intro-numpy-fundamentals.ipynb&quot;&gt;here&lt;/a&gt;. Las time I check SVG images rendered just fine.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;It is no exaggeration to say that &lt;strong&gt;NumPy is at the core of the entire scientific computing Python ecosystem&lt;/strong&gt;, both as a standalone package for numerical computation and as the engine behind most data science packages.&lt;/p&gt;

&lt;p&gt;In this document, I review &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; main components and functionality, with attention to the needs of Data Science and Machine Learning practitioners, and people who aspire to become a data professional. My only assumption is that you have basic familiarity with Python, things like variables, lists, tuples, and loops. Advance Python concepts like Object Oriented Programming are not touched at all.&lt;/p&gt;

&lt;p&gt;The resources I used to build this tutorial are three:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; documentation&lt;/li&gt;
  &lt;li&gt;A few miscellaneous articles from the Internet&lt;/li&gt;
  &lt;li&gt;My own experience with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Content-wise, I’ll say that ~95% is based on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; v1.18 manual, in particular:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://numpy.org/doc/stable/user/index.html&quot;&gt;NumPy user guide&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://numpy.org/doc/stable/reference/index.html&quot;&gt;NumPy reference&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://numpy.org/doc/stable/about.html&quot;&gt;NumPy about&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The rest ~5% comes from a couple of random articles on the Internet and Stack Overflow. I resort to those sources mostly to clarify concepts and functionality that wasn’t clear for me from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; documentation.&lt;/p&gt;

&lt;p&gt;My own experience was the base to organize the tutorial, explain concepts, create practical examples, create images, etc.&lt;/p&gt;

&lt;p&gt;“&lt;em&gt;Why are you using the documentation as the main source of content, instead of the many great tutorials online?&lt;/em&gt;” Because it is the most up-to-date, complete, and reliable source about &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; (and about any library for that matter).&lt;/p&gt;

&lt;p&gt;“&lt;em&gt;Why then I should read this if everything comes from the documentation?&lt;/em&gt;” Well, you don’t need to read this, you are right. Actually, I encourage you to read the documentation and learn from there. What I can offer is my own: &lt;strong&gt;(1) organization of contents, (2) selection of contents, (3) explanations and framing of concepts, (4) images, (5) practical examples, (6) and general perspective.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This tutorial is part of a larger project I am working on, which is an introduction to Python and its libraries for scientific computing, data science, and machine learning that you can find &lt;a href=&quot;https://github.com/pabloinsente/intro-sc-python&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you want to interact with this Notebook, you can open a MyBinder interactive instance by clicking in the MyBinder icon:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To open MyBinder&lt;/strong&gt; -&amp;gt; &lt;a href=&quot;https://mybinder.org/v2/gh/pabloinsente/intro-sc-python/master/?urlpath=lab&quot;&gt;&lt;img src=&quot;https://mybinder.org/badge_logo.svg&quot; alt=&quot;Binder&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As a final note, &lt;strong&gt;if you are &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; expert, advanced user, or developer&lt;/strong&gt;, you may find some inaccuracies or lack of depth in some of my explanations. Two things: (1) feel free to suggest a better explanation or something that I may add to make things clearer, (2) I prioritize conciseness and accessibility over the accuracy, so the lack of accuracy or depth sometimes it is intentional from my part.&lt;/p&gt;

&lt;p&gt;If you have any questions or suggestion feel free to reach me out to at pcaceres@wisc.edu
Here is my &lt;a href=&quot;https://twitter.com/CodeBug88&quot;&gt;Twitter&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/pabloascm/&quot;&gt;LinkedIn&lt;/a&gt;, and &lt;a href=&quot;https://pablocaceres.org/&quot;&gt;personal site&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;table-of-contents&quot;&gt;Table of contents&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#python-is-slow&quot;&gt;Python is slow&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#what-is-numpy&quot;&gt;What is NumPy&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#installing-numpy&quot;&gt;Installing NumPy&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#numpy-arrays&quot;&gt;NumPy Arrays&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#array-creation&quot;&gt;Array creation&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#conversion-from-other-python-structures&quot;&gt;Conversion from other Python structures&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#intrinsic-numpy-array-creation-objects&quot;&gt;Intrinsic NumPy array creation objects&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#use-of-special-library-functions&quot;&gt;Use of special library functions&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#vectorization&quot;&gt;Vectorization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#array-data-type-and-conversions&quot;&gt;Array data type and conversions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#array-mathematics-and-element-wise-operations&quot;&gt;Array mathematics and element-wise operations&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#array-arithmetic&quot;&gt;Array arithmetic&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#trigonometric-functions&quot;&gt;Trigonometric functions&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#hyperbolic-functions&quot;&gt;Hyperbolic functions&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#rounding&quot;&gt;Rounding&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#other-miscellaneous-element-wise-operations&quot;&gt;Other miscellaneous element-wise operations&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#array-data-type-and-conversions&quot;&gt;Array data type and conversions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#array-manipulation&quot;&gt;Array manipulation&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#array-shape-manipulation&quot;&gt;Array shape manipulation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#array-transpose-like-operations&quot;&gt;Array transpose-like operations&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#array-dimension-manipulation&quot;&gt;Array dimension manipulation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#array-broadcasting&quot;&gt;Array broadcasting&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#joining-arrays&quot;&gt;Joining arrays&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#splitting-arrays&quot;&gt;Splitting arrays&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#array-repetition&quot;&gt;Array repetition&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#adding-and-removing-array-elements&quot;&gt;Adding and removing array elements&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#rearranging-array-elements&quot;&gt;Rearranging array elements&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#logic-functions-and-array-evaluation&quot;&gt;Logic functions and array evaluation&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#boolean-testing&quot;&gt;Boolean testing&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#array-elements-testing&quot;&gt;Array elements testing&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#array-type-testing&quot;&gt;Array type testing&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#logical-operators&quot;&gt;Logical operators&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#comparison-operators&quot;&gt;Comparison operators&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#array-indexing&quot;&gt;Array Indexing&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#basic-indexing-in-one-dimensional-arrays&quot;&gt;Basic indexing in one-dimensional arrays&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#slicing-one-dimensional-arrays&quot;&gt;Slicing one-dimensional arrays&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#basic-indexing-in-multidimensional-arrays&quot;&gt;Basic indexing in multidimensional arrays&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#indexing-like-numpy-functions&quot;&gt;Indexing-like NumPy functions&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#boolean-or-mask-indexing&quot;&gt;Boolean or Mask indexing&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#indexing-like-numpy-operations&quot;&gt;Indexing-like NumPy operations&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#array-iteration&quot;&gt;Array iteration&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#basic-array-iteration&quot;&gt;Basic array iteration&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#broadcasting-array-iteration&quot;&gt;Broadcasting array iteration&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#allocating-outputs-from-iteration&quot;&gt;Allocating outputs from iteration&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#iteration-functions&quot;&gt;Iteration functions&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#array-shallow-and-deep-copies&quot;&gt;Array shallow and deep copies&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#array-new-label&quot;&gt;Array new label&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#array-shallow-copy-or-view&quot;&gt;Array shallow copy or view&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#array-deep-copy&quot;&gt;Array deep copy&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#structured-arrays&quot;&gt;Structured arrays&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#random-number-generation-and-sampling-with-numpy&quot;&gt;Random number generation and sampling with NumPy&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#random-sampling-updated&quot;&gt;Random sampling updated&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#basic-random-sampling&quot;&gt;Basic random sampling&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#setting-a-seed-for-reproducibility&quot;&gt;Setting a seed for reproducibility&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#sampling-from-particular-distributions&quot;&gt;Sampling from particular distributions&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#basic-statistics-with-numpy&quot;&gt;Basic statistics with NumPy&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#measures-of-central-tendency&quot;&gt;Measures of central tendency&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#measures-of-dispersion&quot;&gt;Measures of dispersion&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#measures-of-correlation&quot;&gt;Measures of correlation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#histograms&quot;&gt;Histograms&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#basic-linear-algebra-with-numpy&quot;&gt;Basic linear algebra with NumPy&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#basic-vector-operations&quot;&gt;Basic vector operations&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#basic-vector-operations&quot;&gt;Basic vector operations&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#basic-matrix-operations&quot;&gt;Basic matrix operations&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#eigendecomposition&quot;&gt;Eigendecomposition&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#singular-value-decomposition&quot;&gt;Singular value decomposition&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#string-operations-with-numpy&quot;&gt;String operations with NumPy&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#basic-string-manipulation&quot;&gt;Basic string manipulation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#basic-string-comparison&quot;&gt;Basic string comparison&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#string-information&quot;&gt;String information&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;python-is-slow&quot;&gt;Python is slow&lt;/h2&gt;

&lt;p&gt;Scientific and numerical computing often requires processing massive datasets with complex algorithms. If you are a scientist or data professional, you want a programming language than can process data FAST. &lt;strong&gt;The closer a programming language is to machine instructions (binary), the faster it runs&lt;/strong&gt;. That’s why for decades, programming languages like C, C++, and Fortran, were the to-go option for data-intensive applications in science and technology.&lt;/p&gt;

&lt;p&gt;However, writing code in compiled languages like C++ and Fortran can be slow and, frankly, terribly annoying (but that’s just me!). Scientists are not coders (although many end up mutating into scientist-coder hybrids over time, that’s a whole other subject). From a scientist’s perspective, &lt;strong&gt;the easier and faster to write code, the better&lt;/strong&gt;. High-level programming languages, i.e., languages that are closer to human language rather than to machine language do meet such requirements. The likes of Python, PHP, JavaScript, and Ruby, fits the bill: &lt;strong&gt;easy to write, easy to learn, easy to run&lt;/strong&gt;. But, instructions written in high-level programming languages are slow to run by computers, for complicated reasons I do not explore here. The fact is that they are slower to run than C++ or Fortran.&lt;/p&gt;

&lt;p&gt;Scientists face a conundrum: they need an &lt;strong&gt;easy to write&lt;/strong&gt; AND &lt;strong&gt;fast to run&lt;/strong&gt; programming language. They need the best of both worlds. For a long time, such language simply did not exist. Then Python came along.&lt;/p&gt;

&lt;p&gt;Oh wait, Python it is, indeed, easy to learn and write, &lt;em&gt;but slow to run compared to compiled languages&lt;/em&gt;. Like really, really slow. For instance, computing the &lt;a href=&quot;https://en.wikipedia.org/wiki/Matrix_norm&quot;&gt;spectral-norm of a matrix&lt;/a&gt;, which is a common task in data applications, &lt;a href=&quot;https://benchmarksgame-team.pages.debian.net/benchmarksgame/performance/spectralnorm.html&quot;&gt;has been benchmarked&lt;/a&gt; at ~1.9 seconds in C, whereas python takes a whopping ~170.1 seconds, meaning that Python is ~90 times &lt;em&gt;slower&lt;/em&gt;. Considering this, the fact Python became the dominant language in machine learning and data science is a bit of a puzzle.&lt;/p&gt;

&lt;p&gt;There are at least two reasons for this why Python succeed anyways.&lt;/p&gt;

&lt;p&gt;The first is that as computing capacity became cheaper, &lt;strong&gt;processing time has become less important than coding-time&lt;/strong&gt;. Basically, the time you save by writing code in high-level yet slow to-run programming languages compensates by their lack of performance at run time. And Python is exceptionally clean, intuitive, and easy to learn, compared to C or Fortran.&lt;/p&gt;

&lt;p&gt;The second reason, which is probably the major one, is &lt;strong&gt;libraries&lt;/strong&gt;. Particularly libraries written in low-level high-performant languages. Turns out that Python extensibility allows programmers to write the “engine” for numerical computation in languages like C and Fortran, and then Python can invoke such “engines” in the background, meaning that although you write code in Python, it is executed in compiled C or Fortran code instead of Python itself. And that is how you obtain the best of both worlds: &lt;strong&gt;the easy and fast developing time of Python, plus the runtime performance of C or Fortran&lt;/strong&gt;. True, there is some small overhead of going back and forth between languages, but its impact it’s minimal.&lt;/p&gt;

&lt;p&gt;It is important to mention that nowadays, new programming languages have been created exactly with these two necessities (fast development + fast performance) in mind, like &lt;strong&gt;&lt;a href=&quot;https://julialang.org/&quot;&gt;Julia&lt;/a&gt;&lt;/strong&gt;. In the spectral-norm test, Julia was benchmarked at ~2.79 seconds, almost as fast as C. Additionally, Julia is a dynamic language, easy to learn, and write in like Python. Why not Julia then? Probably because of the relative immaturity of its packages ecosystem compared to Python. Julia first appeared in 2012, whereas Python was introduced in 1990. The availability of well-tested libraries for pretty much anything you can imagine in Python is unparalleled. A second reason is probably that Python does not cost you performance anyways (with its libraries), so Why not?&lt;/p&gt;

&lt;h2 id=&quot;what-is-numpy&quot;&gt;What is NumPy&lt;/h2&gt;

&lt;p&gt;It is no exaggeration to say that &lt;strong&gt;NumPy is at the core of the entire scientific computing Python ecosystem&lt;/strong&gt;, both as a standalone package for numerical computation and as the engine behind most data science packages.&lt;/p&gt;

&lt;p&gt;NumPy is a package for array-like or matrix-like high-performance computation. Its “engine” is written in C, meaning that NumPy utilized “in the background” pre-compiled C code to perform computations.&lt;/p&gt;

&lt;h2 id=&quot;installing-numpy&quot;&gt;Installing NumPy&lt;/h2&gt;

&lt;p&gt;If you are running this Notebook in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MyBinder&lt;/code&gt; or locally after running the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip install -r requirements.txt&lt;/code&gt; file, you have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; installed already. Otherwise, you will need to install &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; with one of these options:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Scientific Python Distributions&lt;/li&gt;
  &lt;li&gt;pip&lt;/li&gt;
  &lt;li&gt;System-wide installation via a package manager (apt, brew, etc.)&lt;/li&gt;
  &lt;li&gt;From NumPy source code&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Option 4 is for developers who need to alter source code. Option 3 is not recommended as a system-wide installation of packages may generate dependency conflicts.&lt;/p&gt;

&lt;p&gt;Option 1 is probably the simplest and it’s widely used by practitioners. Within this category we have:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.anaconda.com/products/individual&quot;&gt;Anaconda&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.conda.io/en/latest/miniconda.html&quot;&gt;Mini-conda&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://winpython.github.io/&quot;&gt;WinPython&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pyzo.org/&quot;&gt;Pyzo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;WinPython and Pyzo are less used and I do not have any experience with them. Feel free to experiment with them at your own risk. Anaconda and mini-conda are the most popular options. Anaconda basically is a large bundle of packages for Python and R, and a package manager. Mini-conda is a lightweight version of Anaconda.  Once you install Anaconda or mini-conda, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; will be available within the conda installation.&lt;/p&gt;

&lt;p&gt;I do not like and do not use Anaconda or mini-conda. I just see no reason to install hundreds of libraries I most likely never use. I also see no reason to duplicate functionality which is already provided in the standard Python installation. It just occupies memory and you also need to learn how to use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda&lt;/code&gt;, which sometimes introduces hard to understand issues with multiple Python and packages installations. Yet, many people recommend and use this method. If you decide to use it, go to the &lt;a href=&quot;https://www.anaconda.com/products/individual&quot;&gt;Anaconda&lt;/a&gt; or &lt;a href=&quot;https://docs.conda.io/en/latest/miniconda.html&quot;&gt;Mini-conda&lt;/a&gt; site and follow the instructions for your system.&lt;/p&gt;

&lt;p&gt;My preferred method is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip&lt;/code&gt;, which is available out-of-the-box with your Python installation. To install &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; is as simple as to run:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip3 &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;numpy
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, I highly recommend to create a virtual environment, activate the environment, and then install &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; within that. It boils down to copy-pasting the following in your terminal:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# create the virtual environment&lt;/span&gt;
python3 &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; venv venv

&lt;span class=&quot;c&quot;&gt;# activate the virtual environment&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;source &lt;/span&gt;venv/bin/activate

&lt;span class=&quot;c&quot;&gt;# upgrade pip package manager&lt;/span&gt;
pip3 &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--upgrade&lt;/span&gt; pip

&lt;span class=&quot;c&quot;&gt;#install numpy&lt;/span&gt;
pip3 &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;numpy
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The virtual environment will isolate your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; installation from your system-wide Python installation and other projects you may have in your computer. So, it’s safer. This method will save you gigabytes of memory, time, confusion, and effort. But that’s just me!&lt;/p&gt;

&lt;h2 id=&quot;numpy-arrays&quot;&gt;NumPy arrays&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; fundamental object is the &lt;strong&gt;&lt;a href=&quot;https://numpy.org/doc/1.18/reference/generated/numpy.ndarray.html&quot;&gt;ndarray&lt;/a&gt;&lt;/strong&gt;. Arrays are simply ordered collections of elements, like single numbers, &lt;a href=&quot;https://docs.python.org/3/tutorial/datastructures.html&quot;&gt;lists&lt;/a&gt;, &lt;a href=&quot;https://realpython.com/python-sets/&quot;&gt;sets&lt;/a&gt;, &lt;a href=&quot;https://pabloinsente.github.io/intro-linear-algebra#vectors&quot;&gt;vectors&lt;/a&gt;, &lt;a href=&quot;https://pabloinsente.github.io/intro-linear-algebra#matrices&quot;&gt;matrices&lt;/a&gt;, or &lt;a href=&quot;https://en.wikipedia.org/wiki/Tensor&quot;&gt;tensors&lt;/a&gt;. In Additionally, elements in an array have of &lt;strong&gt;the same type&lt;/strong&gt;. For instance, an array can’t have integers and text at the same time. The reason is simple: mathematical operations with objects containing multiple data types would be slow, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; main goal is fast and efficient numerical computation.&lt;/p&gt;

&lt;p&gt;The “n” in “ndarray” makes references to the arbitrary number of dimensions it can take. An array with one element and one dimension, it’s a “singleton” or just a number. An array with four elements and two dimensions is a 2x2 matrix. Put simply, an array is like an Excel sheet with the caveat that instead of being restricted to two dimensions, it can be extended to 3, 4, or higher dimensions, and that you can’t combine data types in a “sheet”.&lt;/p&gt;

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;, dimensions are called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;axes&lt;/code&gt;, so I will use such term interchangeably with dimensions from now.&lt;/p&gt;

&lt;p&gt;Let’s see a few examples.&lt;/p&gt;

&lt;p&gt;We first need to import &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; by running:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then we can use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;array&lt;/code&gt; method constructor to build an array as:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# 1 axis/dimensions array
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;one_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; 

&lt;span class=&quot;c1&quot;&gt;# 2 axis/dimensions array
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;two_dim_1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt; 

&lt;span class=&quot;c1&quot;&gt;# 2 axis/dimensions array
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;two_dim_2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                     &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
 
&lt;span class=&quot;c1&quot;&gt;# 3 axis/dimensions array
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;three_dim&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; 
                        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt;
                       &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]])&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Visually, we can represent the above arrays as:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-12/numpy-array.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is how arrays look when printed:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;One-dimensional array with 3 elements:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;one_dim&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Two-dimensional array with 1 row and 3 cols:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;two_dim_1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Two-dimensional array with 2 row and 3 cols:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;two_dim_2&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Three-dimensional array:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;three_dim&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;One-dimensional array with 3 elements:
[1 2 3]

Two-dimensional array with 1 row and 3 cols:
[[1 2 3]]

Two-dimensional array with 2 row and 3 cols:
[[1 2 3]
 [4 5 6]]

Three-dimensional array:
[[[1 2 3]
  [4 5 6]]

 [[1 2 3]
  [4 5 6]]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can inspect and confirm &lt;strong&gt;dimensionality&lt;/strong&gt; as:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Number of dimensions array one: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;one_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndim&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Number of dimensions array two-1: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;two_dim_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndim&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Number of dimensions array two-2: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;two_dim_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndim&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Number of dimensions array three: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;three_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndim&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Number of dimensions array one: 1
Number of dimensions array two-1: 2
Number of dimensions array two-2: 2
Number of dimensions array three: 3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;strong&gt;shape&lt;/strong&gt; of an array must not be confused with its dimensionality, as shape reflects the number of elements along each axis, and dimensionality only the number of axes or dimensions.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Shape array one: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;one_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Shape array two-1: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;two_dim_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Shape array two-2: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;two_dim_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Shape array three: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;three_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Shape array one: (3,)
Shape array two-1: (1, 3)
Shape array two-2: (2, 3)
Shape array three: (2, 2, 3)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The first number in the parenthesis represents the number of elements within the first axis/dimension; the second number the number of elements within the second axis/dimension, the third number the number of elements within the third axis/dimensions, and so on.&lt;/p&gt;

&lt;p&gt;For instance, the (2, 2, 3) indicates 2 elements along the first axis, 2 elements along the second axis, and 3 elements along the third axis.&lt;/p&gt;

&lt;p&gt;To count the &lt;strong&gt;number of elements&lt;/strong&gt; within an array type:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Number of elements array one:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;one_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Number of elements array two-1:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;two_dim_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Number of elements array two-2:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;two_dim_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Number of elements array three:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;three_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Number of elements array one:3
Number of elements array two-1:3
Number of elements array two-2:6
Number of elements array three:12
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; utilizes different &lt;strong&gt;data types&lt;/strong&gt; (more on this later) to represent data, which can be inspected as:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Data type array one:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;one_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Data type array two-1:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;two_dim_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Data type array two-2:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;two_dim_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Data type array three:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;three_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Data type array one:int64
Data type array two-1:int64
Data type array two-2:int64
Data type array three:int64
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;array-creation&quot;&gt;Array creation&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; offers several alternatives to create arrays.&lt;/p&gt;

&lt;p&gt;I will review three cases:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Conversion from other Python structures&lt;/li&gt;
  &lt;li&gt;Intrinsic &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; array creation objects&lt;/li&gt;
  &lt;li&gt;Use of special library functions&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;conversion-from-other-python-structures&quot;&gt;Conversion from other Python structures&lt;/h3&gt;

&lt;p&gt;In the previous section, I used the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;array&lt;/code&gt; method to create an array from a Python list. This is an example of array creation from the conversion of an array-like Python object.&lt;/p&gt;

&lt;p&gt;Lists, tuples, and sets are array-like Python objects that serve as options for this method.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;array_list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;array_tuple&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;array_set&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;pikachu&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;snorlax&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;charizard&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array from list:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_list&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array from tuple:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_tuple&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array from set:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_set&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array from list:
[1 2 3]

Array from tuple:
[[1 2 3]
 [4 5 6]]

Array from set:
{&apos;pikachu&apos;, &apos;snorlax&apos;, &apos;charizard&apos;}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;intrinsic-numpy-array-creation-objects&quot;&gt;Intrinsic NumPy array creation objects&lt;/h3&gt;

&lt;p&gt;Manual input of data into arrays can be cumbersome, so &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; offers a series of convenience methods to create arrays for special cases, like zeros, ones, and others. Below some common examples.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# zeros
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# ones
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# arange
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# empty
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;empty&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# linspace
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linespace&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# full
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;full&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;full&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# indices
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array of zeros:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array of ones:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array of empty entries:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Evenly spaced array in a range:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linespace&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array with same number on each entry:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;full&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array from indices:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array of zeros:
[0. 0. 0. 0. 0.]

Array of ones:
[[1. 1. 1.]
 [1. 1. 1.]
 [1. 1. 1.]]

Array of empty entries:
[[4.67794427e-310 6.90921830e-310]
 [0.00000000e+000 0.00000000e+000]]

Evenly spaced array in a range:
[-1.         -0.77777778 -0.55555556 -0.33333333 -0.11111111  0.11111111
  0.33333333  0.55555556  0.77777778  1.        ]

Array with same number on each entry:
[[-2 -2 -2]
 [-2 -2 -2]
 [-2 -2 -2]]

Array from indices:
[[[0 0 0]
  [1 1 1]
  [2 2 2]]

 [[0 1 2]
  [0 1 2]
  [0 1 2]]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zero&lt;/code&gt; method generates an array of zeros of shape defined by a tuple passed to the function&lt;/li&gt;
  &lt;li&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ones&lt;/code&gt; method generates an array of ones of shape defined by a tuple passed to the function&lt;/li&gt;
  &lt;li&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;empty&lt;/code&gt; method generates an empty array (although very small numbers will be printed) of shape defined by a tuple passed to the function&lt;/li&gt;
  &lt;li&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;linespace&lt;/code&gt; method generates an array of evenly spaced entries given a range and a step size&lt;/li&gt;
  &lt;li&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;full&lt;/code&gt; method returns an array of  shape defined by a tuple passed to the function filled with the same value (third argument outside the tuple)&lt;/li&gt;
  &lt;li&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;indices&lt;/code&gt; method generates an array representing the indices of the grid&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;use-of-special-library-functions&quot;&gt;Use of special library functions&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; has a large list of special cases functions that generate arrays, which are too large and seemingly disconnected to enumerate. Here are a few examples:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# diagonal array
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diagonal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;diag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# identity 
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;identity&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;identity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# eye
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eye&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;eye&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# rand
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Diagonal matrix from array-like structure:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diagonal&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Identity matrix:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;identity&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Diagonal matrix with ones and zeros elsewhere:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eye&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array of random numbers sampled from a uniform distribution:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Diagonal matrix from array-like structure:
[[1 0 0]
 [0 2 0]
 [0 0 3]]

Identity matrix:
[[1. 0. 0.]
 [0. 1. 0.]
 [0. 0. 1.]]

Diagonal matrix with ones and zeros elsewhere:
[[0. 1. 0. 0.]
 [0. 0. 1. 0.]
 [0. 0. 0. 1.]
 [0. 0. 0. 0.]]

Array of random numbers sampled from a uniform distribution:
[[0.75060485 0.07962041]
 [0.36030122 0.11582055]
 [0.57917376 0.93888782]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;diagonal&lt;/code&gt; function returns an array with the numbers in the diagonal and zeros elsewhere&lt;/li&gt;
  &lt;li&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;identity&lt;/code&gt; function returns an identity matrix&lt;/li&gt;
  &lt;li&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eye&lt;/code&gt; function returns an array with ones on the diagonal and zeros elsewhere&lt;/li&gt;
  &lt;li&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;random.rand&lt;/code&gt; function returns an array of random numbers sampled from a uniform distribution&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;vectorization&quot;&gt;Vectorization&lt;/h2&gt;

&lt;p&gt;I claimed “pure” Python is slow. One of the culprits of such slowness is Python’s loops. Loops are bad for performance for complicated reasons related to Python design as a dynamically typed language. The shortest answer to why loops are slow is that Python takes multiple actions for each call (e.g., it access memory several times, type checking, etc.), that compound and hurt performance the more loops you execute.&lt;/p&gt;

&lt;p&gt;In scientific computing we want speed, meaning we want to get rid of loops. This is precisely what’s &lt;strong&gt;vectorization&lt;/strong&gt; all about: &lt;strong&gt;getting rid of loops by performing computations on multiple components of a vector at the same time&lt;/strong&gt;. Hence, performing operations in “vector” form. In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;, vectors can be interpreted as an array, so we could call this “arrayization” if you will, but that sounds funny and weird.&lt;/p&gt;

&lt;p&gt;Here is how vectorization looks like conceptually.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-12/vectorization.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now let’s compare the performance gain of vectorization against looping in a simple sum.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Sum two array with a Python loop (non-vectorized)&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;%%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timeit&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;33 µs ± 447 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Sum arrays with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; (vectorized)&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;%%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timeit&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;505 ns ± 4.17 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As you can see, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; vectorized implementation is several orders of magnitude faster. In the runs I’ve done, approximately 67 times faster (~32 microsecond against ~0.49 microseconds).&lt;/p&gt;

&lt;p&gt;Such minuscule fractions of time may not be important for you know, but consider that we are only adding up two arrays of 100 numbers. In modern data science and machine learning applications, hundreds of thousands and even millions of computations are required to fit any model, and most of the time you will want to fit multiple models several times. Just cut or multiply everything by about 70: the model that takes 1 minute to run, will take 70 minutes, and the model that takes one day can take over two months. I do not know about you, but I do not have all that time to spare.&lt;/p&gt;

&lt;p&gt;In the next section, we cover array mathematics with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;, which essentially are vectorized operations.&lt;/p&gt;

&lt;h2 id=&quot;array-mathematics-and-element-wise-operations&quot;&gt;Array mathematics and element-wise operations&lt;/h2&gt;

&lt;h3 id=&quot;array-arithmetic&quot;&gt;Array arithmetic&lt;/h3&gt;

&lt;p&gt;As in regular mathematics, array arithmetic is  fundamentally about addition, subtraction, multiplication, and division. In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;, this kind of operations are performed &lt;strong&gt;element-wise&lt;/strong&gt;. Take the following example:&lt;/p&gt;

\[\begin{bmatrix}
1\\
2\\
3
\end{bmatrix} +
\begin{bmatrix}
4\\
5\\
6
\end{bmatrix} =
\begin{bmatrix}
2\\
7\\
9
\end{bmatrix}\]

&lt;p&gt;As in this example, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; will add the first elements of each array together, the second elements of each array together, and the third elements of each array together. Hence, &lt;strong&gt;element-wise addition&lt;/strong&gt;. The same can be extrapolated to multi-dimensional arrays. Consider the following example:&lt;/p&gt;

\[\begin{bmatrix}
1 &amp;amp; 2\\
3 &amp;amp; 4
\end{bmatrix} +
\begin{bmatrix}
5 &amp;amp; 6\\
7 &amp;amp; 8
\end{bmatrix} =
\begin{bmatrix}
6 &amp;amp; 8\\
10 &amp;amp; 12
\end{bmatrix}\]

&lt;p&gt;The logic is the same: the top-left elements in each array are added together, the top-right elements of each array are added together, and so on. Subtraction, division, multiplication, exponentiation, logarithms, roots, and many other algebraic operations (or arithmetic depending on whom you ask), will be performed in the same manner.&lt;/p&gt;

&lt;p&gt;Here there is a list of common arithmetic operations.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;19&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;addition&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;subtraction&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;multiplication&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;true_division&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;floor_division&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;remainder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;remainder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array a:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array b:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Addition of a and b:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;addition&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Subtraction of a and b:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subtraction&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Multiplication of a and b:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multiplication&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;True divition of a and b:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;true_division&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Floor division of a and b:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;floor_division&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Remainder of a and b:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;remainder&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array a:
[[1 2 3]
 [4 5 6]
 [7 8 9]]

Array b:
[[10 11 12]
 [13 14 15]
 [16 17 18]]

Addition of a and b:
[[11 13 15]
 [17 19 21]
 [23 25 27]]

Subtraction of a and b:
[[-9 -9 -9]
 [-9 -9 -9]
 [-9 -9 -9]]

Multiplication of a and b:
[[ 10  22  36]
 [ 52  70  90]
 [112 136 162]]

True divition of a and b:
[[0.1        0.18181818 0.25      ]
 [0.30769231 0.35714286 0.4       ]
 [0.4375     0.47058824 0.5       ]]

Floor division of a and b:
[[0 0 0]
 [0 0 0]
 [0 0 0]]

Remainder of a and b:
[[1 2 3]
 [4 5 6]
 [7 8 9]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;What do you think will happen if we try to multiply a 3x3 array by a scalar (a single number? There are some options:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The operation will fail, as their shapes do not match&lt;/li&gt;
  &lt;li&gt;Just the first element of the array will be multiplied by the scalar&lt;/li&gt;
  &lt;li&gt;All elements of the array will be multiplied by the scalar regardless&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let’s try it out.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;array_scalar&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;3x3 array:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;3x3 array times an scalar:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_scalar&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;3x3 array:
[[1 2 3]
 [4 5 6]
 [7 8 9]]

3x3 array times an scalar:
[[ 2  4  6]
 [ 8 10 12]
 [14 16 18]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Each element of the array was multiplied by 2. How does this even work? One option is to “loop” over each of array and multiply by 3 sequentially. But that it is slow, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; is all about speed. What happens is that the scalar is “broadcast” to match the shape of the array BEFORE multiplication. In practice, what we have is a 3x3 array times a 3x3 array of 2s as:&lt;/p&gt;

\[\begin{bmatrix}
1 &amp;amp; 2 &amp;amp; 3\\
4 &amp;amp; 5 &amp;amp; 6\\
7 &amp;amp; 8 &amp;amp; 9
\end{bmatrix}
\begin{bmatrix}
2 &amp;amp; 2 &amp;amp; 2\\
2 &amp;amp; 2 &amp;amp; 2\\
2 &amp;amp; 2 &amp;amp; 2
\end{bmatrix} =
\begin{bmatrix}
2 &amp;amp; 4 &amp;amp; 6\\
8 &amp;amp; 10 &amp;amp; 12\\
14 &amp;amp; 16 &amp;amp; 18
\end{bmatrix}\]

&lt;p&gt;Broadcasting will make computation way faster than looping. There is more to say about broadcasting, and I will cover it more in-depth in a later section. For now, this should help you to understand how element-wise operations work in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;trigonometric-functions&quot;&gt;Trigonometric functions&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; provides a series of convenient functions for trigonometric calculations, which also operate in an element-wise fashion.&lt;/p&gt;

&lt;p&gt;There are several trigonometric functions in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; (see &lt;a href=&quot;https://numpy.org/doc/stable/reference/routines.math.html#trigonometric-functions&quot;&gt;here&lt;/a&gt;). Below a couple of the most common ones.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# sin function
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sin&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# cosine function
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cos&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;cos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# tangent function
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tan&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;tan&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Let’s plot to see the outcome&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;matplotlib.pylab&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;style&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;use&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;dark_background&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;InlineBackend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure_format&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;retina&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# to get high resolution images
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ax1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;subplots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;set_title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;sin&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;set_title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;cos&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tan&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;set_title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;tan&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;tight_layout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-12/output_69_0.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;hyperbolic-functions&quot;&gt;Hyperbolic functions&lt;/h3&gt;

&lt;p&gt;Hyperbolic functions are like trigonometric functions but for the hyperbola rather than for the circle. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; also incorporate several cases (see &lt;a href=&quot;https://numpy.org/doc/stable/reference/routines.math.html#hyperbolic-functions&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# sin function
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sinh&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sinh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# cosine function
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cosh&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;cosh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# tangent function
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tanh&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ax1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;subplots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;set_title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;sinh&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;set_title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;cosh&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tan&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;set_title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;tight_layout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-12/output_73_0.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;rounding&quot;&gt;Rounding&lt;/h3&gt;

&lt;p&gt;Rounding is a delicate subject as rounding errors when compounded over sequences of operations, can completely mess up your results. It is also a common operation for presenting and plotting results to others. Rounding is applied element-wise.&lt;/p&gt;

&lt;p&gt;Let’s generate a sequence of random decimal numbers to see the effect of different rounding procedures available in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; (see &lt;a href=&quot;https://numpy.org/doc/stable/reference/routines.math.html#rounding&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;decimals&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.11111111&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.99999999&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# rounding
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;around&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;around&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decimals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# rounding
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;round_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decimals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# rounding to integer
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rint&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;rint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decimals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# rounding integer towards zero
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fix&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;fix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decimals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# round to the floor
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;floor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;floor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decimals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# round to the ceiling
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ceil&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ceil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decimals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array of decimals:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decimals&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;around&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; round to the given number of decimals:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;around&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; yields identical results than &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;around&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;round_&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;rint&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; round to the nearest integer:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rint&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;fix&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; round to the nearest integer towars zero:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fix&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;floor&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; round to the floor of the input:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;floor&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;ceil&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; round to the ceiling of the input:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ceil&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array of decimals:
[0.11111111 0.20987654 0.30864197 0.4074074  0.50617283 0.60493827
 0.7037037  0.80246913 0.90123456 0.99999999]

&apos;around&apos; round to the given number of decimals:
[0.111 0.21  0.309 0.407 0.506 0.605 0.704 0.802 0.901 1.   ]

&apos;round&apos; yields identical results than &apos;around&apos;:
[0.111 0.21  0.309 0.407 0.506 0.605 0.704 0.802 0.901 1.   ]

&apos;rint&apos; round to the nearest integer:
[0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]

&apos;fix&apos; round to the nearest integer towars zero:
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]

&apos;floor&apos; round to the floor of the input:
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]

&apos;ceil&apos; round to the ceiling of the input:
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;exponents-and-logarithms&quot;&gt;Exponents and logarithms&lt;/h3&gt;

&lt;p&gt;Exponents and logarithms are often used in computations related to probability and statistics. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; incorporate several of the common ones (see &lt;a href=&quot;https://numpy.org/doc/stable/reference/routines.math.html#exponents-and-logarithms&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# exponent
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# exponent(x) -1
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expm1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;expm1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# 2^P
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;exp2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# natural log
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# log base 10
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log10&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;log10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# log base 2
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;log2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Compute exponential element-wise:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Compute &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;exp(x) - 1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; with greater precision for small values:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expm1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Compute &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;2**p&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; for all elements p in the array:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp2&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Compute natural logarithm element-wise:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Compute base 10 logarithm element-wise:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log10&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Compute base 2 logarithm element-wise:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log2&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Compute exponential element-wise:
[ 1.10517092  2.71828183 15.15426224 23.14069263]

Compute &apos;exp(x) - 1&apos; with greater precision for small values:
[ 0.10517092  1.71828183 14.15426224 22.14069263]

Compute &apos;2**p&apos; for all elements p in the array:
[1.07177346 2.         6.58088599 8.82497783]

Compute natural logarithm element-wise:
[-2.30258509  0.          1.          1.14472989]

Compute base 10 logarithm element-wise:
[-1.          0.          0.43429448  0.49714987]

Compute base 2 logarithm element-wise:
[-3.32192809  0.          1.44269504  1.65149613]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;other-miscellaneous-element-wise-operations&quot;&gt;Other miscellaneous element-wise operations&lt;/h3&gt;

&lt;p&gt;There are several other common mathematical operations available in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;, that are routinely used at different stages of the data processing and modeling.&lt;/p&gt;

&lt;p&gt;Here is a list of several important ones. As always, you can find more in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; documentation.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;array_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;array_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# sum over
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sum_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sum_3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;c1&quot;&gt;# take product
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prod_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prod_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prod_3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;prod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;prod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;prod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# cumulative sum
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cumsum_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cumsum_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cumsum_3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;cumsum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;cumsum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;cumsum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# clip values
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clip_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clip_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;clip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;clip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# take absolute value
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;absolute_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;absolute_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;absolute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;absolute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;c1&quot;&gt;# take square root
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqrt_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;absolute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;absolute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; 
&lt;span class=&quot;c1&quot;&gt;# take the square power
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;square_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;square_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;square&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;square&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# sign function
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sign_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sign_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sign&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sign&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# n power
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;power&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;power&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;absolute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;absolute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;sum-1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; sum array elements:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum_1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;sum-2&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; sum rows:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum_2&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;sum-3&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; sum cols:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum_3&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;prod-1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; product array elements:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prod_1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;prod-2&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; product rows: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prod_2&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;prod-3&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; product cols: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prod_3&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;cumsum_1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; cumulative sum array elements:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cumsum_1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;cumsum_2&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; cumulative sum rows:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cumsum_2&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;cumsum_3&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; cumulative sum cols:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cumsum_3&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;clip-1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; limit range of values (2-8):&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clip_1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;clip-2&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; limit range of values (2-8):&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clip_2&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;absolute-1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; absolute value array elements:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;absolute_1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;absolute-2&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; absolute value array elements:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;absolute_2&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;sqrt-1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; non-negative square root array elements:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt_1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;sqrt-2&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; non-negative square root array elements:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt_2&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;square-1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; square array elements: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;square_1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;square-2&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; square array elements: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;square_2&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;sign-1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; sign indication of array elements:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sign_1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;sign-2&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; sign indication of array elements:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sign_2&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;power&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; elements of first array raised to powers from the second:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;power&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&apos;sum-1&apos; sum array elements:-9
&apos;sum-2&apos; sum rows:[-9 -3  3]
&apos;sum-3&apos; sum cols:[-21  -3  15]

&apos;prod-1&apos; product array elements:-99225
&apos;prod-2&apos; product rows: [ 81  35 -35]
&apos;prod-3&apos; product cols: [-315    3  105]

&apos;cumsum_1&apos; cumulative sum array elements:
[ -9 -16 -21 -24 -25 -24 -21 -16  -9]
&apos;cumsum_2&apos; cumulative sum rows:
[[ -9  -7  -5]
 [-12  -8  -4]
 [ -9  -3   3]]
&apos;cumsum_3&apos; cumulative sum cols:
[[ -9 -16 -21]
 [ -3  -4  -3]
 [  3   8  15]]

&apos;clip-1&apos; limit range of values (2-8):
[2 2 2 2 2 2 3 5 7]
&apos;clip-2&apos; limit range of values (2-8):
[[2 2 2]
 [2 2 2]
 [3 5 7]]

&apos;absolute-1&apos; absolute value array elements:
[9 7 5 3 1 1 3 5 7]
&apos;absolute-2&apos; absolute value array elements:
[[9 7 5]
 [3 1 1]
 [3 5 7]]

&apos;sqrt-1&apos; non-negative square root array elements:
[3.         2.64575131 2.23606798 1.73205081 1.         1.
 1.73205081 2.23606798 2.64575131]
&apos;sqrt-2&apos; non-negative square root array elements:
[[3.         2.64575131 2.23606798]
 [1.73205081 1.         1.        ]
 [1.73205081 2.23606798 2.64575131]]

&apos;square-1&apos; square array elements: 
[81 49 25  9  1  1  9 25 49]
&apos;square-2&apos; square array elements: 
[[81 49 25]
 [ 9  1  1]
 [ 9 25 49]]

&apos;sign-1&apos; sign indication of array elements:
[-1 -1 -1 -1 -1  1  1  1  1]
&apos;sign-2&apos; sign indication of array elements:
[[-1 -1 -1]
 [-1 -1  1]
 [ 1  1  1]]

&apos;power&apos; elements of first array raised to powers from the second:
[387420489    823543      3125        27         1         1        27
      3125    823543]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;array-data-type-and-conversions&quot;&gt;Array data type and conversions&lt;/h2&gt;

&lt;p&gt;I mentioned &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; arrays can contain a single data type. This constraint makes data storing and manipulation much more efficient than working with mixed type arrays (like Python lists), which is a priority for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Data types in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; is a relatively complicated subject, particularly if you are not familiar with C or memory allocation. For our purposes, some basic data types are worth knowing:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;np.bool_&lt;/code&gt;: used to represent “Booleans” (True or False)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;np.int&lt;/code&gt;: used to integers numbers&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;np.unit&lt;/code&gt;: used to represent positive integers or “unsigned” integers&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;np.float&lt;/code&gt;: used to represent real numbers (decimals, fractions, etc) or “floating point” numbers&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;np.complex&lt;/code&gt;: used to represent complex numbers&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In my experience, booleans, integers, and float point data types are the ones that you end up using the most. At least explicitly. Other data types are used all the time, but you do not have to worry about it because &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; takes care of it for you. Since Python is a “dynamically typed” language, which simply means that you do not have to tell the computer what data type you will use (Python does this for you), most of the time you do not need to indicate which data type you will use.&lt;/p&gt;

&lt;p&gt;For instance, if you simply define and an array of values, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; will decide what data type to allocate for each:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;bools&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ints&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;full&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;floats&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uint8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;complexs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;unicode&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Catbug&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Chris&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Danny&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Wallow&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Beth&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Booleans :&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bools&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Integers :&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ints&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Floats :&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;floats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Unsigned :&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsigned&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Complexs :&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;complexs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Unicode :&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;unicode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Booleans :bool
Integers :int64
Floats :float64
Unsigned :uint8
Complexs :complex128
Unicode :&amp;lt;U6
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In some instances, you may want to explicitly indicate the data type. Here are some examples of how you can do that:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;int_16&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;int16&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;float_32&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;unsigned_int_8&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;uint8&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Integer 16 bytes data type: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;int_16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Float 32 bytes data type: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float_32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Unsigned integer 8 bytes data type: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsigned_int_8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Integer 16 bytes data type: int16
Float 32 bytes data type: float32
Unsigned integer 8 bytes data type: uint8
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that the numbers after the data type, like 8, 16, 32, and 64, indicate the number of bytes is allocated to represent each element of the array. The higher the number, the more memory.&lt;/p&gt;

&lt;p&gt;There are several reasons why you may want to explicitly indicate the data type. One of the most common reasons is &lt;strong&gt;memory efficiency&lt;/strong&gt;. If you know your range of numbers in a variable is small positive numbers, utilizing 8 bytes unsigned integers will use significantly less memory than a 32 bytes integer. Now, this will be an issue only with massive datasets, meaning datasets that make your computer to have difficulty processing your data, as it is not clear what “massive” means without context.&lt;/p&gt;

&lt;p&gt;A final operation you may want to do, is &lt;strong&gt;to change the data type of an array&lt;/strong&gt;. For instance, let’s say you want to stack two arrays, but one has a floating-point data type and the other integers. Recall that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; arrays can have a single data type. Let’s see an example.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;int_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;float_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array 1: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int_array&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;, data type:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array 2: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float_array&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;, data type:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array 1: [0 1 2 3 4 5 6 7 8 9], data type:int64
Array 2: [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.], data type:float64
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Let’s first stack the arrays as they are:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;stacked_arrays&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;float_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Stacked arrays as they are:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stacked_arrays&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;data type:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stacked_arrays&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Stacked arrays as they are:
[[0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]
 [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]],
data type:float64
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As you can see, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; “upcasted” the data type of lower precision, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;int64&lt;/code&gt;, to the data type of higher precision, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float64&lt;/code&gt;. This is simply because &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;int64&lt;/code&gt; can’t represent float point or real numbers, only integers or natural numbers. But &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float64&lt;/code&gt; can represent integers. So it is the smart choice to make things work.&lt;/p&gt;

&lt;p&gt;Now, you may want to keep everything as integers, for whatever reason. If so, this is what you need to do. First, convert the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float64&lt;/code&gt; array to an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;int64&lt;/code&gt; array as:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;int_array_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;float_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;int64&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And now stack things together.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;stacked_arrays_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;int_array_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Stacked arrays after conversion:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stacked_arrays_2&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;data type:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stacked_arrays_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Stacked arrays after conversion:
[[0 1 2 3 4 5 6 7 8 9]
 [0 1 2 3 4 5 6 7 8 9]],
data type:int64
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There you go, the array is composed by integers of 64 bytes now.&lt;/p&gt;

&lt;p&gt;Let’s say you want to save memory by converting the new array to a &lt;strong&gt;lower byte representation&lt;/strong&gt;, like int 8. This is known as “downcasting”, i.e., the opposite of “upcasting”. For this you simply need to:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;stacked_arrays_3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stacked_arrays_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;int8&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Stacked arrays after downcasting:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stacked_arrays_3&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;data type:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stacked_arrays_3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Stacked arrays after downcasting:
[[0 1 2 3 4 5 6 7 8 9]
 [0 1 2 3 4 5 6 7 8 9]],
data type:int8
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can check and compare the memory “footprint” of each array as:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Memory size Int64 array:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stacked_arrays_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nbytes&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Memory size Int8 array:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stacked_arrays_3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nbytes&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Memory size Int64 array:160
Memory size Int8 array:20
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The memory footprint has been reduced by a factor of 8. This may not sound like a lot, but if you are working with a dataset of size, let’s say, 8 gigabytes, you can reduce such memory usage to only 1 gigabyte, which will also do data manipulation faster.&lt;/p&gt;

&lt;h2 id=&quot;array-manipulation&quot;&gt;Array manipulation&lt;/h2&gt;

&lt;p&gt;The shape of an array is given by the number of elements along each axis. Now, if you think in an array as composed by little boxes or LEGO pieces, you can start to think of how those pieces can be rearranged in different shapes. For example, a 2 x 2 array could be “flattened” to be a 1 x 4 array, or maybe you could “swap” the rows and columns of the array, by moving the little boxes around or even take away a couple of pieces reducing the array to a 1 x 2 shape.&lt;/p&gt;

&lt;p&gt;These kinds of operations are extremely common in any kind of data manipulation, and it is one of the most important skills to acquire. Some people prefer to convert &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; arrays to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Pandas&lt;/code&gt; DataFrames, as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Pandas&lt;/code&gt; provide several easy to use functions to manipulate arrays. Nonetheless, manipulating arrays in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; it is not that much harder, it can save you time and effort by preventing you to going back and forth with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Pandas&lt;/code&gt;, and well, this is a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; tutorial, so we are here to learn &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; way.&lt;/p&gt;

&lt;h3 id=&quot;array-shape-manipulation&quot;&gt;Array shape manipulation&lt;/h3&gt;

&lt;p&gt;Arrays can be changed with or without changing is data. This is equivalent to the difference between rearranging LEGO blocks with or without adding/removing pieces.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reshape&lt;/code&gt; method changes the shape of an array &lt;em&gt;without&lt;/em&gt; changing its data.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;array_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array-1 shape: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array-1 shape: (2, 3)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Array-1 has shape (2, 3), meaning it has 2 rows and 3 columns.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# the two syntaxs below are equivalent
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;array_3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array-2:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_2&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;shape: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array-3:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_3&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;shape: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array-2:
[[1 2]
 [3 4]
 [5 6]],
shape: (3, 2)

Array-3:
[[1 2 3 4 5 6]],
shape: (1, 6)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Array-2 and Array-3 preserve the number of elements of Array-1.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Flattening&lt;/strong&gt; an array, this is, collapsing all values into a single axis or dimension, can be done in two manners:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;array_1_ravel&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ravel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;array_1_flatt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array-1 ravel:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_1_ravel&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;shape: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_1_ravel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array-1 flattened:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_1_flatt&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;shape: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_1_flatt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array-1 ravel:
[1 2 3 4 5 6],
shape: (6,)

Array-1 flattened:
[1 2 3 4 5 6],
shape: (6,)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Why on earth are there two methods to do exactly the same? The answer is that they are not doing the same: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ravel()&lt;/code&gt; returns a ‘&lt;strong&gt;view&lt;/strong&gt;’ of the original array, whereas &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flatten()&lt;/code&gt; returns an independent ‘&lt;strong&gt;copy&lt;/strong&gt;’ of it. Views or images are just “pointers” to the original array in memory, whereas copies have their own space in memory. I’ll cover this in-depth later.&lt;/p&gt;

&lt;p&gt;Another thing you might have realized is that Array-3 has the same elements as Array-1-ravel and Array-1-flattened, but it has an extra pair of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[]&lt;/code&gt; and shape &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(1,6)&lt;/code&gt; instead of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(,6)&lt;/code&gt;. What is going on here?&lt;/p&gt;

&lt;p&gt;Put simply, internally, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; arrays have two parts: the information itself and information about how to interpret/read the array. In the case of the shape information, this indicates how many “indices” are associated with an array. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(1,6)&lt;/code&gt; is saying that there &lt;strong&gt;two indices&lt;/strong&gt; identifying the array: the number 1 for all the elements, and the numbers from 1 to 6 for each element. This makes sense if you think in arrays as matrices or excel sheets: the first element is in the first row and first column (1,1), the second in the first row and the second column (1, 2), and so on.&lt;/p&gt;

&lt;p&gt;However, If you think about it, you don’t need two indices to identify the elements of a one-dimensional array. After all, when we count things we do not count “1 and 1, 1 and 2, 1 and 3, 1 and 4…” and so on. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(6,)&lt;/code&gt; is just saying that there is a &lt;strong&gt;single index&lt;/strong&gt; identifying each of the 6 elements of the array, which makes perfect sense. The first element of the array is in position one (1,), the second in position two (2,), and so on.&lt;/p&gt;

&lt;p&gt;Now you may be wondering. Why then add a 1 as an index if it’s unnecessary? Since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; supports multi-dimensional arrays, technically, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(1,6)&lt;/code&gt; is indicating the array has TWO dimensions or axes instead of one. “BUT, the array has just one dimension, right?” Yes and no. The thing is such array can be represented as either: as a collection of elements along one dimension or as a collection of elements along two dimensions, with the caveat that the first dimension has all the data, and the other is basically “empty” or “flat”, but assigned to it. Just like with the first element is in the first row and first column (1,1)” idea.&lt;/p&gt;

&lt;p&gt;If you are familiar with linear algebra or geometry, you should know that a square is an object with two dimensions, but that can ‘live’ in three, four, five, a million, or any number of dimensions. Essentially, higher-dimensional spaces can contain objects with fewer dimensions, but not the other way around. You can’t fit a sphere in a plane. The misunderstanding, in my view, comes from the tendency to think in data as two-dimensional grid-like objects, when in practice does not need to be like that &lt;em&gt;necessarily&lt;/em&gt;. People like to think in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; arrays as matrices, vectors, tensors, etc., but they aren’t, they are arrays with one or more dimensions. Period.&lt;/p&gt;

&lt;p&gt;This whole discussion may sound like I am beating around the bushes, but I am not. Dimensionality mismatch is one of the most important sources of errors, misunderstandings, and frustrations when working with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; arrays. If you ever do anything related to linear algebra, like pretty much all of machine learning and statistics, you need to have a firm understanding of how dimensions work in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Related to our previous discussion, a “trick” you may want to be aware of, is how to &lt;strong&gt;add dimensions to an array&lt;/strong&gt;, since you will find cases where this can be an issue.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array a: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array a shape: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array a dimensions: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndim&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array a: [1 2 3]

Array a shape: (3,)

Array a dimensions: 1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To add a new dimension and keep array &lt;strong&gt;a&lt;/strong&gt; as “row” in a two-dimensional “matrix”, use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;np.newaxis&lt;/code&gt; object:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;a_row&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;newaxis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array a: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a_row&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array a shape: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a_row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array a dimensions: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a_row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndim&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array a: [[1 2 3]]

Array a shape: (1, 3)

Array a dimensions: 2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To add a new dimension and keep array &lt;strong&gt;a&lt;/strong&gt; as “column” in a two-dimensional “matrix”, just flip the order of the arguments:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;a_col&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;newaxis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array a:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a_col&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array a shape: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a_col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array a dimensions: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a_col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndim&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array a:
[[1]
 [2]
 [3]]

Array a shape: (3, 1)

Array a dimensions: 2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;array-transpose-like-operations&quot;&gt;Array transpose-like operations&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Transposing&lt;/strong&gt; means to “swap” or interchange the position and elements between two or more axes.&lt;/p&gt;

&lt;p&gt;The most common operation is the plain &lt;strong&gt;Transpose&lt;/strong&gt; operation, where the axes get permuted.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;array_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# two dimensional array
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# three dimensional array
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array-1:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;shape:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array-2:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_2&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;shape:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array-1:
[[0 1]
 [2 3]],
shape:(2, 2)

Array-2:
[[[ 0  1]
  [ 2  3]]

 [[ 4  5]
  [ 6  7]]

 [[ 8  9]
  [10 11]]],
shape:(3, 2, 2)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now let’s transpose both:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;array_1_T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;array_2_T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array-1 transposed:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_1_T&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;shape:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_1_T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array-2 transposed:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_2_T&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;shape:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_2_T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array-1 transposed:
[[0 2]
 [1 3]],
shape:(2, 2)

Array-2 transposed:
[[[ 0  4  8]
  [ 2  6 10]]

 [[ 1  5  9]
  [ 3  7 11]]],
shape:(2, 2, 3)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Array-1 has swapped the rows for the columns. Array-2 has reshaped from a three 2x2 arrays, into two 2x3 arrays. This is because of the indices “cycle” such that the third index pass to the first place, the second to the third, and the first to the second.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;moveaxis&lt;/code&gt; method is more flexible than transpose as it allows for an arbitrary rearrangement of axes to new positions. The syntax is simple: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;np.moveaxis(original-array, origin-position-axis-to-move, destiny-position-axis-to-move)&lt;/code&gt;. Recall that axes are index as (0, 1, 2, …0).&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;array_move_2_3_4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;24&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;array_move_2_4_3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;moveaxis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_move_2_3_4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# move axis in position two to position one
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_move_3_2_4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;moveaxis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_move_2_3_4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# move axis in position zero to position one
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_move_3_4_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;moveaxis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_move_2_3_4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# move axist in the zero position to position two
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_move_4_2_3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;moveaxis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_move_2_3_4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# move axes in positions two and one, to positions zero and two
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_move_4_3_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;moveaxis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_move_2_3_4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# move axes in positions two and zero, to positions zero and two
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Original order: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_move_2_3_4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;New axes order 1: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_move_2_4_3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;New axes order 2: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_move_3_2_4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;New axes order 3: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_move_3_4_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;New axes order 4: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_move_4_2_3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;New axes order 5: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_move_4_3_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Original order: (2, 3, 4)

New axes order 1: (2, 4, 3)

New axes order 2: (3, 2, 4)

New axes order 3: (3, 4, 2)

New axes order 4: (4, 2, 3)

New axes order 5: (4, 3, 2)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;array-dimension-manipulation&quot;&gt;Array dimension manipulation&lt;/h3&gt;

&lt;p&gt;Intentionally changing the dimensions of arrays is an operation done mostly, in my experience, when you want to combine arrays or to do mathematical operations with two or more arrays. In the dimensions do not match or are not defined in a certain manner, joining or calculations won’t work, or would work in unexpected manners.&lt;/p&gt;

&lt;p&gt;In this section, I’ll mention just two operations: &lt;strong&gt;expanding dimensions&lt;/strong&gt; and &lt;strong&gt;squeezing dimensions&lt;/strong&gt;, which are opposite operations. There is a third extremely important dimension manipulation operation: broadcasting. Broadcasting is not just important but rather complicated to explain so I will give its own section after this one.&lt;/p&gt;

&lt;p&gt;Expanding dimensions it is always possible as higher-dimensional objects can always contain lower-dimensional objects: you can fit a two-dimensional piece of paper inside a three-dimensional box, but not the other way around (I know! paper is three dimensional, but I hope you get the point).&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;array_one_expand&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;expand_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;array_two_expand&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;expand_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;One dimensional array: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;shape: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;One dimensional array expanded: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one_expand&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;shape: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one_expand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Two dimensional array: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;shape: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Two dimensional array expanded: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two_expand&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;shape: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two_expand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;One dimensional array: 
[1 2 3] 
shape: (3,)

One dimensional array expanded: 
[[1 2 3]] 
shape: (1, 3)

Two dimensional array: 
[[1 2 3]
 [4 5 6]] 
shape: (2, 3)

Two dimensional array expanded: 
[[[1 2 3]
  [4 5 6]]] 
shape: (1, 2, 3)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As you can see, both arrays gain an extra dimension when expanded.&lt;/p&gt;

&lt;p&gt;Let’s bring the arrays back to their original dimensionality with the opposite operation: squeezing.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;array_one_squeez&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;squeeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one_expand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;array_two_squeez&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;squeeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two_expand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Three dimensional array squeezed: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one_squeez&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;shape: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one_squeez&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Three dimensional array squeezed: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two_squeez&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;shape: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two_squeez&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Three dimensional array squeezed: 
[1 2 3] 
shape: (3,)

Three dimensional array squeezed: 
[[1 2 3]
 [4 5 6]] 
shape: (2, 3)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can check the squeezed arrays have the same dimensionality that the original ones as:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Are dimensions for array-one and array-one-squeezed equal?: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_one_squeez&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Are dimensions for array-two and array-two-squeezed equal?: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_two_squeez&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Are dimensions for array-one and array-one-squeezed equal?: True

Are dimensions for array-two and array-two-squeezed equal?: True
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;array-broadcasting&quot;&gt;Array broadcasting&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Broadcasting&lt;/strong&gt; is an automatic &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; mechanism to match the dimensionality of arrays with different shapes for element-wise operations. Broadcasting usually improves speed by means of vectorizing operations, meaning that the loop will occur in compiled C code rather than in Python, as Python looping us is resource-intensive and slow. However, there are some cases where broadcasting is not the best option.&lt;/p&gt;

&lt;p&gt;In the array mathematics section, we saw &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; performs several important computations element-wise, which requires having arrays with matching shapes: arrays of shape (,1), (2,2), and (4,3,2), must be multiplied by arrays with shape (,1), (2,2), and (4,3,2), to be compatible. However, there are cases where we want to multiplied arrays with shapes that do not match, for instance:&lt;/p&gt;

\[\textit{A} x =
\begin{bmatrix}
1 &amp;amp; 2 \\
3 &amp;amp; 4
\end{bmatrix}
2\]

&lt;p&gt;Following linear algebra conventions, we should multiply each element of $\textit{A}$ by 2. The way to get around in this in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;, is by &lt;strong&gt;broadcasting&lt;/strong&gt; the scalar to match the shape of $\textit{A}$ as:&lt;/p&gt;

\[\textit{A} x =
\begin{bmatrix}
1 &amp;amp; 2 \\
3 &amp;amp; 4
\end{bmatrix}
\begin{bmatrix}
2 &amp;amp; 2 \\
2 &amp;amp; 2
\end{bmatrix} =
\begin{bmatrix}
2 &amp;amp; 4 \\
6 &amp;amp; 8
\end{bmatrix}\]

&lt;p&gt;The scalar only gets “stretched” vertically and horizontally during computation. Now, creating copies of $x$ is memory inefficient, so &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; does not actually copy the value in memory. This is slightly inaccurate, but in a nutshell, broadcasting works by reusing the original value  (the   This may not be evident in the $\textit{A}x$ example, but just imagine a (1,000,000, 100) array. In such a case, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; would have to duplicate the size of the dataset, i.e., to create 100,000,000 of values just to perform matrix-scalar multiplication.&lt;/p&gt;

&lt;p&gt;Not all arrays can be broadcast. They must meet certain conditions, the “Broadcasting rule”, which according to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; documentation states:&lt;/p&gt;

&lt;p&gt;“In order to broadcast, the size of the trailing axes for both arrays in an operation must either be the same size or one of them must be one.”&lt;/p&gt;

&lt;p&gt;This is easier to understand visually. The figure below shows the cases where broadcasting is valid, and the next one when it is not.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-12/broadcasting.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For instance:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ones &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;(2,2) array:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;shape: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;(1, ) array:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;shape: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Element-wise operations are valid between a and b:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(2,2) array:
[[1. 1.]
 [1. 1.]]
shape: (2, 2)]

(1, ) array:
[1.]
shape: (1,)]

Element-wise operations are valid between a and b:
[[2. 2.]
 [2. 2.]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Invalid operations are variations of:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-12/invalid-broadcasting.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can verify that the above operation does not work:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# a + b 
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this case, we get a “ValueError: operands could not be broadcast together” error message.&lt;/p&gt;

&lt;p&gt;Most of the time you won’t need to think in dimension matching beforehand. Either it will work or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; will let you know dimensions do not match. The important part is to be aware of broadcasting mechanics such that you can debug dimension mismatch problems rapidly.&lt;/p&gt;

&lt;h3 id=&quot;joining-arrays&quot;&gt;Joining arrays&lt;/h3&gt;

&lt;p&gt;Joining arrays is another common operation in data processing, particularly to put together data coming from different sources. For instance, large datasets are commonly split into several sub-datasets containing different features or variables associated with the same population.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://numpy.org/doc/stable/reference/routines.array-manipulation.html#joining-arrays&quot;&gt;Here&lt;/a&gt; are all the joining methods in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;. Below a couple of the main methods.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;base_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;join_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;concatenate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;concatenate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;join_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;hstack&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;hstack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;join_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;vstack&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;vstack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;join_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To &lt;strong&gt;concatenate&lt;/strong&gt; arrays must have at least one equal dimension, which must be defined as the axis reference. Here we concatenate along the first axis (rows match). If you try to concatenate along the second axis the operation will fail (columns do not match).&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Row-wise concatenation:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concatenate&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Row-wise concatenation:
[[1 2 3]
 [4 5 6]
 [7 8 9]
 [1 2 3]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To &lt;strong&gt;stack&lt;/strong&gt; arrays, all the arrays must have the same dimensions. The logic here is to generate an array with an extra dimension, like stacking LEGO pieces with the same shape.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Stacking:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;shape before stacking:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;shape after stacking:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Stacking:
[[[1 2 3]
  [4 5 6]
  [7 8 9]]

 [[1 2 3]
  [4 5 6]
  [7 8 9]]]

shape before stacking:(3, 3)
shape after stacking:(2, 3, 3)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Horizontal stacking&lt;/strong&gt; (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hstack&lt;/code&gt;) and &lt;strong&gt;vertical stacking&lt;/strong&gt; (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vstack&lt;/code&gt;), stack arrays along the horizontal and vertical axes, i.e., column-wise and row-wise, meaning that the array will “grow” horizontally (attached to the right) and vertically (attached below), respectively.&lt;/p&gt;

&lt;p&gt;In most cases, the same effect can be accomplished with the concatenation method along axis 1 (cols) and axis 0 (rows). To work, &lt;strong&gt;horizontal stacking&lt;/strong&gt; must match along all axes but the first one, the first one being the horizontal one, or “the rows” in the 2-dimensional case. This is why we had to transpose the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;join_array&lt;/code&gt;, such that rows match. In other words, you can have an arbitrary number of columns but everything else must match. &lt;strong&gt;Vertical stacking&lt;/strong&gt; is analogous: you can have an arbitrary number of rows, but columns must match.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Horizontal-wise or column-wise stacking:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hstack&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Vertical-wise or row-wise stacking:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vstack&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Horizontal-wise or column-wise stacking:
[[1 2 3 1]
 [4 5 6 2]
 [7 8 9 3]]

Vertical-wise or row-wise stacking:
[[1 2 3]
 [4 5 6]
 [7 8 9]
 [1 2 3]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;splitting-arrays&quot;&gt;Splitting arrays&lt;/h3&gt;

&lt;p&gt;Splitting arrays is common when you want to analyze, model, or plot a subset of the data. Also when your data size is enormous and you want to save it in chunks.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://numpy.org/doc/stable/reference/routines.array-manipulation.html#splitting-arrays&quot;&gt;Here&lt;/a&gt; are all the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; splitting functions. Let’s explore a couple.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;split_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;split_two&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;array_split_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_split_two&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;hsplit_one&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;hsplit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;hsplit_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vsplit_two&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;hsplit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;vsplit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;split&lt;/code&gt; method will work as long as you ask for a number of sub-arrays which can be obtained via equal division of the original array. For instance, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;array_one&lt;/code&gt; can be equally divided into two arrays.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array one (9,):&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array two (2,2,2):&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array one is split into 3 (1,3) sub-arrays:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array two is split into 2 (1,2,2) sub-arrays:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array one (9,):
(9,)

Array two (2,2,2):
(2, 2, 2)

Array one is split into 3 (1,3) sub-arrays:
[0 1 2]
[3 4 5]
[6 7 8]

Array two is split into 2 (1,2,2) sub-arrays:
[[[0 1]
  [2 3]]]
[[[4 5]
  [6 7]]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;array_split&lt;/code&gt; function provides identical functionality than the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;array&lt;/code&gt; function, with the difference that it will work even when the original array cannot be equally divided into the requested number of sub-arrays. Basically, if you try to split the array one (9,) into 2, the operation will work and the last number (the “9”) will be completely ignored. On the other hand, if you try to split an array two (2,2,2) into 3 parts, it will generate  an extra empty axis.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array one split into 2 sub-arrays:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_split_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_split_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array two split into 3 sub-arrays:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_split_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_split_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_split_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array one split into 2 sub-arrays:
[0 1 2 3 4]
[5 6 7 8]

Array two split into 3 sub-arrays:
[[[0 1]
  [2 3]]]
[[[4 5]
  [6 7]]]
[]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As with concatenation, horizontal split (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hsplit&lt;/code&gt;) and vertical split (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vsplit&lt;/code&gt;) provide equivalent functionality than the split method (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;split&lt;/code&gt;), but restricted to the horizontal and vertical axis respectively. Equal division is also a constrain here. Array one can’t be split vertically because it has only one dimension.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array one horizontal split into 3 sub-arrays:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hsplit_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hsplit_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hsplit_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array two horizontal split into 2 sub-arrays:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hsplit_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hsplit_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array two horizontal split into 2 sub-arrays:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vsplit_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vsplit_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array one horizontal split into 3 sub-arrays:
[0 1 2]
[3 4 5]
[6 7 8]

Array two horizontal split into 2 sub-arrays:
[[[0 1]]

 [[4 5]]]
[[[2 3]]

 [[6 7]]]

Array two horizontal split into 2 sub-arrays:
[[[0 1]
  [2 3]]]
[[[4 5]
  [6 7]]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;array-repetition&quot;&gt;Array repetition&lt;/h3&gt;

&lt;p&gt;Arrays can be constructed or expanded via repetition. Data simulation and image manipulation are situations where you may want to use this functionality.&lt;/p&gt;

&lt;p&gt;There are two &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; methods that may look similar at first but they are not: &lt;strong&gt;tiling&lt;/strong&gt; and &lt;strong&gt;repetition&lt;/strong&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tile_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tile_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tile_three&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tile_four&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;tile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;tile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;tile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;tile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;  
&lt;span class=&quot;n&quot;&gt;repeat_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;repeat_two&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;repeat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;repeat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;repeat_three&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;repeat_four&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;repeat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;repeat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Tiling&lt;/strong&gt; will attach an entire copy of the array (as a block) at its right or below it. The number of copies attached is specified as a tuple, with the first indicating the number of “rows” copies and the second the number of “column” copies.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Repeat array one twice column-wise:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tile_one&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Repeat array one twice row-wise:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tile_two&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Repeat array two twice column-wise:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tile_three&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Repeat array twotwice  row-wise:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tile_four&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Repeat array one twice column-wise:
[[1 2 3 1 2 3]]

Repeat array one twice row-wise:
[[1 2 3]
 [1 2 3]]

Repeat array two twice column-wise:
[[1 2 3 1 2 3]
 [4 5 6 4 5 6]
 [7 8 9 7 8 9]]

Repeat array twotwice  row-wise:
[[1 2 3]
 [4 5 6]
 [7 8 9]
 [1 2 3]
 [4 5 6]
 [7 8 9]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;strong&gt;repeat&lt;/strong&gt; method will replicate the &lt;strong&gt;elements&lt;/strong&gt; of the array in place. For instance:&lt;/p&gt;

\[\begin{bmatrix}
1 &amp;amp; 2
\end{bmatrix}\]

&lt;p&gt;Will be repeated as:&lt;/p&gt;

\[\begin{bmatrix}
1 &amp;amp; 1 &amp;amp; 2 &amp;amp; 2
\end{bmatrix}\]

&lt;p&gt;Instead as:&lt;/p&gt;

\[\begin{bmatrix}
1 &amp;amp; 2 &amp;amp; 1 &amp;amp; 2
\end{bmatrix}\]

&lt;p&gt;The latter behavior is expected from the tile method instead. The axis for repetition is specified independently as “axis=0” for rows and “axis=1” for columns.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Repeat array one twice row-wise:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;repeat_one&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Repeat array two twice and flattened into one dimension:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;repeat_two&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Repeat array two twice column-wise:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;repeat_three&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Repeat array two twice row-wise:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;repeat_four&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Repeat array one twice row-wise:
[1 1 2 2 3 3]

Repeat array two twice and flattened into one dimension:
[1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8 9 9]

Repeat array two twice column-wise:
[[1 1 2 2 3 3]
 [4 4 5 5 6 6]
 [7 7 8 8 9 9]]

Repeat array two twice row-wise:
[[1 2 3]
 [1 2 3]
 [4 5 6]
 [4 5 6]
 [7 8 9]
 [7 8 9]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;repeat_three&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;repeat_four&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(array([[1, 1, 2, 2, 3, 3],
        [4, 4, 5, 5, 6, 6],
        [7, 7, 8, 8, 9, 9]]),
 array([[1, 2, 3],
        [1, 2, 3],
        [4, 5, 6],
        [4, 5, 6],
        [7, 8, 9],
        [7, 8, 9]]))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;adding-and-removing-array-elements&quot;&gt;Adding and removing array elements&lt;/h3&gt;

&lt;p&gt;There are several &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; methods to add and remove elements from arrays. You may want to do this to clean a dataset, subset datasets, combine dataset, or maybe just playing a prank on someone 🤷.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;array_three&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;delete_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;delete_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;delete_three&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;delete&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;delete&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;delete&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;insert_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;insert_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;insert_three&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;insert_four&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;insert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;insert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;insert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;insert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;unique_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;unique_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;unique_three&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;unique&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_three&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;unique&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_three&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;unique&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_three&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;strong&gt;delete&lt;/strong&gt; method remove elements along the specified axis. In essence, you have to index the sub-array you want to remove to the method call.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array one:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array two:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Delete element in position 2 in array one:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;delete_one&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Delete column (along axis 1) in position 0 in array two:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;delete_two&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Delete row (along axis 0) in position 2 in array two:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;delete_three&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array one:
[1 2 3]

Array two:
[[1 2 3]
 [4 5 6]
 [7 8 9]]

Delete element in position 2 in array one:
[1 2]

Delete column (along axis 1) in position 0 in array two:
[[2 3]
 [5 6]
 [8 9]]

Delete row (along axis 0) in position 2 in array two:
[[1 2 3]
 [7 8 9]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;strong&gt;insert&lt;/strong&gt; method will insert elements along the specified axis. If no axis is specified the value will be inserted in a flattened version of the array. To insert values you also have to indicate the position index.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Insert a &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; at position 1 in array one:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;insert_one&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Insert a &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; at position 5 in array two:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;insert_two&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Insert a sub-array of &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;9s&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; at position 1 in array two along axis 0 (rows):&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;insert_three&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Insert a sub-array of &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;9s&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; at position 1 in array two along axis 1 (cols):&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;insert_four&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Insert a &quot;9&quot; at position 1 in array one:
[1 9 2 3]

Insert a &quot;9&quot; at position 5 in array two:
[1 2 3 4 5 9 6 7 8 9]

Insert a sub-array of &quot;9s&quot; at position 1 in array two along axis 0 (rows):
[[1 2 3]
 [9 9 9]
 [4 5 6]
 [7 8 9]]

Insert a sub-array of &quot;9s&quot; at position 1 in array two along axis 1 (cols):
[[1 9 2 3]
 [4 9 5 6]
 [7 9 8 9]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;strong&gt;unique&lt;/strong&gt; method will return the unique elements along the specified axis. If no axis is provided the unique method will operate over a flattened version of the array. By unique we refer to the unique rows and columns as a whole, not the unique elements within a row or a column.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array three:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_three&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Unique elements flattened version array three:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique_one&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Unique elements along axis 0 (rows) array three:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique_two&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Unique elements along axis 1 (cols) array three:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique_three&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array three:
[[1 1 2 2]
 [1 1 2 2]
 [2 2 3 3]
 [2 2 3 3]]

Unique elements flattened version array three:
[1 2 3]

Unique elements along axis 0 (rows) array three:
[[1 1 2 2]
 [2 2 3 3]]

Unique elements along axis 1 (cols) array three:
[[1 2]
 [1 2]
 [2 3]
 [2 3]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;rearranging-array-elements&quot;&gt;Rearranging array elements&lt;/h3&gt;

&lt;p&gt;By rearranging we refer to altering the order or position of the elements of an array without changing its shape (for that see the shape manipulation section).&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;flip_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flip_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flip_three&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flip_four&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;flip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;flip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;flip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;flip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;roll_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;roll_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;roll_three&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;roll_four&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;roll&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;roll&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;roll&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;roll&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;strong&gt;flip&lt;/strong&gt; reverse the order of elements in an array along the specified axis. If no axis is specified, the order of the elements is reversed as if it were a flattened array, but the shape is preserved. Notice that for arrays with 2 or more axis, flipping happens to entire rows or columns (or elements of the axis) rather than that to elements within rows or columns. There are many ways to flip an array by combining position and axis, here just a couple of examples.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array one:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array two:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Reverse array one:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flip_one&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Reverse array two:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flip_two&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Reverse array two along axis 0 (rows):&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flip_three&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Reverse array two along axis 1 (cols):&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flip_four&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array one:
[1 2 3 4 5 6 7 8 9]

Array two:
[[1 2 3]
 [4 5 6]
 [7 8 9]]

Reverse array one:
[9 8 7 6 5 4 3 2 1]

Reverse array two:
[[9 8 7]
 [6 5 4]
 [3 2 1]]

Reverse array two along axis 0 (rows):
[[7 8 9]
 [4 5 6]
 [1 2 3]]

Reverse array two along axis 1 (cols):
[[3 2 1]
 [6 5 4]
 [9 8 7]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;strong&gt;roll&lt;/strong&gt; method moves or “push” elements in an array along the specified axis. This has the effect of moving all elements at once, so all get repositioned. There are many ways to roll an array by combining the number of positions to be roll and the axis, here just a couple of examples.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array one:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array two:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Roll elements array one by one position:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;roll_one&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Roll elements array two by one position:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;roll_two&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Roll elements array two by one position along axis 0 (rows):&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;roll_three&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Roll elements array two by one position along axis 1 (cols):&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;roll_four&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array one:
[1 2 3 4 5 6 7 8 9]

Array two:
[[1 2 3]
 [4 5 6]
 [7 8 9]]

Roll elements array one by one position:
[9 1 2 3 4 5 6 7 8]

Roll elements array two by one position:
[[9 1 2]
 [3 4 5]
 [6 7 8]]

Roll elements array two by one position along axis 0 (rows):
[[7 8 9]
 [1 2 3]
 [4 5 6]]

Roll elements array two by one position along axis 1 (cols):
[[3 1 2]
 [6 4 5]
 [9 7 8]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;logic-functions-and-array-evaluation&quot;&gt;Logic functions and array evaluation&lt;/h2&gt;

&lt;p&gt;There are multiple cases where applying logic functions to evaluate array elements will come in handy. Slicing, indexing, and data transformation rely heavily on logic functions.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; logic functions can be divided on boolean testing, array identity testing, array elements testing, logic operators, and comparison operators.&lt;/p&gt;

&lt;h3 id=&quot;boolean-testing&quot;&gt;Boolean testing&lt;/h3&gt;

&lt;p&gt;Boolean testing refers to whether &lt;strong&gt;all&lt;/strong&gt; or &lt;strong&gt;some&lt;/strong&gt; elements of an array are &lt;strong&gt;True&lt;/strong&gt;. There are two functions for this &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;all&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;any&lt;/code&gt;. Below I exemplify several cases:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;true_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;some_true_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;false_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ones_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;some_ones_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;zeros_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;NAN_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nan&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nan&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nan&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Infinity_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;All elements of true_array are True: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;true_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Some elements of true_array are True: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;any&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;true_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;All elements of some_true_array are True: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;some_true_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Some elements of some_true_array are True: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;any&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;some_true_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;All elements of false_array are True: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;false_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Some elements of false_array are True: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;any&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;false_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;All elements of ones_array are True: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Some elements of ones_array are True: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;any&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;All elements of some_ones_array are True: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;some_ones_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Some elements of some_ones_array are True: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;any&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;some_ones_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;All elements of zeros_array are True: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Some elements of zeros_array are True: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;any&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;All elements of NAN_array are True: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NAN_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Some elements of NAN_array are True: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;any&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NAN_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;All elements of Infinity_array are True: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Infinity_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Some elements of Infinity_array are True: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;any&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Infinity_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;All elements of true_array are True: True
Some elements of true_array are True: True

All elements of some_true_array are True: False
Some elements of some_true_array are True: True

All elements of false_array are True: False
Some elements of false_array are True: False

All elements of ones_array are True: True
Some elements of ones_array are True: True

All elements of some_ones_array are True: False
Some elements of some_ones_array are True: True

All elements of zeros_array are True: False
Some elements of zeros_array are True: False

All elements of NAN_array are True: True
Some elements of NAN_array are True: True

All elements of Infinity_array are True: True
Some elements of Infinity_array are True: True
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;array-elements-testing&quot;&gt;Array elements testing&lt;/h3&gt;

&lt;p&gt;This subset of functions tests the identity elements of an array, particularly for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; constant like NAN or infinity. This is useful for data cleaning and debugging purposes. Return values are always True or False. Below some examples:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;element_testing&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nan&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,]&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array to test:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;element_testing&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Element-wise testing for finiteness:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;isfinite&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;element_testing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Element-wise testing for infinity:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;isinf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;element_testing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Element-wise testing for negative infinity:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;isneginf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;element_testing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Element-wise testing for positive infinity:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;isposinf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;element_testing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Element-wise testing for not a number:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;isnan&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;element_testing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array to test:
[1, 0, nan, inf, -inf]

Element-wise testing for finiteness:
[ True  True False False False]

Element-wise testing for infinity:
[False False False  True  True]

Element-wise testing for negative infinity:
[False False False False  True]

Element-wise testing for positive infinity:
[False False False  True False]

Element-wise testing for not a number:
[False False  True False False]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Notice that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;np.nan&lt;/code&gt; is neither infinity nor finite, simply because is not a number, and only numbers can be tested for that.&lt;/p&gt;

&lt;h3 id=&quot;array-type-testing&quot;&gt;Array type testing&lt;/h3&gt;

&lt;p&gt;Array type testing is another example of element-wise testing but for the specific case of &lt;strong&gt;data type&lt;/strong&gt;. Return values are always True or False. Here are a couple of examples of the available functions:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# use Python list instead of array to mix data types
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;type_testing&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nan&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;3j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array tested:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;type_testing&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Is real:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;isreal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;type_testing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Is scalar:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;isreal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;type_testing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Is complex:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;iscomplex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;type_testing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array tested:
[(1+1j), 0, 1, 2.0, False, nan, inf, 3j]

Is real:
[False  True  True  True  True  True  True False]

Is scalar:
[False  True  True  True  True  True  True False]

Is complex:
[ True False False False False False False  True]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;logical-operators&quot;&gt;Logical operators&lt;/h3&gt;

&lt;p&gt;Logic operators are a subset of logical functions in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;. Basically, the operators you will find in logic gates or Truth tables: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;and&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;or&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;not&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xor&lt;/code&gt; (exclusive &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;or&lt;/code&gt;). Return values are always True or False. Keep in mind that each element of the array is tested independently on both conditions.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Greater than 1 AND less than 5:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;logical_and&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Greater than 1 OR less than 5:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;logical_or&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Greater than 1 NOT less than 5:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;logical_and&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Greater than 1 XOR less than 5:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;logical_xor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array:
[0 1 2 3 4 5 6]

Greater than 1 AND less than 5:
[False False  True  True  True False False]

Greater than 1 OR less than 5:
[ True  True  True  True  True  True  True]

Greater than 1 NOT less than 5:
[False False  True  True  True False False]

Greater than 1 XOR less than 5:
[ True  True False False False  True  True]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;comparison-operators&quot;&gt;Comparison operators&lt;/h3&gt;

&lt;p&gt;Comparison operators assess the relationship between a pair of arrays or array elements. Given the inaccuracies resulting from the &lt;em&gt;finite&lt;/em&gt; or &lt;em&gt;truncated&lt;/em&gt; representation of infinite or very large (or small) numbers, a comparison of quantities should proceed with caution. Let’s begin for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;greater&lt;/code&gt; to illustrate the logic:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; 

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Element-wise GREATER than comparison:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;greater&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Element-wise GREATER than comparison shorthand (&amp;gt;):&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Element-wise GREATER than comparison (flip):&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;greater&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Element-wise GREATER than comparison shorthand (&amp;gt;) (flip):&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Element-wise GREATER than comparison:
[False False  True]

Element-wise GREATER than comparison shorthand (&amp;gt;):
[False False  True]

Element-wise GREATER than comparison (flip):
[False  True False]

Element-wise GREATER than comparison shorthand (&amp;gt;) (flip):
[False  True False]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Notice that although we are comparing the same arrays, the order matters. In the first case you are asking: “is 1 greater than 1, is 1 greater than 2, is 3 greater than 2”. Whereas in the second case: “is 1 greater than 1, is 2 greater than 1, is 2 greater than 3”. Also, notice you can use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;gt;&lt;/code&gt; shorthand. The same logic applies to the following cases:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Element-wise GREATER_EQUAL than comparison:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;greater_equal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Element-wise GREATER_EQUAL than comparison shorthand (&amp;gt;=):&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Element-wise LESS than comparison:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;less&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Element-wise LESS than comparison shorthand (&amp;lt;):&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Element-wise LESS_EQUAL than comparison:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;less_equal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Element-wise LESS_EQUAL than comparison shorthand (&amp;lt;=):&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Element-wise EQUAL than comparison:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;equal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Element-wise EQUAL than comparison shorthand (==):&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Element-wise NOT_EQUAL than comparison:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;not_equal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Element-wise NOT_EQUAL than comparison shorthand (!=):&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Element-wise GREATER_EQUAL than comparison:
[ True False  True]

Element-wise GREATER_EQUAL than comparison shorthand (&amp;gt;=):
[ True False  True]

Element-wise LESS than comparison:
[False  True False]

Element-wise LESS than comparison shorthand (&amp;lt;):
[False  True False]

Element-wise LESS_EQUAL than comparison:
[ True  True False]

Element-wise LESS_EQUAL than comparison shorthand (&amp;lt;=):
[ True  True False]

Element-wise EQUAL than comparison:
[ True False False]

Element-wise EQUAL than comparison shorthand (==):
[ True False False]

Element-wise NOT_EQUAL than comparison:
[False  True  True]

Element-wise NOT_EQUAL than comparison shorthand (!=):
[False  True  True]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now we review comparison operators which help to deal with cases where you would think two values should be considered equal, but they are not:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;array_three&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;array_four&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.00001e10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array three: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_three&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;, Array four: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_four&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Element-wise equality: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;equal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_three&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_four&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Element-wise is close: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;isclose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_three&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_four&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array three: [1.e+10 1.e-08], Array four: [1.00001e+10 1.00000e-09]

Element-wise equality: [False False]
Element-wise is close: [ True  True]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the example above both numbers are “practically” the same, but technically they are not. Depending on your task at hand, you may want them to be evaluated as equal (given some tolerance level), and you can use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;isclose&lt;/code&gt; method for such purpose. Examples of how the evaluation changes as you change the tolerance:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Element-wise is close: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;isclose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_three&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_four&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;atol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Element-wise is close: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;isclose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_three&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_four&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;atol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Element-wise is close: [ True False]
Element-wise is close: [ True  True]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To compare whether two arrays are equal, this is, if they contain the same elements and have the same shape:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;array_five&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_six&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Are array-three and array-four equal: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array_equal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_three&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_four&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Are array-five and array-six equal: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array_equal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_five&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_six&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Are array-three and array-four equal: False

Are array-five and array-six equal: True
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;array-indexing&quot;&gt;Array Indexing&lt;/h2&gt;

&lt;p&gt;Elements in a have &lt;strong&gt;indices&lt;/strong&gt;, which simply are numbers identifying the &lt;strong&gt;position&lt;/strong&gt; each element occupies in the array.&lt;/p&gt;

&lt;p&gt;Indexing in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; is 0-based, as in native Python, meaning that you start to count positions at 0 rather than at 1. Indexing is done by utilizing square brackets as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;([])&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Indexing is a versatile operation useful in a wide variety of cases. The most common ones are to &lt;strong&gt;insert&lt;/strong&gt; (“assignment”) values, to &lt;strong&gt;extract&lt;/strong&gt; (“reference”) values, to &lt;strong&gt;delete&lt;/strong&gt; values, and to &lt;strong&gt;change&lt;/strong&gt; values.&lt;/p&gt;

&lt;p&gt;In what follows I refer to operations and objects with concepts that are commonly used in data science, but that deviate a bit from technical indexing terminology in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;. For a more technical treatment of the topic see &lt;a href=&quot;https://numpy.org/doc/stable/reference/arrays.indexing.html#&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I also mostly use extraction or “reference” kind of operations to illustrate concepts, but the same ideas apply to insert, deleting, or changing values.&lt;/p&gt;

&lt;h3 id=&quot;basic-indexing-in-one-dimensional-arrays&quot;&gt;Basic indexing in one-dimensional arrays&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; support indexing in one and multiple dimensions. Let’s explore a simple case with a couple of examples.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array one: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array one dimensions: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndim&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;, shape:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array one: [ 1  2  3  4  5  6  7  8  9 10]
Array one dimensions: 1, shape:(10,)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Elements in array are index with square brackets:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Select element at position [0]: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Select element at position [5]: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Select element at position [9]: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Select element at position [-5]: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Select element at position [-1]: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Select element at position [0]: 1
Select element at position [5]: 6
Select element at position [9]: 10
Select element at position [-5]: 6
Select element at position [-1]: 10
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Notice that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;array_one[9]&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;array_one[-1]&lt;/code&gt; return the same value, which is the last element of the array. This shows that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; (as Python does) can index elements both ways: (1) from &lt;strong&gt;left-to-right&lt;/strong&gt; starting at &lt;strong&gt;0&lt;/strong&gt;, and (2) from &lt;strong&gt;right-to-left&lt;/strong&gt; starting at &lt;strong&gt;-1&lt;/strong&gt;. The image below illustrates &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; indexing that you can use as a mental model.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-12/indexing.svg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;slicing-one-dimensional-arrays&quot;&gt;Slicing one-dimensional arrays&lt;/h3&gt;

&lt;p&gt;To select a &lt;strong&gt;range&lt;/strong&gt; of elements, also known as &lt;strong&gt;slicing&lt;/strong&gt;, we use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[:]&lt;/code&gt; notation:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Elements from position [0] to position [3]: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Elements from position [5] to position [9]: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Elements from position [-9] to position [-5]: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Elements from position [-3] to position [-1]: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Elements from position [3] to position [-1]: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Elements from position [0] to position [3]: [1 2 3]
Elements from position [5] to position [9]: [6 7 8 9]
Elements from position [-9] to position [-5]: [2 3 4 5]
Elements from position [-3] to position [-1]: [8 9]
Elements from position [3] to position [-1]: [4 5 6 7 8 9]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There are a couple of interesting facts here. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; operates including the first element on the left but the last on the right. In set notation:&lt;/p&gt;

\[[\text{included}:\text{not-included})\]

&lt;p&gt;This is why the element at position 3 (i.e., number 4) is not included in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;array_one[0:3]&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;It’s also interesting to notice that when using negative indices (right-to-left), you still have to ‘think’ with a left-to-right logic, this is, considering that indices are organized as [-10, -9, -8, …, -3, -2, -1]. You can also ‘mix’ left-to-right and right-to-left indices as in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;array_one[3:-1]&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;When we slice arrays to return sub-arrays, we can specify the &lt;strong&gt;stride&lt;/strong&gt;, this is, how many steps we take when pointing at indices to retrieve array elements. The default stride is one. This is simple to see with a couple of examples:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Slice from position [0] to position [6] with stride [1]: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Slice from position [0] to position [6] with stride [2]: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Slice from position [-6] to position [-1] with stride [3]: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Slice from position [0] to position [6] with stride [1]: [1 2 3 4 5 6]
Slice from position [0] to position [6] with stride [2]: [1 3 5]
Slice from position [-6] to position [-1] with stride [3]: [5 8]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;basic-indexing-in-multidimensional-arrays&quot;&gt;Basic indexing in multidimensional arrays&lt;/h3&gt;

&lt;p&gt;Array indexing with multiple dimensions follows the same logic as with one dimension. The &lt;strong&gt;NumPy indexing model&lt;/strong&gt; figure above illustrates this as well.&lt;/p&gt;

&lt;p&gt;There are two ways to index arrays with multiple dimensions:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;referencing each dimension/axis within a &lt;strong&gt;single pair of square brackets&lt;/strong&gt; with each dimension/axis separated by commas &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[,]&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;referencing each dimension independently with &lt;strong&gt;as many square brackets as dimensions/axes&lt;/strong&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[][]&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Here a couple of examples:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;array_three&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array two dimensions/axes: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array three dimensions/axes: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_three&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array two dimensions: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndim&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;, shape:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array three dimensions: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_three&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndim&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;, shape:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_three&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array two dimensions/axes: 
[[1 2 3]
 [4 5 6]
 [7 8 9]]

Array three dimensions/axes: 
[[[1 2]
  [3 4]]

 [[5 6]
  [7 8]]]

Array two dimensions: 2, shape:(3, 3)
Array three dimensions: 3, shape:(2, 2, 2)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Element at position 1 in first axis (rows) and position 1 in second axis (cols): &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Element at position 0 in first axis (rows) and position 2 in second axis (cols): &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Element at position -1 in first axis (rows) and position -3 in second axis (cols): &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Element at position 1 in first axis (rows) and position 1 in second axis (cols): 5
Element at position 0 in first axis (rows) and position 2 in second axis (cols): 3
Element at position -1 in first axis (rows) and position -3 in second axis (cols): 7
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can retreive the same elements by utilizing the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[][]&lt;/code&gt; notation as:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Element at position 1 in first axis (rows) and position 1 in second axis (cols): &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Element at position 0 in first axis (rows) and position 2 in second axis (cols): &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Element at position -1 in first axis (rows) and position -3 in second axis (cols): &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Element at position 1 in first axis (rows) and position 1 in second axis (cols): 5
Element at position 0 in first axis (rows) and position 2 in second axis (cols): 3
Element at position -1 in first axis (rows) and position -3 in second axis (cols): 7
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;What notation to use then? If you plan to delete the original array (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;array_one&lt;/code&gt; in this example), you are better of utilizing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[][]&lt;/code&gt; notation as this creates a new temporary array that occupies memory. Otherwise, you will be better off by utilizing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[,]&lt;/code&gt; instead as it is only a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;view&lt;/code&gt;, i.e., a pointer to the original array that does not occupy extra memory, so it’s faster.&lt;/p&gt;

&lt;p&gt;To index ranges or intervals in multidimensional arrays, we can mix the slice notation &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[:]&lt;/code&gt; with the multidimensional index notation &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[,]&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Selecting elements from the first axis or “row-wise” in two-dimensional arrays:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array two as reference: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;All elements at position 0 from first axis (all elements from first row): &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;All elements at position 1 from first axis (all elements from second row): &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;All elements at position 2 from first axis (all elements from third row): &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array two as reference: 
[[1 2 3]
 [4 5 6]
 [7 8 9]]

All elements at position 0 from first axis (all elements from first row): 
[1 2 3]

All elements at position 1 from first axis (all elements from second row): 
[4 5 6]

All elements at position 2 from first axis (all elements from third row): 
[7 8 9]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Selecting elements from the second axis or “column-wise” in two-dimensional arrays:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array two as reference: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;All elements at position 0 from second axis (all elements from first  column): &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;All elements at position 1 from second axis (all elements from second column): &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;All elements at position 2 from second axis (all elements from third  column): &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array two as reference: 
[[1 2 3]
 [4 5 6]
 [7 8 9]]

All elements at position 0 from second axis (all elements from first  column): 
[1 4 7]

All elements at position 1 from second axis (all elements from second column): 
[2 5 8]

All elements at position 2 from second axis (all elements from third  column): 
[3 6 9]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Selecting elements ranges of elements in both axes in two dimensional arrays:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Elements at position 0 and 1 from first axis (rows) and position 0 from second axis (cols): &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Elements at position 0 and 1 from first axis (rows) and position 0 and 1 from second axis (cols): &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Elements at position 1 and 2 from first axis (rows) and position 1 and 2 from second axis (cols): &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Elements at position 0 and 1 from first axis (rows) and position 0 from second axis (cols): 
[1 4]

Elements at position 0 and 1 from first axis (rows) and position 0 and 1 from second axis (cols): 
[[1 2]
 [4 5]]

Elements at position 1 and 2 from first axis (rows) and position 1 and 2 from second axis (cols): 
[[5 6]
 [8 9]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As with one dimensional arrays, we can also specify the “stride” to select elements by adding a&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;All elements at position 0 from first axis (all elements from first row) with stride 1: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;All elements at position 0 from first axis (all elements from first row) with stride 2: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;All elements at position 0 from second axis (all elements from first  column) with stride 1: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;All elements at position 0 from second axis (all elements from first  column) with stride 2: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;All elements at position 0 from first axis (all elements from first row) with stride 1: 
[1 2 3]

All elements at position 0 from first axis (all elements from first row) with stride 2: 
[1 3]

All elements at position 0 from second axis (all elements from first  column) with stride 1: 
[1 4 7]

All elements at position 0 from second axis (all elements from first  column) with stride 2: 
[1 7]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To select elements from three-dimensional arrays you follow the same logic. Recall that the axes (2,2,2), represent 2 arrays with 2 rows and 2 columns each. Hence, the second and third axes represent how “height” and “width” of the two-dimensional arrays, whereas the first index how many of those are “stack” together. Below a couple of examples:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array three as reference: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_three&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;First two-dimensional array:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_three&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Second two-dimensional array:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_three&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;All elements at position 0 from first two-dimensional array (first row first array):&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_three&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;All elements at position 1 from second two-dimensional array (second row second array):&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_three&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array three as reference: 
[[[1 2]
  [3 4]]

 [[5 6]
  [7 8]]]

First two-dimensional array:
[[1 2]
 [3 4]]

Second two-dimensional array:
[[5 6]
 [7 8]]

All elements at position 0 from first two-dimensional array (first row first array):
[1 2]

All elements at position 1 from second two-dimensional array (second row second array):
[7 8]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As I mentioned at the beginning of this section, inserting, deleting, and changing values is done with the same logic.&lt;/p&gt;

&lt;p&gt;Below a couple of examples inserting constant values:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array x:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;33&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Insert a 33 at position 2: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;728&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Insert a 728 between positions 6 and -1: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array x:[0 1 2 3 4 5 6 7 8 9]

Insert a 33 at position 2: [ 0  1 33  3  4  5  6  7  8  9]

Insert a 728 between positions 6 and -1: [  0   1  33   3   4   5 728 728 728   9]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can insert ranges of values or sub-arrays as long as is shape-consistent:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;105&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Insert values [100 101 102 103 104] between positions -6 and -1:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Insert values [100 101 102 103 104] between positions -6 and -1:

[  0   1   2   3 100 101 102 103 104   9]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can change specific values by specifying their index position:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;27&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Multiply by 27 values from position [2] to position [5]:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Multiply by 27 values from position [2] to position [5]:

[  0   1  54  81 108   5   6   7   8   9]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;boolean-or-mask-indexing&quot;&gt;Boolean or Mask indexing&lt;/h3&gt;

&lt;p&gt;Boolean arrays, or arrays made of &lt;strong&gt;True&lt;/strong&gt; and/or &lt;strong&gt;False&lt;/strong&gt; values, can be used to index elements from other arrays. This type of indexing is also known as &lt;strong&gt;“mask” indexing&lt;/strong&gt;. To work, the Boolean array must be of the same shape as the array to be indexed.&lt;/p&gt;

&lt;p&gt;Here is a mental image you can use: imagine you have two arrays, one with the numbers from 1 to 10, and a Boolean array with “True” in the even positions and “False” in the odd positions. Now, imagine the squares with “True” are transparent whereas the ones with “False” are opaque. If you overlay the Boolean array on top of the regular array, the numbers in even positions will be visible but not the ones in odd positions. This is analogous to wearing a mask on your face: only the regions with “holes” or transparent will be visible, typically your eyes. This is why (according to me!) Boolean array is called “mask” indexing. The image below exemplifies the process:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-12/boolean_index.svg&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Mask or Boolean arrayw with &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; for values strictly grater than 6: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Return an sub-array where &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; elements are &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array: 
[ 0  1  2  3  4  5  6  7  8  9 10 11]

Mask or Boolean arrayw with &quot;True&quot; for values strictly grater than 6: 
[False False False False False False False  True  True  True  True  True]

Return an sub-array where &quot;mask&quot; elements are &quot;True&quot;: 
[ 7  8  9 10 11]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can as many conditions to create Boolean arrays as we desire. The syntax is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[(firs-condition) &amp;amp; (second-condition) &amp;amp; ... (last-condition)]&lt;/code&gt;. For instance, to select elements larger than 1 AND smaller than 5, we do:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Boolean array with elements larger than 1 and smaller than 5:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Select elements larger than 1 and smaller than 5 from 1-12:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Boolean array with elements larger than 1 and smaller than 5:
[array([False, False,  True,  True,  True, False, False, False, False,
       False, False, False])]

Select elements larger than 1 and smaller than 5 from 1-12:
[2 3 4]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can combine multiple logic, comparison, and identity operators to create complex Boolean arrays (see &lt;a href=&quot;https://www.w3schools.com/python/python_operators.asp&quot;&gt;here&lt;/a&gt;). For instance:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Select elements equal to 2 OR larger than 9 :&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Select even elements (modulo == 0) OR larger than 9 :&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Select elements that are NOT 2 and NOT 7 and NOT 9 :&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Select elements equal to 2 OR larger than 9 :
[ 2 10 11]

Select even elements (modulo == 0) OR larger than 9 :
[ 0  2  4  6  8 10 11]

Select elements that are NOT 2 and NOT 7 and NOT 9 :
[ 0  1  3  4  5  6  8  9 11] 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This type of indexing comes in handy for &lt;strong&gt;conditional selection or modification&lt;/strong&gt; of array elements. I use it all the time when I need &lt;strong&gt;to subset datasets by any attribute(s)&lt;/strong&gt;: people older than 65, income lower than 1,000 and higher than a 100,000, scores in between 50-80, states larger than 5,000 of inhabitants, males with diabetes or chronic kidney disease, and so on.&lt;/p&gt;

&lt;p&gt;Boolean indexing in &lt;strong&gt;multidimensional arrays&lt;/strong&gt; is no different. You just need to pay attention to match the dimensionality of the “mask” and the array to be indexed, &lt;em&gt;in at least one of the dimensions&lt;/em&gt;. For instance, if you tried to index a two-dimensional array with a one-dimensional boolean array, it won’t work. Same if the array has  the same dimensions/axes, but different shape, i.e., numbers of elements along each axis.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;17&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;mask_two&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Two-dimensional array: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Mask or Boolean arrayw with &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; for values strictly greater than 8: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mask_two&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Return an sub-array where &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; elements are &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mask_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Shape new sub-array: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mask_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Two-dimensional array: 
[[ 1  2  3  4]
 [ 5  6  7  8]
 [ 9 10 11 12]
 [13 14 15 16]]

Mask or Boolean arrayw with &quot;True&quot; for values strictly greater than 8: 
[[False False False False]
 [False False False False]
 [ True  True  True  True]
 [ True  True  True  True]]

Return an sub-array where &quot;mask&quot; elements are &quot;True&quot;: 
[ 9 10 11 12 13 14 15 16]

Shape new sub-array: (8,)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Notice that the returned sub-array was flattened to a one-dimensional array with 8 elements. If you think about it, this makes sense as the “mask” we used to index values was one-dimensional in the first place.&lt;/p&gt;

&lt;p&gt;I also mentioned that array shapes need to match &lt;strong&gt;in at least one of the dimensions&lt;/strong&gt;. This means that a (4, 4) array can be indexed by a boolean array that matches its shape in either the first axis (rows), second axis (columns), or both. In practice, this implies we can index an (n , n) array with an (n, n) mask, a  (, n) mask, or an (n, ) mask. Otherwise, it won’t work. Let’s see some examples:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;mask_three&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;mask_four&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;


&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Two-dimensional array: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Select elements at position 1 and 4 along the first axis (first and last rows): &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mask_three&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Select elements at position 2 and 3 along the second axis (second and third cols): &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mask_four&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Two-dimensional array: 
[[ 1  2  3  4]
 [ 5  6  7  8]
 [ 9 10 11 12]
 [13 14 15 16]]

Select elements at position 1 and 4 along the first axis (first and last rows): 
[[ 1  2  3  4]
 [13 14 15 16]]

Select elements at position 2 and 3 along the second axis (second and third cols): 
[[ 2  3]
 [ 6  7]
 [10 11]
 [14 15]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;indexing-like-numpy-functions&quot;&gt;Indexing-like NumPy functions&lt;/h3&gt;

&lt;p&gt;There are several &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; functions design to reference or extract elements in arrays (see &lt;a href=&quot;https://numpy.org/doc/stable/reference/routines.indexing.html#indexing-like-operations&quot;&gt;here&lt;/a&gt;). In general, these are things you can accomplish with basic indexing notation, but that can get pretty complicated to craft.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;take&lt;/code&gt; function will “take” elements along some axis given some indices. It’s simply like pointing and choosing: “hey, I want elements at positions 0, 2 -1 from the rows (or cols, or flattened array)”.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;One-dimensional array:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Two-dimensional array:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# take
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Take elements at positions 0, 2 and -1 from one-dim array:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;take&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Take elements at positions 0, 2 and -1 from one-dim array along the first (row) axis:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;take&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Take elements at positions 0, 2 and -1 from one-dim array along the second (col) axis:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;take&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;One-dimensional array:
[ 0  1  2  3  4  5  6  7  8  9 10 11]

Two-dimensional array:
[[ 1  2  3  4]
 [ 5  6  7  8]
 [ 9 10 11 12]
 [13 14 15 16]]

Take elements at positions 0, 2 and -1 from one-dim array:
[ 0  2 11]

Take elements at positions 0, 2 and -1 from one-dim array along the first (row) axis:
[[ 1  2  3  4]
 [ 9 10 11 12]
 [13 14 15 16]]

Take elements at positions 0, 2 and -1 from one-dim array along the second (col) axis:
[[ 1  3  4]
 [ 5  7  8]
 [ 9 11 12]
 [13 15 16]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;choose&lt;/code&gt; function will return an array given a set of indices and arrays to choose from. The logic’s a bit more complicated. It’s similar to ordering food in a restaurant from different types of food: “The first element of my new array will be the first element of array 3, the second element will be the second element from array 1, the third element will be the third element from array 2, and the fourth element will be the fourth element from array 0”.&lt;/p&gt;

&lt;p&gt;The mechanics are always the same. The flexibility comes from deciding the order from which array you extract elements. This allows for complex indexing like “diagonals” and “staircase”.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# choose
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choices_one&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;choices_diagonal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;choices_diagonal_back&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;


&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Two-dimensional array:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Choose the 1st element from the 4th array, the 2nd from the 1st, the 3th from the 2nd, and the 4th from the 1st: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;choose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choices_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Choose diagonal elements from top-left to botton-right: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;choose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choices_diagonal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Choose diagonal elements from bottom-left to top-right: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;choose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choices_diagonal_back&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Two-dimensional array:
[[ 1  2  3  4]
 [ 5  6  7  8]
 [ 9 10 11 12]
 [13 14 15 16]]

Choose the 1st element from the 4th array, the 2nd from the 1st, the 3th from the 2nd, and the 4th from the 1st: 
[13  6 11  4]

Choose diagonal elements from top-left to botton-right: 
[ 1  6 11 16]

Choose diagonal elements from bottom-left to top-right: 
[13 10  7  4]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;A simpler and more flexible way to extract diagonals is with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;diagonal&lt;/code&gt; function:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Two-dimensional array:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Extract 1st-diagonal diagonal elements from top-left to bottom-right: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;diagonal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;offset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Extract 2st-diagonal diagonal elements from top-left to bottom-right: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;diagonal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;offset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Extract 3st-diagonal diagonal elements from top-left to bottom-right: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;diagonal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;offset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Two-dimensional array:
[[ 1  2  3  4]
 [ 5  6  7  8]
 [ 9 10 11 12]
 [13 14 15 16]]

Extract 1st-diagonal diagonal elements from top-left to bottom-right: 
[ 1  6 11 16]

Extract 2st-diagonal diagonal elements from top-left to bottom-right: 
[ 2  7 12]

Extract 3st-diagonal diagonal elements from top-left to bottom-right: 
[3 8]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To extract the diagonals in the opposite direction, from bottom-right to the top-left, you have to flip the array vertically and then horizontally.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Two-dimensional array&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Vertical flip: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;flipud&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Horizontal flip: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;fliplr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Vertical and horizontal flip: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;fliplr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;flipud&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Extract 1st-diagonal diagonal from bottom-right to top-left: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;diagonal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;fliplr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;flipud&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;offset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Extract 2st-diagonal diagonal from bottom-right to top-left: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;diagonal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;fliplr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;flipud&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;offset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Extract 3st-diagonal diagonal from bottom-right to top-left: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;diagonal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;fliplr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;flipud&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;offset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Two-dimensional array
[[ 1  2  3  4]
 [ 5  6  7  8]
 [ 9 10 11 12]
 [13 14 15 16]]

Vertical flip: 
[[13 14 15 16]
 [ 9 10 11 12]
 [ 5  6  7  8]
 [ 1  2  3  4]]

Horizontal flip: 
[[ 4  3  2  1]
 [ 8  7  6  5]
 [12 11 10  9]
 [16 15 14 13]]

Vertical and horizontal flip: 
[[16 15 14 13]
 [12 11 10  9]
 [ 8  7  6  5]
 [ 4  3  2  1]]

Extract 1st-diagonal diagonal from bottom-right to top-left: 
[16 11  6  1]

Extract 2st-diagonal diagonal from bottom-right to top-left: 
[15 10  5]

Extract 3st-diagonal diagonal from bottom-right to top-left: 
[14  9]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To obtain the opposite diagonal or “anti-diagonal” from the top-right to the bottom-left (and its reverse):&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Two-dimensional array: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Antidiagonal: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;diagonal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;fliplr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;offset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Antidiagonal from bottom-left to top-right: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;diagonal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;flipud&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;offset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Two-dimensional array: 
[[ 1  2  3  4]
 [ 5  6  7  8]
 [ 9 10 11 12]
 [13 14 15 16]]

Antidiagonal: 
[ 4  7 10 13]

Antidiagonal from bottom-left to top-right: 
[13 10  7  4]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;select&lt;/code&gt; function allows for combining multiple conditions to choose elements from multiple arrays. The output will be the elements where the conditions are evaluated as True, and the rest will be set to 0 or to a user-defined default value.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;condlist&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;choicelist&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Select elements from x where x&amp;lt;3, and elements from y where x&amp;gt;5: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;select&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;condlist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;choicelist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Select elements from x where x&amp;lt;3, and elements from y where x&amp;gt;5, with default value 99 for False: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;select&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;condlist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;choicelist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;default&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;99&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Select elements from x where x&amp;lt;3, and elements from y where x&amp;gt;5: 
[ 0  1  2  0  0  0  7  8  9 10]

Select elements from x where x&amp;lt;3, and elements from y where x&amp;gt;5, with default value 99 for False: 
[ 0  1  2 99 99 99  7  8  9 10]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you want to return only the values where a condition is True, a simple approach is to subset the array to the non-zero values as (or any value you defined as default):&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;non_zero&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;select&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;condlist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;choicelist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;non_nine&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;select&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;condlist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;choicelist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;default&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;99&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Select non-zero value: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;non_zero&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;non_zero&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Select non-ninety-nine values (This option prevents you to remove zeros when zero is a valid value):&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;non_nine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;non_nine&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;99&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Select non-zero value: 
[ 1  2  7  8  9 10]

Select non-ninety-nine values (This option prevents you to remove zeros when zero is a valid value):
[ 0  1  2  7  8  9 10]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;array-iteration&quot;&gt;Array iteration&lt;/h2&gt;

&lt;p&gt;Iterating over arrays refers to the operation of “visiting” elements of an array in a systematic fashion. To iterate over array elements we utilize standard Python syntax (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;for&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;while&lt;/code&gt; loops) plus the functionality provided by the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nditer&lt;/code&gt; iterator object.&lt;/p&gt;

&lt;p&gt;“OK, so iteration is looping. Wasn’t looping bad though?” Right, I said that. Although is true that you want to avoid explicit Python loops as the plague, there are circumstances where iteration is unavoidable, so you better learn how to do it properly.&lt;/p&gt;

&lt;p&gt;I’ll say upfront that array iteration can be a very complex topic. I rarely have had to use this functionality as most libraries will take care of these issues for you. Here I’ll just cover the most basics topics related to array iteration in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; you have a good enough notion of what is going on and what are your options.&lt;/p&gt;

&lt;h3 id=&quot;basic-array-iteration&quot;&gt;Basic array iteration&lt;/h3&gt;

&lt;p&gt;Here is an example of the most basic operation you can do: iterate over array elements one by one.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Two-dimensional array:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;element&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;nditer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;element&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Two-dimensional array:
[[1 2 3]
 [4 5 6]
 [7 8 9]]

1 2 3 4 5 6 7 8 9 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There is a technical detail you should be aware of: the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nditer&lt;/code&gt; object iterates over the array matching the way on which the data is stored in memory. This means that regardless of how you “present” the array to the iterator, you will get back the elements in the same order. This done simply because is faster. For instance:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Transposed (along main diagonal) two-dimensional array:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;element&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;nditer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;element&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Transposed (along main diagonal) two-dimensional array:
[[1 4 7]
 [2 5 8]
 [3 6 9]]

1 2 3 4 5 6 7 8 9 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you were expecting to get back 1, 4, 7, 2, 5, 8, 3, 6, 9, you are not alone. I was expecting that order too. But that is not the order data is stored in memory. To visit the elements in the order you would expect by looking at the array, you have to explicitly add you want the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;C&lt;/code&gt; order:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Transposed (along main diagonal) two-dimensional array:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;element&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;nditer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;order&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;element&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Transposed (along main diagonal) two-dimensional array:
[[1 4 7]
 [2 5 8]
 [3 6 9]]

1 4 7 2 5 8 3 6 9 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There may be times when you want to update the values of an array while iterating. For instance, reinforcement learning models constantly update values after each iteration. The default behavior of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nditer&lt;/code&gt; is “read-only”, meaning it won’t let you change values. Hence, you have to specify either “readwrite” or “writeonly” options to update values. Additionally, you have to signal to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nditer&lt;/code&gt; when you have finished iterating over values, as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nditer&lt;/code&gt; needs to update the array with the new values. This happens because &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nditer&lt;/code&gt; first save the updated values in a temporary memory space instead of changing values “on the fly”. According to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; docs, there are two ways to do this:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nditer&lt;/code&gt; as a context manager utilizing the Python &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;with&lt;/code&gt; statement&lt;/li&gt;
  &lt;li&gt;calling the iterator’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;close&lt;/code&gt; method at the end of the iteration process&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let’s see an example:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;array_to_update&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array to update:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_to_update&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;nditer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_to_update&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;op_flags&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;readwrite&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iterator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;element&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iterator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;element&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[...]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;element&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
        
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Updated array (squared):&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_to_update&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array to update:
[[1 2 3]
 [4 5 6]
 [7 8 9]]

Updated array (squared):
[[ 1  4  9]
 [16 25 36]
 [49 64 81]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Although looping is not completely avoidable while iterating, there is a way to speed up iteration by partially vectorizing the innermost loop of the iteration. The details of how this work is not relevant from an applied perspective. What you want to know is that it will be faster and that you have to declare the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;external_loop&lt;/code&gt; flag in the iterator. Let’s time both approaches:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;%%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timeit&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;large_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;element&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;nditer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;large_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;element&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;10.3 ms ± 107 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;%%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timeit&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;large_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;element&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;nditer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;large_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flags&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;external_loop&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;element&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;12.6 µs ± 51.2 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;By utilizing an external loop, we increased the speed for a factor of around 816 times (0.0125 ms vs 10.2 ms). Not all operations will gain so much on speed, but in general, you will get large gains.&lt;/p&gt;

&lt;h3 id=&quot;broadcasting-array-iteration&quot;&gt;Broadcasting array iteration&lt;/h3&gt;

&lt;p&gt;If you find yourself having to iterate over multiple arrays with different shapes and dimensionality, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nditer&lt;/code&gt; object is smart enough to apply broadcast rules during iteration. For instance:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;one_dim_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;two_dim_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;nditer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;one_dim_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;two_dim_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt;

    &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;x: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;, y:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;x: 0, y:0
x: 1, y:1
x: 2, y:2
x: 0, y:3
x: 1, y:4
x: 2, y:5
x: 0, y:6
x: 1, y:7
x: 2, y:8
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here we can see that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nditer&lt;/code&gt; broadcast the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;one_dim_array&lt;/code&gt; to match the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;two_dim_array&lt;/code&gt; so iteration does not break.&lt;/p&gt;

&lt;h3 id=&quot;allocating-outputs-from-iteration&quot;&gt;Allocating outputs from iteration&lt;/h3&gt;

&lt;p&gt;There are cases where you want to create a function utilizing the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nditer&lt;/code&gt; object. This is, functions that take an array as input, iterate over the array elements, and instead of modifying the original array, they return the output somewhere else. Here is a basic example I took from &lt;a href=&quot;https://numpy.org/doc/stable/reference/arrays.nditer.html#iterator-allocated-output-arrays&quot;&gt;the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; docs&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;square&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;nditer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

            &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[...]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;operands&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;input_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Input array: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_array&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Input array squared: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;square&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Input array: [1 2 3 4 5]
Input array squared: [ 1  4  9 16 25]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;iteration-functions&quot;&gt;Iteration functions&lt;/h3&gt;

&lt;p&gt;There are a couple of additional functions in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; in addition to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nditer&lt;/code&gt; object that you can check &lt;a href=&quot;https://numpy.org/doc/stable/reference/routines.indexing.html#iterating-over-arrays&quot;&gt;here&lt;/a&gt;. An example is the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ndenumerate&lt;/code&gt; that returns both the coordinate index values for each element in the array, plus the element itself:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;element&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ndenumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Pair of indices: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;, Element: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;element&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Pair of indices: (0, 0), Element: 1
Pair of indices: (0, 1), Element: 2
Pair of indices: (0, 2), Element: 3
Pair of indices: (1, 0), Element: 4
Pair of indices: (1, 1), Element: 5
Pair of indices: (1, 2), Element: 6
Pair of indices: (2, 0), Element: 7
Pair of indices: (2, 1), Element: 8
Pair of indices: (2, 2), Element: 9
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;array-shallow-and-deep-copies&quot;&gt;Array shallow and deep copies&lt;/h2&gt;

&lt;p&gt;To copy an array in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; can mean &lt;strong&gt;three different things&lt;/strong&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;To put a new label&lt;/li&gt;
  &lt;li&gt;To create a “view” or “shallow copy” that refers to the same chunk of data in memory&lt;/li&gt;
  &lt;li&gt;To create an independent or “deep copy” of the array in a different location in memory&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Creating “shallow copies” instead of “deep copies” can significantly speed up computation and save space, but it has limitations to keep in mind. The figure below illustrates the differences between the three alternatives.&lt;/p&gt;

&lt;p&gt;Let’s examine each case.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-12/copies.svg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;array-new-label&quot;&gt;Array new label&lt;/h3&gt;

&lt;p&gt;There are cases where no copy at all is created. All that is done is attaching a new label to the original array. We can use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;is&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;may_share_memory&lt;/code&gt; methods to check object identity and memory sharing.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array one:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array two:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Are array-one and array-two the same object?: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Do array-one and array-two share memory?: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;may_share_memory&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array one:[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]

Array two:[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]

Are array-one and array-two the same object?: True

Do array-one and array-two share memory?: True
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The key here is that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;array_one&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;array_two&lt;/code&gt; &lt;strong&gt;share memory AND are the same object&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;A logical consequence of the fact that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;array_one&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;array_two&lt;/code&gt; share the same data and identity, is that if you change &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;array_two&lt;/code&gt; you will be inadvertently changing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;array_one&lt;/code&gt; as well:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;99&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;New array-two:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array-one is changed although no explicit operation was done to it:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;New array-two:
[ 1.  1.  1.  1.  1.  1.  1.  1.  1. 99.]

Array-one is changed although no explicit operation was done to it:
[ 1.  1.  1.  1.  1.  1.  1.  1.  1. 99.]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you want two independent copies of the same data, you need a “deep copy” (more on that below).&lt;/p&gt;

&lt;h3 id=&quot;array-shallow-copy-or-view&quot;&gt;Array shallow copy or view&lt;/h3&gt;

&lt;p&gt;“Shallow copies” or “views” are objects which &lt;strong&gt;are not the same&lt;/strong&gt;, but &lt;strong&gt;share the same data&lt;/strong&gt; source. For instance:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;array_three&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array one:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array three:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_three&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Are array-one and array-three the same object?: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_three&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Do array-one and array-three share memory?: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;may_share_memory&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_three&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array one:[ 1.  1.  1.  1.  1.  1.  1.  1.  1. 99.]

Array three:[1. 1. 1. 1. 1. 1.]

Are array-one and array-three the same object?: False

Do array-one and array-three share memory?: True
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The key here is that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;array_one&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;array_three&lt;/code&gt; &lt;strong&gt;are NOT the same object BUT share memory&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The effect of shallow copies in the base-data is a bit trickier. There are cases where changing a view does not change the base-data, as reshaping:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;array_three_reshape&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_three&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array-three change shape:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_three_reshape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;But this does not change array-two (the source):&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array-three change shape:
[[1. 1. 1.]
 [1. 1. 1.]]

But this does not change array-two (the source):
[ 1.  1.  1.  1.  1.  1.  1.  1.  1. 99.]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And cases where it does change the original base-data, as inserting new values:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;array_three&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;99&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array-three new value at position 0:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_three&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;This does change array-two (the source):&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_two&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array-three new value at position 0:
[99.  1.  1.  1.  1.  1.]

This does change array-two (the source):
[99.  1.  1.  1.  1.  1.  1.  1.  1. 99.]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;array-deep-copy&quot;&gt;Array deep copy&lt;/h3&gt;

&lt;p&gt;“Deep copies” are independent copies located in a different position in memory. For instance:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;array_four&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array one:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array four:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_four&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Are array-one and array-four the same object?: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_four&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Do array-one and array-four share memory?: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;may_share_memory&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_four&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array one:[99.  1.  1.  1.  1.  1.  1.  1.  1. 99.]

Array four:[99.  1.  1.  1.  1.]

Are array-one and array-four the same object?: False

Do array-one and array-four share memory?: False
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The key here is that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;array_one&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;array_four&lt;/code&gt; &lt;strong&gt;NEITHER are the same object NOR share memory&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;In this case, there is no way to affect &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;array_one&lt;/code&gt; (the base) by changing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;array_four&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;array_four&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;99&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Array-four new value at position 3:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_four&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;This does NOT change array-one (the source):&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_one&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Array-four new value at position 3:
[99.  1.  1. 99.  1.]

This does NOT change array-one (the source):
[99.  1.  1.  1.  1.  1.  1.  1.  1. 99.]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Deep copies are recommended when you want to keep a subset of the data and throw away the base array, or when you need to manipulate two or more copies of the same data independently.&lt;/p&gt;

&lt;h2 id=&quot;structured-arrays&quot;&gt;Structured arrays&lt;/h2&gt;

&lt;p&gt;My only goal introducing structured arrays is to advise to not use them unless you need to interface with C code or to do low-level manipulation of structured buffers (As recommended in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; docs). If you need to do such kinds of things you are probably a very advance &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; user or developer, i.e., you won’t read this anyway.&lt;/p&gt;

&lt;p&gt;Let’s look at what structured arrays are:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;structured_array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Bulbasaur&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Grass&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;15.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;71.12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                             &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Charmander &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Fire&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;18.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;60.96&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)],&lt;/span&gt;
                           &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Name&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;U10&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                                  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Type&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;U10&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                                  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Weight&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f4&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                                  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Height&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f4&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Structured array:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;structured_array&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;First element structured array:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;structured_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Second element structured array:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;structured_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Structured array:
[(&apos;Bulbasaur&apos;, &apos;Grass&apos;, 15.2, 71.12) (&apos;Charmander&apos;, &apos;Fire&apos;, 18.7, 60.96)]

First element structured array:
(&apos;Bulbasaur&apos;, &apos;Grass&apos;, 15.2, 71.12)

Second element structured array:
(&apos;Charmander&apos;, &apos;Fire&apos;, 18.7, 60.96)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;From the example you can gather than structured arrays are n-dimensional arrays composed of mixed data types with named fields. For instance, the first element has four fields: a string for “Name”, a string for “Type”, a float for “Weight”, and a float for “height”. Essentially, the kind of data you would find in a CSV file or a relational database.&lt;/p&gt;

&lt;p&gt;What to use then if not structured arrays? &lt;a href=&quot;https://pandas.pydata.org/&quot;&gt;Pandas&lt;/a&gt;, just use 🐼 which is specifically designed to deal with table-like datasets with mixed data types. Alternatives are &lt;a href=&quot;http://xarray.pydata.org/en/stable/&quot;&gt;xarray&lt;/a&gt; and or query languages like &lt;a href=&quot;https://www.postgresql.org/&quot;&gt;PostgreSQL&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;random-number-generation-and-sampling-with-numpy&quot;&gt;Random number generation and sampling with NumPy&lt;/h2&gt;

&lt;p&gt;Here is an example of something that happened to me: I wrote an on-line book introducing neural network models of cognition. While creating examples, I often had to generate random numbers, particularly to initialize the weights of the network. They were cases where sampling from a &lt;em&gt;uniform random distribution&lt;/em&gt; vs &lt;em&gt;random normal distribution&lt;/em&gt; was &lt;strong&gt;the difference between a model solving the problem and not solving it at all&lt;/strong&gt;. As you can imagine, knowing how to work with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; random generator capabilities is crucial to get such kind of issues right.&lt;/p&gt;

&lt;h3 id=&quot;random-sampling-updated&quot;&gt;Random sampling updated&lt;/h3&gt;

&lt;p&gt;This or may not be a surprise to you, but &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Numpy&lt;/code&gt; does not actually generate random numbers but &lt;strong&gt;pseudo-random numbers&lt;/strong&gt; basically because generating random numbers is impossible. Just trying out, you won’t be able to, because you will always depend on picking some non-random event to generate the sequence. But worry not: for all &lt;strong&gt;practical purposes&lt;/strong&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; random number generator is “random enough” such that you can use it as if it were “truly random”.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; random generator capabilities were updated on version 1.17.0, meaning that you will probably found outdated ways to use the random number generator online, something like:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;random_numbers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;standard_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;According to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; documentation, this is not the recommended way. To generate a sequence of random numbers sampled from a standard normal distribution use:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy.random&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;default_rng&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rng&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;default_rng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;random_numbers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;standard_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Random numbers sequence sampled from a normal distributon:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random_numbers&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Random numbers sequence sampled from a normal distributon:
[ 1.28412127 -0.59084961  1.19645635 -1.43902792 -1.16416342]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To explore the difference between the “old” and “new way” to generate random numbers in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;  see &lt;a href=&quot;https://numpy.org/doc/stable/reference/random/new-or-different.html#new-or-different&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;basic-random-sampling&quot;&gt;Basic random sampling&lt;/h3&gt;

&lt;p&gt;The three main methods to generate random numbers are &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;integers&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;random&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;choice&lt;/code&gt;. The first generates random integers, the second floats, and the third a uniform random sample from a one-dimensional array. Let’s see them in action.&lt;/p&gt;

&lt;p&gt;For the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;integers&lt;/code&gt; method, you need to pass at least one argument indicating the ceiling to be considered:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;A random integer between [1, 10) (10 non-inclusive): &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;integers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;5 random integers between [1, 10) (10 non-inclusive): &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;integers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;A random integer between [1, 10) (10 non-inclusive): 
8

5 random integers between [1, 10) (10 non-inclusive): 
[9 3 3 5 8]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;random&lt;/code&gt; method is more flexible, as it allows to specify a tuple with the shape of the expected array of random numbers:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;A random float between [0.0, 1.0) (1.0 non-inclusive): &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;3 random floats between [0.0, 1.0) (1.0 non-inclusive): &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,))&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;3,3 random floats between [0.0, 1.0) (1.0 non-inclusive): &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;3,3,3 random floats between [0.0, 1.0) (1.0 non-inclusive): &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;A random float between [0.0, 1.0) (1.0 non-inclusive): 
0.6419758158608625

3 random floats between [0.0, 1.0) (1.0 non-inclusive): 
[0.42937336 0.52477446 0.02976526]

3,3 random floats between [0.0, 1.0) (1.0 non-inclusive): 
[[0.3604225  0.88741889 0.07464158]
 [0.36458258 0.75476422 0.26216883]
 [0.69558381 0.49518423 0.77079096]]

3,3,3 random floats between [0.0, 1.0) (1.0 non-inclusive): 
[[[0.23506917 0.38961231 0.07247969]
  [0.16011855 0.7030183  0.86692858]
  [0.00326837 0.34421767 0.05739803]]

 [[0.05635481 0.38558642 0.86025178]
  [0.83572317 0.96456634 0.28242747]
  [0.35763072 0.81671697 0.62012315]]

 [[0.5108489  0.65798614 0.07344178]
  [0.89918787 0.8241409  0.30456018]
  [0.96989985 0.55320836 0.61100954]]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;choice&lt;/code&gt; method needs a one-dimensional array as argument to work:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;A random number from an array [1, 10] (inclusive): &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;5 random numbers from an array [1, 10] (inclusive): &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;A random number from an array [1, 10] (inclusive): 
1

5 random numbers from an array [1, 10] (inclusive): 
[2 7 2 1 6]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There are a couple of additional options for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;choice&lt;/code&gt; method to keep in mind:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;The &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;p=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; argument indicates the weight for each element of the sample space&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;A non-uniform sample of random numbers from an array [1, 10] (inclusive): &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;The previous examples were sampling with replacement. We can sample without replacement as well:&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Uniform sample of random numbers from an array [1, 10] (inclusive): &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;The &apos;p=&apos; argument indicates the weight for each element of the sample space
A non-uniform sample of random numbers from an array [1, 10] (inclusive): 
[9 9 3 3 3]


The previous examples were sampling with replacement. We can sample without replacement as well:
Uniform sample of random numbers from an array [1, 10] (inclusive): 
[2 8 3 6 9]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;setting-a-seed-for-reproducibility&quot;&gt;Setting a seed for reproducibility&lt;/h3&gt;

&lt;p&gt;In the previous examples, there is no way to reproduce the numbers we generated because we didn’t specify a “seed”. “&lt;em&gt;Wait, we are generating random numbers, why on earth I would want to repeat the same random number?&lt;/em&gt;”. To allow others to reproduce your results. In Data Science and Machine Learning different starting points (seeds) may lead to widely different results. There is a joke going around the Internet saying that the “seed” of a random number generator is another parameter of the model to be adjusted, and indeed, it is.&lt;/p&gt;

&lt;p&gt;Setting a seed for generating reproducible random numbers is simple:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy.random&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;default_rng&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;rg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;default_rng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9320&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;integers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;integers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;integers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[0 2 1 0 4]
[2 2 2 3 1]
[3 1 1 0 1]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, to get the same sequence of random integers, we just need to use the same seed:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;rg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;default_rng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9320&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Numbers are the same as in the example above:&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;integers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;integers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;integers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Numbers are the same as in the example above:
[0 2 1 0 4]
[2 2 2 3 1]
[3 1 1 0 1]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you don’t set the seed, you will get different values after every run:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;rg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;default_rng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; 

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;This time numbers will change at random:&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;integers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;integers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;integers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;This time numbers will change at random:
[4 1 4 0 1]
[0 3 3 2 0]
[3 2 4 4 2]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Again:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;rg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;default_rng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; 

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;integers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;integers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;integers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[1 0 3 4 4]
[3 0 1 3 4]
[4 0 3 3 3]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;sampling-from-particular-distributions&quot;&gt;Sampling from particular distributions&lt;/h3&gt;

&lt;p&gt;If you are familiar with probability theory and statistics, you probably know you can sample at random from a wide variety of distributions other than uniform and normal. Luckily, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; provides many options to chose from (37 last time I check!). I’ll just illustrate a couple, so check the documentation &lt;a href=&quot;https://numpy.org/doc/stable/reference/random/generator.html#distributions&quot;&gt;here&lt;/a&gt; to learn more about the other options.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;matplotlib.pylab&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;style&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;use&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;dark_background&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;InlineBackend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure_format&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;retina&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# to get high resolution images
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Sampling from a &lt;strong&gt;binomial&lt;/strong&gt; distribution example:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;binomial_fair&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;binomial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;binomial_bias&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;binomial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ax1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;subplots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;constrained_layout&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;suptitle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Sampling from binomial distribution&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;binomial_fair&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;set_title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;50/50 chance&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;binomial_bias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;set_title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;20/80 chance&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-12/output_372_0.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sampling from a &lt;strong&gt;chisquare&lt;/strong&gt; distribution example:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;chisquare1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;chisquare&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;chisquare2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;chisquare&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ax1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;subplots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;constrained_layout&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;suptitle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Sampling from chisquare distribution&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chisquare1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;set_title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;5 degrees of freedom&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chisquare2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;set_title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;50 degrees of freedom&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-12/output_375_0.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sampling from a &lt;strong&gt;poisson&lt;/strong&gt; distribution example:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;poisson1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;poisson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;poisson2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;poisson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ax1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;subplots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;constrained_layout&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;suptitle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Sampling from poisson distribution&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;poisson1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;set_title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Expectation of interval: 5&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;poisson2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;set_title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Expectation of interval: 50&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-12/output_378_0.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;basic-statistics-with-numpy&quot;&gt;Basic statistics with NumPy&lt;/h2&gt;

&lt;p&gt;Although &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; is not a library for statistical analysis, it does provide several descriptive statistics functions. In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; documentation these are presented as “order”, “average and variances”, “correlating” and “histograms”, but all of those are just descriptive statistics. Also, keep in mind that pretty much any statistical package in Python you’d find around is based in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; as its “engine” anyways.&lt;/p&gt;

&lt;p&gt;There are just too many statistical functions to explore them all (see &lt;a href=&quot;https://numpy.org/doc/stable/reference/routines.statistics.html&quot;&gt;here&lt;/a&gt;), so I’ll focus my attention on the most common ones.&lt;/p&gt;

&lt;h3 id=&quot;measures-of-central-tendency&quot;&gt;Measures of central tendency&lt;/h3&gt;

&lt;p&gt;Measures of central tendency are indicators of the center or typical value of data distributions. Let’s check the most common ones:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Arithmetic mean (or simply mean) of poisson distribution: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;poisson1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Arithmetic mean (or simply mean) of chisquare distribution: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chisquare1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Median of poisson distribution: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;median&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;poisson1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Median of chisquare distribution: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;median&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chisquare1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      
      
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Weighted average of poisson distribution: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;average&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;poisson1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;flip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;poisson1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Weighted average of chisquare distribution: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;average&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chisquare1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;flip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chisquare1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Arithmetic mean (or simply mean) of poisson distribution: 5.015
Arithmetic mean (or simply mean) of chisquare distribution: 4.89668496863773

Median of poisson distribution: 5.0
Median of chisquare distribution: 4.212365231904483

Weighted average of poisson distribution: 5.030109670987039
Weighted average of chisquare distribution: 4.931798987029843
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;measures-of-dispersion&quot;&gt;Measures of dispersion&lt;/h3&gt;

&lt;p&gt;Measures of dispersion are indicators of the extent to which data distributions are stretched or squeezed. Let’s check the most common ones:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Standard deviation of poisson distribution: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;poisson1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Standard deviation of chisquare distribution: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chisquare1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Variance of poisson distribution: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;poisson1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Variance of chisquare distribution: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chisquare1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Range of values of poisson distribution: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ptp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;poisson1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Range of values of chisquare distribution: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ptp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chisquare1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Min and max of poisson distribution: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;amin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;poisson1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;amax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;poisson1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Min and max of chisquare distribution: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;amin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chisquare1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;amax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;poisson1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Percentile 50th of poisson distribution: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;percentile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;poisson1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Percentile 50th of chisquare distribution: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;percentile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chisquare1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Standard deviation of poisson distribution: 2.322665494641878
Standard deviation of chisquare distribution: 3.2006656960017534

Variance of poisson distribution: 5.394774999999999
Variance of chisquare distribution: 10.24426089756239

Range of values of poisson distribution: 14
Range of values of chisquare distribution: 23.067229474073194

Min and max of poisson distribution: (0, 14)
Min and max of chisquare distribution: (0.15474240923106727, 14)

Percentile 50th of poisson distribution: 5.0
Percentile 50th of chisquare distribution: 4.212365231904483
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;meausres-of-correlation&quot;&gt;Meausres of correlation&lt;/h3&gt;

&lt;p&gt;Measures of correlation are indicators of the extent and how two or more variables are related to each other (regardless of causality). Let’s check the most common ones:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;rand_matrix&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Pearson product-moment correlation coefficient:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;corrcoef&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;poisson1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;poisson2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Cross-correlation coefficient:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;correlate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;poisson1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;poisson2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Covariance matrix coefficients:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;cov&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;poisson1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;poisson2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Pearson product-moment correlation coefficient:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;corrcoef&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Covariance matrix coefficients:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;cov&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Pearson product-moment correlation coefficient:
[[ 1.         -0.03087462]
 [-0.03087462  1.        ]]

Cross-correlation coefficient:
[249451]

Covariance matrix coefficients:
[[ 5.40017518 -0.53725726]
 [-0.53725726 56.07296897]]

Pearson product-moment correlation coefficient:
[[ 1.         -0.91426916 -0.37348227  0.47770458 -0.17469488]
 [-0.91426916  1.          0.34706955 -0.63275776 -0.14893734]
 [-0.37348227  0.34706955  1.         -0.80069146 -0.14583296]
 [ 0.47770458 -0.63275776 -0.80069146  1.          0.61109471]
 [-0.17469488 -0.14893734 -0.14583296  0.61109471  1.        ]]

Covariance matrix coefficients:
[[ 0.08273057 -0.04727347 -0.03477924  0.04340413 -0.01916139]
 [-0.04727347  0.03231623  0.02019965 -0.03593242 -0.01021005]
 [-0.03477924  0.02019965  0.10481758 -0.08188813 -0.01800473]
 [ 0.04340413 -0.03593242 -0.08188813  0.09978777  0.07361411]
 [-0.01916139 -0.01021005 -0.01800473  0.07361411  0.14542122]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;histograms&quot;&gt;Histograms&lt;/h3&gt;

&lt;p&gt;Finally, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; also offers some convinient functions to compute histograms:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Histogram poisson distribution:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;histogram&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;poisson1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Histogram chisquare distribution:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;histogram&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chisquare1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Histogram poisson distribution with 4 bins:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;histogram&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;poisson1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Histogram chisquare distribution with 4 bins:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;histogram&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chisquare1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bins&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Histogram poisson distribution:
(array([ 44,  91, 320, 168, 124, 176,  40,  28,   6,   3]), array([ 0. ,  1.4,  2.8,  4.2,  5.6,  7. ,  8.4,  9.8, 11.2, 12.6, 14. ]))

Histogram chisquare distribution:
(array([243, 329, 212, 128,  53,  20,  10,   1,   2,   2]), array([ 0.15474241,  2.46146536,  4.7681883 ,  7.07491125,  9.3816342 ,
       11.68835715, 13.99508009, 16.30180304, 18.60852599, 20.91524894,
       23.22197188]))

Histogram poisson distribution with 4 bins:
(array([ 11,  33, 215]), array([0, 1, 2, 3]))

Histogram chisquare distribution with 4 bins:
(array([ 41, 129, 153]), array([0, 1, 2, 3]))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;basic-linear-algebra-with-numpy&quot;&gt;Basic linear algebra with NumPy&lt;/h2&gt;

&lt;p&gt;Linear algebra is a subject where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; shines as an array-like numerical computing library. Much of machine learning and data science is applied linear algebra and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; is the (for the most part) perfect tool for that. Since I already wrote a ~20,000  article on linear algebra with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; and Python (check it out &lt;a href=&quot;https://pabloinsente.github.io/intro-linear-algebra&quot;&gt;here&lt;/a&gt;) I focus only in a couple of methods I find more important to be aware of. This also means that I won’t spend time explaining what each operation is, just how to compute it with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;basic-vector-operations&quot;&gt;Basic vector operations&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Vector x: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;, vector y: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Vector addition: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Vector scalar-multiplication: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Linear combinations of vectors: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Vector-vector multiplication: dot product: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Vector x: [0 1 2], vector y: [4 5 6]

Vector addition: [4 6 8]

Vector scalar-multiplication: [0 2 4]

Linear combinations of vectors: [12 17 22]

Vector-vector multiplication: dot product: 17
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;basic-matrix-operations&quot;&gt;Basic matrix operations&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Matrix A:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Matrix B:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Matrix-matrix addition:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Matrix-scalar multiplication:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Matrix-vector multiplication: dot product:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Matrix-matrix multiplication: dot product:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Matrix inverse:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Matrix transpose:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Hadamard product: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Matrix A:
[[1 2 3]
 [4 5 6]
 [7 8 9]]

Matrix B:
[[11 12 13]
 [14 15 16]
 [17 18 19]]

Matrix-matrix addition:
[[12 14 16]
 [18 20 22]
 [24 26 28]]

Matrix-scalar multiplication:
[[ 2  4  6]
 [ 8 10 12]
 [14 16 18]]

Matrix-vector multiplication: dot product:
[ 8 17 26]

Matrix-matrix multiplication: dot product:
[[ 90  96 102]
 [216 231 246]
 [342 366 390]]

Matrix inverse:
[[-21.32169045  -3.8131569   38.56457317]
 [  4.50732439   3.31820901 -12.24388988]
 [  7.14926296  -0.18735867  -8.81170042]]

Matrix transpose:
[[1 4 7]
 [2 5 8]
 [3 6 9]]

Hadamard product: 
[[ 11  24  39]
 [ 56  75  96]
 [119 144 171]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;eigendecomposition&quot;&gt;Eigendecomposition&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;eigen_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eigen_vectors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;eig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Matrix eigenvalues:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eigen_values&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Matrix eigenvectors:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eigen_vectors&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Matrix eigenvalues:
[ 1.68949164 -0.03099435  0.20589414]

Matrix eigenvectors:
[[-0.49129322 -0.93332275  0.55829177]
 [-0.79415823  0.21672628 -0.77249178]
 [-0.35769216  0.28624879  0.30259999]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;singular-value-decomposition&quot;&gt;Singular value decomposition&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;U&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;S&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;svd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Left orthogonal matrix C:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;U&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Singular values diagonal matrix C:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Right orthogonal matrix C:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Left orthogonal matrix C:
[[-0.54 -0.69  0.48]
 [-0.75  0.65  0.09]
 [-0.38 -0.31 -0.87]]

Singular values diagonal matrix C:
[1.89 0.27 0.02]

Right orthogonal matrix C:
[[-0.36 -0.57 -0.74]
 [ 0.07  0.77 -0.63]
 [-0.93  0.28  0.23]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;strings-operations-with-numpy&quot;&gt;Strings operations with NumPy&lt;/h2&gt;

&lt;p&gt;Turns out &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; is not only a power number-crunching engine but also pretty good at handling strings. Although strings (letters, characters) and numbers are completely different things from a human perspective, both reduce to sequences of zeros and ones to the computer, so &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; can work with strings in a vectorized fashion as well.&lt;/p&gt;

&lt;h3 id=&quot;basic-string-manipulation&quot;&gt;Basic string manipulation&lt;/h3&gt;

&lt;p&gt;String manipulation is a whole area of expertise in itself, so we can’t and won’t dig very deep into it. Yet, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; can help you out to perform a wide variety of common string operations with relative ease. Let’s check a few.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;string1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Ms&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Mx&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Mr&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Dr&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Lord&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;string2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Weird&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Smelly&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Smart&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Strong&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Happy&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;string3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; pants &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; feet &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; belly buttom &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; elbow &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; jaw &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Add strings:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;string2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Multiply strings:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;multiply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Capitalize first letter of strings:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;capitalize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Join strings in a sequence:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;string1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Replace string elements:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;S&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Strip char elements from the beginning and end of the string (useful to remove white spaces):&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;strip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;chars&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Title case strings:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Upper case strings:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;upper&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Add strings:
[&apos;MsWeird&apos; &apos;MxSmelly&apos; &apos;MrSmart&apos; &apos;DrStrong&apos; &apos;LordHappy&apos;]

Multiply strings:
[&apos;MsMs&apos; &apos;MxMx&apos; &apos;MrMr&apos; &apos;DrDr&apos; &apos;LordLord&apos;]

Capitalize first letter of strings:
[&apos; pants &apos; &apos; feet &apos; &apos; belly buttom &apos; &apos; elbow &apos; &apos; jaw &apos;]

Join strings in a sequence:
[&apos;M-s&apos; &apos;M-x&apos; &apos;M-r&apos; &apos;D-r&apos; &apos;L-o-r-d&apos;]

Replace string elements:
[&apos;Weird&apos; &apos;Pmelly&apos; &apos;Pmart&apos; &apos;Ptrong&apos; &apos;Happy&apos;]

Strip char elements from the beginning and end of the string (useful to remove white spaces):
[&apos;pants&apos; &apos;feet&apos; &apos;belly buttom&apos; &apos;elbow&apos; &apos;jaw&apos;]

Title case strings:
[&apos; Pants &apos; &apos; Feet &apos; &apos; Belly Buttom &apos; &apos; Elbow &apos; &apos; Jaw &apos;]

Upper case strings:
[&apos; PANTS &apos; &apos; FEET &apos; &apos; BELLY BUTTOM &apos; &apos; ELBOW &apos; &apos; JAW &apos;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;basic-string-comparison&quot;&gt;Basic string comparison&lt;/h3&gt;

&lt;p&gt;String comparison in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; utilizes the same logic as with numbers. Keep in mind you have to use the methods from the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;char&lt;/code&gt; module. Be aware that white spaces at the end of the string will be removed before comparison.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;strings&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;cat&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;dog&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;dog &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;lizard&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Equality comparison cat-dog: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;equal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Equality comparison dog-dog: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;equal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Inequality comparison cat-dog: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;not_equal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Inequality comparison dog-dog: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;not_equal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Greather than comparison cat-lizard: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;greater&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Less than comparison lizard-dog: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;not_equal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Equality comparison cat-dog: False
Equality comparison dog-dog: True

Inequality comparison cat-dog: True
Inequality comparison dog-dog: False

Greather than comparison cat-lizard: False
Less than comparison lizard-dog: True
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;string-information&quot;&gt;String information&lt;/h3&gt;

&lt;p&gt;There are many instances where you will want to search for or information contained in a string. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; also has a rich list of methods to approach that:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;strings2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Psychotomimetic&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Trichotillomania&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                     &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Omphaloskepsis&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Xenotransplantation&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                     &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Embourgeoisement&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Polyphiloprogenitive&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                     &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;12345&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;     &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Count number of times substring &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; occurs in string: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strings2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Check whether the strings ends with &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;endswith&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strings2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Find the first ocurrence &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; and return the index postion: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;find&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strings2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Find strings with numeric characters only: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;isnumeric&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strings2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Find strings with at least one white space: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;isspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strings2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Count number of times substring &apos;p&apos; occurs in string: [0 0 2 1 0 2 0 0]

Check whether the strings ends with &apos;s&apos;: [False False  True False False False False False]

Find the first ocurrence &apos;s&apos; and return the index postion: [ 1 -1  7  8 10 -1 -1 -1]

Find strings with numeric characters only: [False False False False False False  True False]

Find strings with at least one white space: [False False False False False False False  True]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

</description>
        <pubDate>Tue, 14 Jul 2020 00:00:00 +0800</pubDate>
        <link>//intro-numpy-fundamentals</link>
        <link href="/intro-numpy-fundamentals"/>
        <guid isPermaLink="true">/intro-numpy-fundamentals</guid>
      </item>
    
      <item>
        <title>The Data Science Tree of Knowledge - What is Data Science and How to Educate Data Scientists</title>
        <description>&lt;iframe src=&quot;https://github.com/sponsors/pabloinsente/card&quot; title=&quot;Sponsor pabloinsente&quot; height=&quot;225&quot; width=&quot;600&quot; style=&quot;border: 0;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Here it is:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/post-14/ds-tree.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now the essay.&lt;/p&gt;

&lt;h2 id=&quot;data-science-challenges&quot;&gt;Data Science challenges&lt;/h2&gt;

&lt;p&gt;As with any other science-based discipline, Data Science tools and core of knowledge changes constantly. During my sociology, public policy, and psychology studies, I quickly learned that research perpetually redefines the boundaries of knowledge. Through friends in fields like physics, biology, engineering, and medical sciences, I have learned that the situation is no different there. Yet, I believe there are a couple of characteristics that make Data Science a particularly hard to “keep up with” as a field. More than most fields I am aware of at least. If you are active in social media, you will quickly find testimonies of people feeling overwhelmed by the pace of change of the field, and its overall fuzziness. With this in mind, in this brief essay, I try to answer the following questions: (1) &lt;strong&gt;What is Data Science?&lt;/strong&gt;; (2) &lt;strong&gt;Why is so hard to “keep up with” Data Science tools and knowledge?&lt;/strong&gt;; (2) &lt;strong&gt;What can we do about it (question 2)?&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;what-is-data-science-buzzword-or-discipline&quot;&gt;What is Data Science? Buzzword or discipline?&lt;/h2&gt;

&lt;p&gt;There is no universal agreement around what Data Science “is”. True, you may have read or heard things like “&lt;em&gt;Data Science is the intersection of computer science, mathematics/stats, and science&lt;/em&gt;” or maybe “&lt;em&gt;Data Science is the application of data-related tools and knowledge to solve practical problems&lt;/em&gt;”. However, there is no reason for you to accept such definitions. There is nothing special about them. All of them are essentially &lt;em&gt;proposals&lt;/em&gt;, no matter how many Twitter followers, degrees, academic papers, or awards the definition proponent possesses. Based on this fact, you may contest the whole “&lt;em&gt;What Data Science is&lt;/em&gt;” as a pseudo-problem. Maybe Data Science is just another empty label created by people who wanted to profit from making applied statistics to sound cooler. If I had a dollar for every time I read/heard Data Science is just “applied stats” I would be doing something fun my pile of money instead of writing this.&lt;/p&gt;

&lt;p&gt;But this is the thing: &lt;strong&gt;the whole “what a discipline really is” is a universal problem&lt;/strong&gt;. If you have had a chance to read about the history and epistemological foundations of any discipline, you will find that more often than not, multiple definitions and perspectives about what a discipline “really is”, each one with its group of advocates and followers. Just go to ask a cognitive psychologist and a behavioral psychologist for a definition of psychology. You will end up with 4 or 5 definitions (yeah, from just 2 people). So, the whole “Platonic” conundrum about what disciplines “really are” should not deter you from taking a discipline seriously. Particularly with disciplines that can impact large numbers of individuals, especially vulnerable individuals, as Data Science. In any case, whatever you think Data Science “really is”, probably has something to do with things like data, mathematics, algorithms, and human insight, blended in such a way that makes sense.&lt;/p&gt;

&lt;p&gt;“&lt;em&gt;Got it. There are many reasonable definitions of Data Science and it may be a thing. So what?&lt;/em&gt;” You see, I do care about this because your understanding of what Data Science “is”, provides substance to the discussion of why, in my view, it is so damn hard to keep up with the pace at which things change. Let me begin by claiming the following: Data Science is an applied discipline, like engineering or medicine. This may or may not amount to anything to you but saying that it is not like physics or pure mathematics is a step forward in my view.
Take applied fields like engineering and medicine. They exist in a duality: they have their &lt;strong&gt;own goals and points of view&lt;/strong&gt;, but at the same time are &lt;strong&gt;constrained by&lt;/strong&gt; the evolution of disciplines like physics, biology, mathematics, and computer science (among others). It is pretty transparent that Data Science also depends on the evolution of computer science and mathematics, but does Data Science have any “independent” goal? Any “unique” point of view? Its unique concerns? If Data Science cannot find any particular “angle” and unique “space of concerns”, there is little point to raise Data Science to the category of “Discipline”.&lt;/p&gt;

&lt;p&gt;With the aforementioned ideas in mind, let me define Data Science goals as follow: answering user-defined meaningful questions by applying scientific reasoning and mathematical and computational tools. Great, but what kind of questions? Electrical engineering is about, well, electricity. Hence, electrical engineering deals with questions related to the technology of electricity. For instance, does a question about women’s health which applies the scientific method and mathematical and computational tools qualify as a “Data Science question”? Probably yes, but it also qualifies as a Public Health question, and Public Health experts exist already, so why on earth to invent a new label for something it has one already?&lt;/p&gt;

&lt;p&gt;This is the main challenge for people who self-identify as Data Scientist: &lt;strong&gt;what makes you different from an econometrician, or a biostatistician, or a demographer, or a computer scientist, and any other data expert within a field?&lt;/strong&gt; As it now, it sounds like we are just playing a “labels game” for convenience (marketability). I have read/heard people pointing out to their programming skills “&lt;em&gt;I am a better programmer than any biostatistician I know of&lt;/em&gt;”. If this answer suffices for you, cool, you can skip the rest of this article. Otherwise, if you are like me, such an answer is unsatisfactory. After all, neither biostatistics nor Data Science is a programming-skills competitions, and what counts as a “good programmer” is not clear at all.&lt;/p&gt;

&lt;p&gt;In my view, what sets apart and helps to Define Data Science as a discipline, it is the how-to jointly use data, mathematics, and computational tools, to address empirical questions IN ITSELF. If this sounds like a reiteration of my first definition, pause and ponder for a second. It is not. The key is the “IN ITSELF” part of it. Rephrasing it, is about the art and science of the question-answering with data plus math plus computational tools. “Wait, isn’t that the goal of computer science or statistics”. It is not. At least not necessarily. Strictly speaking, statistics can generate new and useful knowledge without any regard for, for instance, practical computational constraints. True, ultimately, to be useful, you must pay attention to things like data collection procedures and computational resources, but the point is that this is not required in principle.  Same with computer science, which is the study of computation and information. How you get such data, or how to formulate appropriate questions, or how to apply the scientific method, are not concerns of computer science, not in principle at least. Yet, I propose, they must be concerns of Data Science and Data Scientist, always.&lt;/p&gt;

&lt;p&gt;What about biostatistics? Or Econometrics? After all, it sounds like they do care about all that stuff at once. Here is my issue: econometricians do not care (no in principle) whether the methods will be useful for biology, and biostatistics do not care whether their methods are appropriated for microeconomics applications. More generally, they do not care about developing tools and methods to answer empirical questions &lt;strong&gt;in general&lt;/strong&gt;, and tend to be a bit indifferent to things like computer architectures and other disciplines’ algorithms. &lt;strong&gt;But those issues MUST be of consideration for Data Science as a discipline&lt;/strong&gt; (as I understand it). Ideally, Data Science should be able, as a discipline, to address the mathematical and computational needs of a wide variety of disciplines and fields of inquiry. “But NOBODY can do that, Pablo!” Of course, no single individual can! In the same manner that no single medical student can address all medical problems of the human species. But medicine, as a discipline, aims to address that issue, and Data Science as a discipline should have such a general scope. Here are some examples of things that a Data Scientist should care about:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;What is the best way to store data from different sizes and formats?&lt;/li&gt;
  &lt;li&gt;When to automate data processing pipelines?&lt;/li&gt;
  &lt;li&gt;When is it a good idea to use a declarative versus an imperative approach to data visualization?&lt;/li&gt;
  &lt;li&gt;What questions are appropriated to ask when using unsupervised learning methods?&lt;/li&gt;
  &lt;li&gt;At what point is it a good idea to move your data processing pipeline to a cluster of computers in the cloud?&lt;/li&gt;
  &lt;li&gt;In what circumstances quasi-experimental methods are an acceptable replacement for randomized experimental methods?&lt;/li&gt;
  &lt;li&gt;At what point is worth writing code in low-level programming languages like C++ or Fortran rather than Python or R?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These are just a couple of examples of questions which, I believe, share the two traits: &lt;strong&gt;generality&lt;/strong&gt;, i.e., are relevant for a wide variety of fields of expertise, and cross-discipline, meaning that they require knowledge and skills of several fields to be answered.&lt;/p&gt;

&lt;p&gt;With this idea in mind, and assuming you find my perspective acceptable, I can address the next question.&lt;/p&gt;

&lt;h2 id=&quot;why-is-so-hard-to-keep-up-with-data-science-tools-and-knowledge&quot;&gt;Why is so hard to “keep up with” Data Science tools and knowledge?&lt;/h2&gt;

&lt;p&gt;Short answer: because of its generality and immaturity. Trying to keep up with “medical sciences”, as a whole, is impossible for any finite human being. Keeping up with Data Science as a discipline, it is equally impossible. I believe many people in the field feel overwhelmed because they are constantly bombarded with the latest and greatest advances in “Data Science”, which can come from: time-series analysis, declarative data visualization, computer vision, affective computing, natural language processing, emotion synthesis, econometrics, biostatistics, signal processing, applied mathematics, information theory, new python libraries, new R libraries, new MATLAB libraries, new JS libraries, new computer architectures, new GPUs, new Big Data technologies, new optimization techniques, and the list goes on and on. &lt;strong&gt;The broad scope of Data Science is a course for its practitioners&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;One of the reasons sometimes it feels like Data Science is impossible to narrow down in scope, is because of hiring practices. For example, I have gone through at least 20 cycles of interview processes, from places as diverse as Airbnb, Asana, Trivago, McKinsey &amp;amp; Company, and Akuna Capital, and pretty much all of them were different. “&lt;em&gt;Did you even look at the job description?&lt;/em&gt;” The hell I did, and no, the job description was of little use to narrow down my preparation, as most of them incorporated all skills and technologies under the sun. I have been asked to solve problems using traditional machine learning techniques, using deep learning techniques, to implementing advance SQL queries, to create engaging and insightful graphs, to design randomized controlled experiments, to utilize quasi-experimental econometric techniques, to solve complex algorithm problems, to code in R, to code in Python, and the list goes on and on. When the market implicitly defines Data Science as the whole field at once, you are doom to feel overwhelmed by the share size and pace of change of the field. You are never ready, you are never good enough, you will never master your craft because the craft is too big and brittle to master.&lt;/p&gt;

&lt;p&gt;Another reason is the eagerness from academia, industry, government, and even single contributors, to innovate and “put things out there” as fast as they can. Not that constant investment and innovation is bad, not at all, but it does create a perpetual cycle of fast obsolescence of skill and knowledge. “&lt;em&gt;Oh, so you spent a year becoming a TensorFlow 1.X expert? Too bad, we want TensorFlow 2.X now&lt;/em&gt;” Or “&lt;em&gt;Wait, no, we changed our mind, we want PyTorch experts now. But you know what, we learned bout this great new thing, MXNet, we heard that Amazon use it, and we want to follow their lead, so we want experts in both&lt;/em&gt;”. Or changing from R to Python. Or mastering both natural language processing AND time-series analysis, because, why not, how hard it can be, right?&lt;/p&gt;

&lt;p&gt;Last but not least, I mentioned Data Science immaturity. This implies two things: (1) most people have no idea of where does Data Science begins and ends, or what a Data Scientists is supposed to look like, or what they can or cannot do; (2)  most people have no idea how to educate Data Scientists, or what does constitute a well-stablished core of knowledge for the discipline.&lt;/p&gt;

&lt;p&gt;Engineering and medicine are old enough disciplines that there is some degree of agreement regarding what they are, how do they look like, and what are they expected roles and preparation. Most schools more or less agree on how to educate engineers and physicians, and a &lt;strong&gt;sensible core of knowledge&lt;/strong&gt; has been defined as a basis for the discipline. I believe, this is what Data Science is missing: an agreement about the identity, roles, preparation, and what a &lt;strong&gt;sensitive core of knowledge&lt;/strong&gt; looks like.&lt;/p&gt;

&lt;h2 id=&quot;what-to-do-about-it&quot;&gt;What to do about it&lt;/h2&gt;

&lt;p&gt;In my experience, when I point out an issue, people are quick to ask me for a solution, usually a bit annoyed by my impertinence. I do not know how to fix the issue as a whole, but I do have a couple of ideas, particularly on the side of Data Science education and definition of a “sensible core of knowledge”. This I what I - presumptuously- call “&lt;strong&gt;The Data Science Tree of Knowledge&lt;/strong&gt;”.&lt;/p&gt;

&lt;h2 id=&quot;the-data-science-tree-of-knowledge&quot;&gt;The Data Science Tree of Knowledge&lt;/h2&gt;

&lt;p&gt;Before offering my perspective about what should constitute the “Data Science Tree of Knowledge”, I want to briefly look at what “top” programs in the world are offering as an educational core at the undergraduate and graduate level in Data Science.&lt;/p&gt;

&lt;p&gt;UC Berkeley offers a good example of the disciplinary basis for Data Science education:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- Foundations of Data Science
- Calculus I and II
- Linear Algebra
- Program Structures
- Data Structures
- Domain Emphasis
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;At the graduate level, the NYU PhD in Data Science is a great example:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- Introduction to Data Science
- Probability and Statistics for Data Science
- Machine Learning
- Big Data
- Inference and Representation
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Another way to assess this is by looking at eligibility requirements for graduate programs. The University of Toronto’s MS in Data Science specifics:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    At least two senior level statistics courses equivalent to:
        STA 302H Applied Regression Analysis, and/or
        STA 347H Probability Theory, and/or
        STA 452H/453H Introduction to Mathematical Statistics

    At least three senior level computer science courses equivalent to:
        CSC 373H Algorithms and complexity
        CSC 343H Database Systems
        CSC 369H Operating Systems
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Undergraduate and graduate education operate under time, financial, and human capital constraints, so it is expected they will not put everything they would like to in the curriculum.&lt;/p&gt;

&lt;p&gt;My goal is a bit more abstract and unconstrained by the factual realities of universities and professional degrees. I want to propose an &lt;strong&gt;idealized yet sensible corpus of knowledge for the discipline&lt;/strong&gt;, which may serve as a foundation for Data Science education. I use the &lt;strong&gt;Tree of Knowledge metaphor&lt;/strong&gt;, as it nicely represents the interconnectedness, fuzziness, and importance of strong foundations I want to convey.&lt;/p&gt;

&lt;h3 id=&quot;roots&quot;&gt;Roots&lt;/h3&gt;

&lt;p&gt;The roots of the tree are interconnected disciplines which provide the foundation’s Data Science as a discipline and Data Scientists as practitioners. In an ideal world, these are the subject people should spend most of their time and effort, as strong foundations will allow for quicker and, more importantly,  higher-quality learning of subsequent subjects. In sum, these are the topics I think all Data Scientists should learn about regardless of their future area of expertise:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Calculus&lt;/li&gt;
  &lt;li&gt;Linear algebra&lt;/li&gt;
  &lt;li&gt;Probability theory and statistics&lt;/li&gt;
  &lt;li&gt;Discrete mathematics (set theory)&lt;/li&gt;
  &lt;li&gt;Intro to optimization&lt;/li&gt;
  &lt;li&gt;Intro to information theory&lt;/li&gt;
  &lt;li&gt;Intro to algorithms&lt;/li&gt;
  &lt;li&gt;Intro to programming&lt;/li&gt;
  &lt;li&gt;Intro to computer science&lt;/li&gt;
  &lt;li&gt;Intro to data structures&lt;/li&gt;
  &lt;li&gt;Intro to data management&lt;/li&gt;
  &lt;li&gt;Scientific method and reasoning&lt;/li&gt;
  &lt;li&gt;Science communication&lt;/li&gt;
  &lt;li&gt;Quantitative and qualitative research methods&lt;/li&gt;
  &lt;li&gt;Philosophy of science&lt;/li&gt;
  &lt;li&gt;Intro to data visualization&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;trunk&quot;&gt;Trunk&lt;/h3&gt;

&lt;p&gt;The trunk of the tree represents a collection of topics that can be selected depending on interest and area of desired expertise. Not all Data Scientists need to learn about Deep Learning.   If you have been following all the buzz about Deep Learning, you may think this is utter nonsense. “EVERYONE should know about Deep Learning”. Well, I do not believe so. Becoming a proficient Deep Learning practitioner takes an enormous amount of time and dedication. I do not believe you can master experimental and quasi-experimental methods, AND Deep Learning at the same time. True, it is useful to know what is “Deep Learning all about” but that it is completely different from becoming a Deep Learning practitioner. So, it is nice to have notions about most subjects, but practitioners should focus on a synergistic (is that even a word?) and a manageable subset of topics like:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Advance optimization&lt;/li&gt;
  &lt;li&gt;Intermediate algorithms&lt;/li&gt;
  &lt;li&gt;Machine learning foundations&lt;/li&gt;
  &lt;li&gt;Deep learning foundations (neural networks)&lt;/li&gt;
  &lt;li&gt;Applied statistics&lt;/li&gt;
  &lt;li&gt;Applied Bayesian statistics&lt;/li&gt;
  &lt;li&gt;Advance data visualization&lt;/li&gt;
  &lt;li&gt;Randomized experiments&lt;/li&gt;
  &lt;li&gt;Quasi-experimental methods&lt;/li&gt;
  &lt;li&gt;Programming in Python or R&lt;/li&gt;
  &lt;li&gt;Advance databases&lt;/li&gt;
  &lt;li&gt;Cloud and distributed computing&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;branches&quot;&gt;Branches&lt;/h3&gt;

&lt;p&gt;Finally, the branches of the tree represent topics that require intense dedication and expertise. Think of this as acquiring a medical specialty, like pediatrician or internal medicine. This means I do not believe everyone should learn about these topics. Actually, as many physicians do, you may skip this section altogether and become a perfectly high-quality and competent professional by mastering some cluster of topics from the trunk of the tree. When I see people offering courses to master stuff like “Computer vision” or “Graphical models” in a 2 months course, I cannot avoid but to feel such a person is just trying to profit from the hopes and dreams of hardworking people. True, there are examples of people that learned a lot from such courses, and some went as far as getting a job in that specialty. But those are exceptions, not the rule. I do not have the numbers here, but I am pretty sure they are not many computer vision experts who learned their craft in a 2 months Udemy course. By the way, I do not look down at those courses at all! I have taken (yes, I PAID) $11 Udemy courses as well. There are fine but they are no replacement for proper Data Science education. Notice I say “education” not “degree”, which I think are different things: self-educated people is valuable and competent as any fancy degree-holder, but self-education ought to be done seriously, meaning a plan, meaning long hours, meaning years of effort. In any case, these are topics that require two things: a solid foundation in general Data Science topics (see now?), and significant time and effort as their complexity requires:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Time-series analysis&lt;/li&gt;
  &lt;li&gt;Advance applied statistics&lt;/li&gt;
  &lt;li&gt;Computer vision&lt;/li&gt;
  &lt;li&gt;Natural language processing&lt;/li&gt;
  &lt;li&gt;Reinforcement learning&lt;/li&gt;
  &lt;li&gt;Deep reinforcement learning&lt;/li&gt;
  &lt;li&gt;Graphical models&lt;/li&gt;
  &lt;li&gt;Advance artificial intelligence&lt;/li&gt;
  &lt;li&gt;High-performance computing&lt;/li&gt;
  &lt;li&gt;High-throughput computing&lt;/li&gt;
  &lt;li&gt;Cloud computing&lt;/li&gt;
  &lt;li&gt;Representation learning&lt;/li&gt;
  &lt;li&gt;Deep generative models&lt;/li&gt;
  &lt;li&gt;Advance Bayesian models&lt;/li&gt;
  &lt;li&gt;Monte-carlo methods&lt;/li&gt;
  &lt;li&gt;Affective computing&lt;/li&gt;
  &lt;li&gt;Human-computer interaction&lt;/li&gt;
  &lt;li&gt;Robotics&lt;/li&gt;
  &lt;li&gt;Hybrid models&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;My roots, trunk, and branches lists are by no means complete or exhaustive, but I hope are enough to convey my main points: &lt;strong&gt;Data Science, as a discipline, does require a sensible core of knowledge to define itself as a discipline, and to envision a clear learning path for people who aspire to become a Data Scientist&lt;/strong&gt;.&lt;/p&gt;
</description>
        <pubDate>Tue, 07 Jul 2020 00:00:00 +0800</pubDate>
        <link>//ds-tree-knowledge</link>
        <link href="/ds-tree-knowledge"/>
        <guid isPermaLink="true">/ds-tree-knowledge</guid>
      </item>
    
      <item>
        <title>Why Do I Create Free Data Science and Machine Learning Educational Content - For Revenge</title>
        <description>&lt;iframe src=&quot;https://github.com/sponsors/pabloinsente/card&quot; title=&quot;Sponsor pabloinsente&quot; height=&quot;225&quot; width=&quot;600&quot; style=&quot;border: 0;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Why do I spend so much creating freely accessible educational content on data science? Many reasons. The most immediate are my professional development (teaching others is the best way to learn “for me”), to showcase my skills to potential employers, and because I just like it. But there is another one: &lt;strong&gt;revenge&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;“Revenge? Seriously? That’s weird compadre…” Well, maybe. You see, during my educational experience, I was told that &lt;strong&gt;mathematics and “hard” sciences were probably beyond the scope of my learning skills and intelligence&lt;/strong&gt;. Directly and indirectly. Even during my PhD studies, people have told me that taking class “X” is probably too hard for me. “Sure Pablo, but I bet other people were encouraging too” Absolutely right: I have had both encouraging and discouraging people in my life, telling me that “of course you can learn anything!” and “Maybe this is too hard for you”, respectively. So, what is the issue? After all, you can simply ignore discouraging people. Right?. This may or may not apply to you, but I was not the kind of child and teenager who could simply brush away discouraging and hurtful commentaries. &lt;strong&gt;When you grew up as a poor kid, in a poor country, with uneducated caregivers and unskilled educators, amid violence and neglect, learning to ignore negative feedback it is difficult&lt;/strong&gt;. Also convincing yourself you can learn anything turns out to be hard. My point is: depending on your circumstances, &lt;strong&gt;discouraging and harmful commentaries about your skill and intelligence can have a “disproportionally” large negative effect in your learning outcomes and feelings of self-efficacy&lt;/strong&gt;. “Well, maybe this is jus you!” True, I am claiming this based on my personal experience (and what I have observed in others), but there is a large corpus of research in psychology and educational sciences showing that it is not just me (I wont’ cite the papers here, I am kinda tired of writing everything as a scientific article). It took me many years of effort, luck, and small victories to convince myself I could indeed learn anything if I put the time and effort.&lt;/p&gt;

&lt;p&gt;My first memory of learning math is my father’s frustration and anger because of my “inability” to understand how place value works in an abacus. I was so nervous and scared of my father than I just could not put my head to work. Those study sessions did not instill a lot of self-confidence in me. Teachers at school were not much better. Many operated under the absurd idea that they were some students for whom math came “naturally”, and the rest, well, the rest should try very hard and hope for the best. Society needs lawyers and historians too. Then high school happened. I almost failed my first and second year of high school because of math: I ended up the first semester, in both years, with failing grades in math. A failing grade in my high school meant having to repeat that year. But I did not. Failing students look bad for public school. This is what they did: they hand me over an “extra” final test and said that if I got at least a 4 (the minimum passing grade in the Chilean system), they will let me pass to the next year. I did, both times, I got a 4, and move to the next grade.&lt;/p&gt;

&lt;p&gt;By when I graduated from high school, I had convinced myself that math was something I was not “made for”. They were “math and science kids” and I was not one of them. I ended up pursuing a career in, from all things, dancing. I liked dancing, but just as a hobby, more or less in the same manner I liked singing or playing video games. But my father was a self-taught dancer who dreamed of me pursuing the education in dancing he did not (it was mostly about him), and very good at forcing me to do things I did not want to do. I gave up dancing eventually and tried to get into college. At the time, I read a book about the perils of Chilean society and decided I wanted to study sociology (the book author happened to be a sociologist). When I realized sociology had “statistics” in the curriculum, I almost lost hope. I tried hard to learn math for the university admission test on my own (the Chilean “PSU”), but I was failing miserably. Then, &lt;strong&gt;I got my first stroke of luck&lt;/strong&gt;: I just happened to know this guy, Mauro, who gave private math lessons. He was the math tutor of an upper-class friend from high school. It just occurred to me to ask him for math lessons. He got very excited, said yes, and decided to charge me (and my wife) an extremely low fee for the lessons. He literally said: “how much can you pay?” and charged us what we said (he even returned the money back to us at the end). I found out later, that he usually charged ~5 times more what we paid, as he turned out to be one of the best private math instructors in the country, who usually took students from the richest families in Chile at very high fees (&lt;a href=&quot;https://www.youtube.com/channel/UCbdoqtgHjDZwYASjQJlxB4A&quot;&gt;he has the largest YouTube channel about math education in Chile now&lt;/a&gt;). He also happened to be a politically progressive and extremely generous person. We had around 10-12 study sessions in a ~2.5 months period, just before the university entry exam. I was absolutely flabbergasted by Mauro’s energy, confidence, expertise, and skill. During that time, and for the first time, I realized there was nothing “magical” or “beyond the scope of my skills” about math. Mauro did not want to “show off”, he just wanted to “show us” that &lt;strong&gt;we could learn anything&lt;/strong&gt;. That anyone could learn math given the right instructor, circumstances, and enough time and effort to invest. We did not have anything fancy: old math books, paper, and pencil. That was it. The key was him convincing us that &lt;strong&gt;math was accessible, useful, and more importantly, that we could definitively master the contents if we just put the time and effort following his instructions and practicing&lt;/strong&gt;. This was the first step in a long process of realizing, not just about math, but about any skill or area of expertise, that I could learn pretty much anything. It may take me more time, more effort, a different book, or a different instructor. But it could be accomplished.&lt;/p&gt;

&lt;p&gt;During my sociology studies at the Universidad de Chile, I got deeply interested in psychology, neuroscience, and economics. I eventually decided I wanted to do a master’s in economics. I went to talk with a couple of professors in the Economics department at the Universidad de Chile. &lt;strong&gt;The conversations I had with them were disheartening&lt;/strong&gt;. They basically told me that I was way too far behind the required level of mathematical knowledge and skill to get into the program. They also took care to make it sound very difficult and inaccessible for “someone with my background”, as I asked if I could study on my own or take classes over the summer to fill the gaps. They eventually talked me out of applying to the program. They also said that the Public Policy master’s program was “less mathematical” and more suited for someone like me, so I ended up pursuing that program (and eventually graduating at the top of the class).&lt;/p&gt;

&lt;p&gt;In 2017, I came to the USA for my PhD in Psychology at UW-Madison with a Fulbright Scholarship for low SES-students. Early on, I got interested in Data Science and Machine Learning. My experience was mixed here. People encouraging me to learn Python and statistics, and people telling me that the class “X” was too hard for me, usually, people related to the Computer Science and Engineering departments. By then, I had enough encouragement and self-confidence to say “Well, let’s try it out and see what happens”. I just started to use all my free time and summers to learn about programming, computer science, mathematics, and anything related to data science and machine learning. It was hard, it took time, but after every iteration learning some new topic, it started to feel easier and easier. I eventually decided to take a graduate-level “mathematics for machine learning” at the engineering department, along with a bunch of graduate students from computer science, mathematics, and engineering. Although I did not take the class for credit, I did attend all classes and did all the homework (I just skipped the midterm and final exam). And that was it: &lt;strong&gt;realizing I could do the same exercises, and sometimes even EXPLAIN to graduate students in the engineering and other departments something tricky about a linear algebra problem was eye-opening to me&lt;/strong&gt;. There is no magic, no special math brain module “enabling” you to learn math.&lt;strong&gt;It is about opportunity, good instructors, patience, support, enough resources, time, and effort.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The way I see things in the data science and machine learning space nowadays is like a &lt;strong&gt;space dominated by two extremes&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;On the one hand, you have people telling others learning Data Science and Machine Learning is &lt;strong&gt;dead simple&lt;/strong&gt;, so simple that you can learn it in 6 months or even 3 months, or by watching their series of twenty short videos, or that math and programming skills are not “really” relevant, so just skip that, it’s a waste of time, no challenges at all! Just &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda&lt;/code&gt; install this, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Shift&lt;/code&gt;+&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Ctrl&lt;/code&gt; these cells in my Jupyter Notebook and you are good to apply for a position at Google and Netflix.&lt;/p&gt;

&lt;p&gt;On the other hand, you have people telling others that “real” Data Science and Machine Learning require taking this long list of “difficult” math, physics, and computer science classes, or reading cover to cover this stack of books, or publishing at least two papers in NeurIPS, or MA and a PhD in CS/Math/Physics from a “reputable” institution.&lt;/p&gt;

&lt;p&gt;I disagree with both perspectives. The first is misleading, creates false hope, trivializes the challenges and effort necessary to learn data science, and encourages people to flood into online courses and bootcamps with the expectation to become a data scientist in 6 months. Many people are wasting time and money following the lead of those individuals. The second is essentially a form of gatekeeping, from people who like to intellectually intimidate others, and who probably need to put others down to protect their socio-economic status and feelings of superiority.&lt;/p&gt;

&lt;p&gt;I do not want to say my perspective is a “middle-ground”. It is not. I reject such dichotomy. It is just different. &lt;strong&gt;Learning Data Science and Machine Learning is no different than learning about any other subject&lt;/strong&gt;. Learning about engineering, programming, physical therapy, finance, law, sociology, and any other professional discipline simply requires a mix of opportunity, discipline, resources, good guidance, time, and effort. My philosophy is as follows:&lt;strong&gt;put a lot of time and effort learning the fundamentals, and then speed up your learning about more specific subjects&lt;/strong&gt;. This translates into spending a significant amount of time learning linear algebra, calculus, probability theory, basic algorithms, basic programming, and topics of that kind. If you master the fundamentals, learning more advance or applied topics, like neural networks, graphical models, or randomized experiments, becomes significantly faster and easier. It is essentially like saving money works: &lt;strong&gt;the more you invest now, the more you will gain in the future&lt;/strong&gt;. This approach will probably require a couple of years, but I do not get why such an idea may be controversial. Any person expecting to become an engineer or a psychologist knows that a good 4-6 years must be invested. I am not saying you need 4 years (maybe your background allows you to speed up your process), but I doubt you can get a good data science education in less than 2-3 years. &lt;strong&gt;The issue is that many people will make you feel you cannot do it or is too hard, or that you have to be a math person, or a computer science/physics/math major, or just passive-aggressively intimidate you out of the idea&lt;/strong&gt;. They really like making things sound harder than they are, more alien, more special, and more for a highly selected group of MIT/Stanford/Berkeley graduates. That makes &lt;strong&gt;them&lt;/strong&gt; “feel” smarter, “feel” more special, “feel” more worthy than &lt;strong&gt;you&lt;/strong&gt; are. The other side of the spectrum, as I argued, is no good either.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/post-13/rock-lee.jpg&quot; alt=&quot;rock-lee&quot; /&gt;&lt;br /&gt;
&lt;a href=&quot;https://www.deviantart.com/designerrenan/art/Rock-Lee-Lotus-Primary-And-Video-543827918&quot;&gt;Source&lt;/a&gt; by DesignerRenan&lt;/p&gt;

&lt;p&gt;So, what is my revenge all about? &lt;strong&gt;I am revenging from all the people who made me doubt myself, of my skill, my intelligence, and my capacity to learn anything, particularly math and science-related subjects&lt;/strong&gt;. My brain was not missing any “math module” or “special math instinct”. I was just systematically forced out of the topic by people who were too ignorant, too lazy, too unskilled, or too unkind to guide me in the right direction and provide the right support. I like to think that by creating content from the perspective of someone who was told that these topics were beyond its skill and intelligence, can help others in similar positions to regain their confidence and learn what they thought was not possible.&lt;/p&gt;
</description>
        <pubDate>Thu, 02 Jul 2020 00:00:00 +0800</pubDate>
        <link>//why-ds-content</link>
        <link href="/why-ds-content"/>
        <guid isPermaLink="true">/why-ds-content</guid>
      </item>
    
      <item>
        <title>Introduction to the UNIX Shell (Bash)</title>
        <description>&lt;iframe src=&quot;https://github.com/sponsors/pabloinsente/card&quot; title=&quot;Sponsor pabloinsente&quot; height=&quot;225&quot; width=&quot;600&quot; style=&quot;border: 0;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; sections:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;#what-is-the-unix-shell&quot;&gt;What is the UNIX shel&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;#why-to-use-the-unix-shell&quot;&gt;Why to use the UNIX shell&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;#a-curated-list-of-basic-shell-commands&quot;&gt;A curated list of basic shell commands&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The rest of the content is really useful, but not strictly required to use the shell.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;NOTE 1&lt;/strong&gt;: Apologies for typos and misspelled words in advance. This document has not been proofread (yet).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;NOTE 2&lt;/strong&gt;: There is a GitHub repository (&lt;a href=&quot;https://github.com/pabloinsente/intro-sc-python&quot;&gt;here&lt;/a&gt;) associated with this tutorial that you can clone to follow along. You should also be able to recreate the contents of this tutorial in your own machine by simply typing everythin in the shell. If you are not comfortable using Git/Github, you still can read this as a conceptual introduction with examples.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;NOTE FOR WINDOWS USERS&lt;/strong&gt;: As Windows is not an Unix-like or Linux-based system, most the commands and examples here won’t work, as the Windows Command Prompt and the Windows Power Shell are not bash based. You have a couple of options to follow along:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;downloading and installing terminal emulators like &lt;a href=&quot;https://gitforwindows.org/&quot;&gt;GitBash&lt;/a&gt; and &lt;a href=&quot;https://www.cygwin.com/&quot;&gt;Cygwin&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;to install the &lt;a href=&quot;https://docs.microsoft.com/en-us/windows/wsl/install-win10&quot;&gt;Windows Subsystem for Linux (WSL)&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;open the GitHub repository associated with this tutorial in the cloud environment provided by &lt;strong&gt;MyBinder&lt;/strong&gt; by clicking the icon below. Once the environment is ready (it may take a couple of minutes to build), open a terminal there. To open a terminal simply go to “File -&amp;gt; New -&amp;gt; Terminal” or click on the “Terminal” icon under the “Other” section in the landing page. The file with this tutorial is in  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unix-shell/&lt;/code&gt; directory named as &lt;strong&gt;unix_shell.md&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;To open MyBinder&lt;/strong&gt; -&amp;gt; &lt;a href=&quot;https://mybinder.org/v2/gh/pabloinsente/intro-sc-python/master/?urlpath=lab&quot;&gt;&lt;img src=&quot;https://mybinder.org/badge_logo.svg&quot; alt=&quot;Binder&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If you are a beginner, &lt;a href=&quot;https://gitforwindows.org/&quot;&gt;GitBash&lt;/a&gt; and &lt;a href=&quot;https://www.cygwin.com/&quot;&gt;Cygwin&lt;/a&gt; should work just fine, and easier to set-up. The MyBinder environment is a good option too, but you won’t be able to save your work. I do not advise trying (WSL) unless you feel comfortable with using the terminal already. Yet, WSL is the best long-term solution for Windows users.&lt;/p&gt;

&lt;h2 id=&quot;table-of-contents&quot;&gt;Table of contents&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#what-is-the-unix-shell&quot;&gt;What is the UNIX shel&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#why-to-use-the-unix-shell&quot;&gt;Why to use the UNIX shell&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#shell-syntax-basics&quot;&gt;Shell syntax basics&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#standard-input-output-and-error&quot;&gt;Standard input, output, and error&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#shell-commands-basics&quot;&gt;Shell commands basics&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#single-commands&quot;&gt;Single commands&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#composed-commands&quot;&gt;Composed commands&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#a-curated-list-of-basic-shell-commands&quot;&gt;A curated list of basic shell commands&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#basic-commands&quot;&gt;Basic commands&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#file-commands&quot;&gt;File commands&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#directory-commands&quot;&gt;Directory commands&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#system-commands&quot;&gt;System commands&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-is-the-unix-shell&quot;&gt;What is the UNIX shell&lt;/h2&gt;

&lt;p&gt;We begin our journey with the UNIX shell, that cryptic program that runs in your terminal enabling you to do all sort of tasks in your computer. The UNIX shell is a program to &lt;em&gt;interface&lt;/em&gt; with the lowest level of UNIX-based operating systems (i.e., the &lt;em&gt;kernel&lt;/em&gt;). If you are running any Mac OS or Linux Distribution, you are using a &lt;em&gt;UNIX-based&lt;/em&gt; or &lt;em&gt;Unix-like&lt;/em&gt; operating system. UNIX-based operating systems have two main parts: the &lt;em&gt;kernel&lt;/em&gt; and the &lt;em&gt;utilities&lt;/em&gt;.  The &lt;em&gt;kernel&lt;/em&gt; is the program managing and allocating the resources of the computer hardware (i.e., the Central Processing Unit or CPU, the Random Access Memory or RAM, and devices like the mouse, speaker, etc). It is a &lt;em&gt;software layer&lt;/em&gt; facilitating the control of the computer hardware. The &lt;em&gt;utilities&lt;/em&gt; are a set of commands to interface with the kernel. For instance, if you type &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pwd&lt;/code&gt; in your terminal, the kernel will load a program called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pwd&lt;/code&gt; into the RAM, read the program instructions, and display the output, in this case, the current working directory path.&lt;/p&gt;

&lt;p&gt;The so-called &lt;em&gt;shell&lt;/em&gt;, also happens to be a UNIX utility program. It has a dual identity: as a &lt;em&gt;user interface&lt;/em&gt; to the UNIX utilities, and as a &lt;em&gt;programming language&lt;/em&gt; facilitating the usage and combination of the UNIX utilities. When you open the terminal, the shell program is loaded into the computer memory. When you type commands in the terminal, the shell reads the commands and converts them into a format that is readable by the kernel to be executed. It provides an interactive instance to start programs, manage files, and processes running in the computer. Since the shell is just a program, many variations have been created since 1969, when &lt;a href=&quot;https://en.wikipedia.org/wiki/Ken_Thompson&quot;&gt;Ken Thompson&lt;/a&gt; developed the the first UNIX implementation at &lt;a href=&quot;https://en.wikipedia.org/wiki/Bell_Labs&quot;&gt;Bell Labs&lt;/a&gt;. The original UNIX shell was written by &lt;a href=&quot;https://en.wikipedia.org/wiki/Stephen_R._Bourne&quot;&gt;Steve Bourne&lt;/a&gt; in 1970, and it’s known as &lt;em&gt;Bourne shell&lt;/em&gt; or &lt;em&gt;sh&lt;/em&gt;. The Bourne shell was not available for free at the time, which limited its usage by other programmers. To alleviate this problem, in 1988, the Free Software Foundation tasked &lt;a href=&quot;https://en.wikipedia.org/wiki/Brian_Fox_(computer_programmer)&quot;&gt;Brian Fox&lt;/a&gt; to develop an open-source reimplementation of the Bourne shell, the so-called &lt;em&gt;Bourne again shell&lt;/em&gt; or &lt;em&gt;bash&lt;/em&gt;. Today, the &lt;em&gt;bash&lt;/em&gt; shell is probably the most widely use implementation of the Unix shell, and the one that serves as a base for us.&lt;/p&gt;

&lt;h2 id=&quot;why-to-use-the-unix-shell&quot;&gt;Why to use the UNIX shell&lt;/h2&gt;

&lt;p&gt;If you haven’t use the shell before, you’re probably accustomed to interact with computer software via &lt;em&gt;Graphical User Interfaces&lt;/em&gt; or a &lt;em&gt;GUI&lt;/em&gt;. This is perfectly fine for most day to day task, but in a research context, there are many important capabilities that GUI interfaces do not provide. In particular, there are a few key capabilities that I want to highlight:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Repetition&lt;/strong&gt;: there are situations when you want to repeat the same action multiple times, sometimes hundreds or thousands of times, actions like changing the extension of a large batch of files or extracting the last line of multiple text files.  Repeating these actions thousands of times with a GUI is beyond unpractical (and probably bad for your health too), and here is where the shell thrives. For example, changing thousands of files with a &lt;em&gt;.txt&lt;/em&gt; extension to a &lt;em&gt;.md&lt;/em&gt; extension can be accomplished with a single line like this one:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ rename &lt;span class=&quot;s2&quot;&gt;&quot;s/txt/md/&quot;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;HEADS UP&lt;/em&gt;&lt;/strong&gt;: In the code blocks, you see will see a  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;➜&lt;/code&gt; as a  &lt;em&gt;prompt&lt;/em&gt; before the actual command. Anything below that indicates the output printed to the terminal. Some commands do not print to the terminal. For instance:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# With standard output to the terminal&lt;/span&gt;
➜ &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;shell-command] &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;ARGUMENT&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;s&lt;span class=&quot;o&quot;&gt;)]&lt;/span&gt;
some standard output

&lt;span class=&quot;c&quot;&gt;# Without standard output to the terminal&lt;/span&gt;
➜ &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;shell-command] &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;ARGUMENT&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;s&lt;span class=&quot;o&quot;&gt;)]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you copy the instructions to your terminal, remember to delete or ignore the ➜ symbol. Your terminal may have &lt;strong&gt;$&lt;/strong&gt; symbol or some other character as a prompt, indicating that the terminal is ready to receive input.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Automation&lt;/strong&gt;: sometimes, instead of repeating the same action, you may want to repeat sequences of actions, or maybe just a single long and complicated action. You may also need to trigger an action automatically in response to some process in your computer. In either case, using the GUI makes you more error prone and slower. Writing the instructions in a &lt;em&gt;shell script&lt;/em&gt;, a text file with sequences of shell commands, can facilitate these tasks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Reproducibility&lt;/strong&gt;: the fact that you can type sequences of instructions in shell scripts, makes extremely easy to &lt;em&gt;exactly&lt;/em&gt; reproduce steps in data processing pipelines. Alternatively, you could take snapshots of your GUI and provide lengthy instructions of what to point and click at every step, but that would take more effort, more time, and increase the probability of error.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Remote server connection&lt;/strong&gt;: if you ever need to connect to another computer from your computer, this is, a &lt;em&gt;remote server&lt;/em&gt;, you will have to use the shell. For instance, &lt;em&gt;High-performance computing&lt;/em&gt; (HPC),  &lt;em&gt;High-throughput computing&lt;/em&gt; (HTC), and &lt;em&gt;Amazon Web Services&lt;/em&gt; (AWS), are all forms of remote computing that require the users to connect and interact via shell commands.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;shell-syntax-basics&quot;&gt;Shell syntax basics&lt;/h2&gt;

&lt;p&gt;When you pass commands to the terminal, &lt;em&gt;roughly&lt;/em&gt; speaking, the shell performs the following set of operations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;If you pass starting with the ‘#’ symbol, the shell will ignore that as a ‘&lt;em&gt;comment&lt;/em&gt;’ (i.e., it won’t do nothing). Comments are usually used in shell scripts rather than in interactive mode, as descriptors of the action to be taken.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If you pass commands without the ‘#’ symbol, the shell reads the inputs and divides them into ‘&lt;em&gt;words&lt;/em&gt;’ and ‘&lt;em&gt;operators&lt;/em&gt;’. By &lt;em&gt;words&lt;/em&gt; we mean any commands like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cd&lt;/code&gt; (change directory) or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cat&lt;/code&gt; (concatenate files to standard output), plus other constructs related to the specific command; by &lt;em&gt;operators&lt;/em&gt; we mean arithmetic operators (like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;+&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-&lt;/code&gt;) , relational operators (like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-eq&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-ne&lt;/code&gt;), boolean operators (like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-o&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;!&lt;/code&gt;), string operators(like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;!=&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;=&lt;/code&gt;), or file operators (like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-b&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;c&lt;/code&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Then, the shell parse the words and operators into subtypes, performs a series of intermediate steps like expansions and redirections, to finally execute the commands instructions.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;standard-input-output-and-error&quot;&gt;Standard input, output, and error&lt;/h2&gt;

&lt;p&gt;Before moving into more applied topics, I want to briefly review the concepts of standard input, standard output, and standard error, as I will use them constantly in this tutorial.&lt;/p&gt;

&lt;p&gt;Linux or unix-like systems have what is known as “standard streams of data”. Any process run in such systems is initialized with three data streams: &lt;em&gt;standard input&lt;/em&gt;, &lt;em&gt;standard ouput&lt;/em&gt;, and &lt;em&gt;standard error&lt;/em&gt;. By data we mean instructions in plain text formart.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Standard input&lt;/strong&gt; or “stdin”, referes the “place” where programs or processes &lt;em&gt;get&lt;/em&gt; information from. By default, the shell “takes” input from the keyboard. In other words, standard input is the default place and source of information for Linux/Bash programs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Standard output&lt;/strong&gt; or “stdout”, referes the “place” where programs or processes &lt;em&gt;send&lt;/em&gt; information to. By default, the shell output will be directed to the screen or monitor (i.e., printed in the terminal), but it can also for to a text file or a printer. In other words, standard outut is the default place where information is send after processing.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Standard error&lt;/strong&gt; or “stderr”, referes the “place” where programs or processes &lt;em&gt;send&lt;/em&gt; errors. By default, the shell output will be directed to the screen or the monitor(i.e., printed in the terminal). In other words, standard error is the default place where the shell send messages about processes that went wrong.&lt;/p&gt;

&lt;p&gt;Knowing this concepts will make undertanding bash documentation much easier.&lt;/p&gt;

&lt;h2 id=&quot;shell-commands-basics&quot;&gt;Shell commands basics&lt;/h2&gt;

&lt;p&gt;Broadly speaking, there two types of shell commands: &lt;em&gt;single&lt;/em&gt; commands and &lt;em&gt;composed&lt;/em&gt; commands.&lt;/p&gt;

&lt;h3 id=&quot;single-commands&quot;&gt;Single commands&lt;/h3&gt;

&lt;p&gt;Single commands are a combination of the command itself, a blank space, a the command arguments. For instance:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ &lt;span class=&quot;nb&quot;&gt;ls&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-a&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here we have:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ls&lt;/code&gt; : the command to list information about files&lt;/li&gt;
  &lt;li&gt;A white space&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-a&lt;/code&gt; : an argument saying “do not ignore files starting with .” (hidden files).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;composed-commands&quot;&gt;Composed commands&lt;/h3&gt;

&lt;p&gt;Composed commands are created by combining simple commands into &lt;em&gt;pipelines&lt;/em&gt;, &lt;em&gt;lists&lt;/em&gt;, &lt;em&gt;compounds&lt;/em&gt;, and &lt;em&gt;coproceses&lt;/em&gt;. We will examine the first three, as are the ones more commonly used.&lt;/p&gt;

&lt;h4 id=&quot;pipeline&quot;&gt;Pipeline&lt;/h4&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;A pipeline is a sequence of one or more commands separated by one of the control operators ‘&lt;/td&gt;
      &lt;td&gt;’ or ‘&lt;/td&gt;
      &lt;td&gt;&amp;amp;’, where the &lt;em&gt;output&lt;/em&gt; of the first command becomes the &lt;em&gt;input&lt;/em&gt; of the next. For instance:&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ &lt;span class=&quot;nb&quot;&gt;ls&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-l&lt;/span&gt; | &lt;span class=&quot;nb&quot;&gt;grep&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;.txt&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ls&lt;/code&gt; command list the files in the current directory, and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grep&lt;/code&gt; command will print the lines matching the “.txt” extension to the terminal.&lt;/p&gt;

&lt;h4 id=&quot;lists&quot;&gt;Lists&lt;/h4&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Lists are a sequence of one or more pipelines separated by one of the operators ‘;’, ‘&amp;amp;’, ‘&amp;amp;&amp;amp;’, or ‘&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;’. For instance:&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ &lt;span class=&quot;nb&quot;&gt;ls&lt;/span&gt; | &lt;span class=&quot;nb&quot;&gt;grep&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;.txt&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;ls&lt;/span&gt; | &lt;span class=&quot;nb&quot;&gt;grep&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;.csv&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The pipeline on the left side of &lt;em&gt;&amp;amp;&amp;amp;&lt;/em&gt; will print the files matching the “.txt” , and the pipeline on the right side, will print the files matching the “.csv” extension, only if the left side was executed successfully.&lt;/p&gt;

&lt;h4 id=&quot;compound-commands&quot;&gt;Compound commands&lt;/h4&gt;

&lt;p&gt;Compound commands are shell programming constructs that allow for more complex operations, particularly related to control flow. Compound commands are further divided into: looping constructs, conditional constructs, and grouping constructs. This type of commands begin with a reserved keyword indicating the beginning of the processes, and another keyword to indicate the end. For instance:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ &lt;span class=&quot;k&quot;&gt;while &lt;/span&gt;test-commands&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do &lt;/span&gt;consequent-commands&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This will tell the shell to repeat certain action (&lt;em&gt;consequent-commands&lt;/em&gt;), while some other condition is true (&lt;em&gt;test-commands&lt;/em&gt;). The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;while&lt;/code&gt; keyword indicates the beginning, and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;done&lt;/code&gt; keyword indicate the end.&lt;/p&gt;

&lt;h2 id=&quot;a-curated-list-of-basic-shell-commands&quot;&gt;A curated list of basic shell commands&lt;/h2&gt;

&lt;p&gt;Now that we have enough background knowledge about the inner workings of the shell, we will review a list of the, in my opinion, most useful commands in a research context. This is the most important and practical part of this tutorial. I don’t pretend to be exhaustive here. More commands will be introduced in later sections.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;HEADS UP&lt;/em&gt;&lt;/strong&gt;: In case you use the GitHub repository to follow the examples, be aware the exact output showed here may be different, given that this is a project in constant development. However, the instructions and command description hold true.&lt;/p&gt;

&lt;h3 id=&quot;basic-commands&quot;&gt;Basic commands&lt;/h3&gt;

&lt;h4 id=&quot;echo&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;echo&lt;/code&gt;&lt;/h4&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;echo&lt;/code&gt; command display text in the terminal. Is often used in combination with other operators to pass information to a file.&lt;/p&gt;

&lt;h5 id=&quot;simple-display&quot;&gt;Simple display&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ &lt;span class=&quot;nb&quot;&gt;echo &lt;/span&gt;this is my first &lt;span class=&quot;nb&quot;&gt;command
&lt;/span&gt;this is my first &lt;span class=&quot;nb&quot;&gt;command&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;combining-to-pass-text-to-a-file&quot;&gt;Combining to pass text to a file&lt;/h5&gt;

&lt;p&gt;This line will pass the text into the file, instead of printing in the terminal&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ &lt;span class=&quot;nb&quot;&gt;echo &lt;/span&gt;this is my first &lt;span class=&quot;nb&quot;&gt;command&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; first-command.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;clear&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;clear&lt;/code&gt;&lt;/h4&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;clear&lt;/code&gt; command clears the terminal. It is often the case the your terminal will get cluttered with information, which can make things confusing to work with, so clearing often it is useful.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ clear
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;man&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;man&lt;/code&gt;&lt;/h4&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;man&lt;/code&gt; command allows you to access the on-line reference manual in the terminal. Any time that you want to learn anything about any command, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;man&lt;/code&gt; will help you.&lt;/p&gt;

&lt;p&gt;The basic syntax is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;man [NAME OF THE COMMAND]&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;For instance you can learn about the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;man&lt;/code&gt; command itself by&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ man man
MAN&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;1&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;                        Manual pager utils                        MAN&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;1&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

NAME
       man - an interface to the on-line reference manuals

...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;help&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;help&lt;/code&gt;&lt;/h4&gt;

&lt;p&gt;Technically, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;help&lt;/code&gt; is not a command, but an option for most commands. I’m including this here because along with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;man&lt;/code&gt; is one of the most handy tools to learn about and use bash. You can think on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;help&lt;/code&gt; as quicker way to look at the documentation.&lt;/p&gt;

&lt;p&gt;For instance, to learn more about  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;man&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ man &lt;span class=&quot;nt&quot;&gt;--help&lt;/span&gt;
Usage: man &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;OPTION...] &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;SECTION] PAGE...

  &lt;span class=&quot;nt&quot;&gt;-C&lt;/span&gt;, &lt;span class=&quot;nt&quot;&gt;--config-file&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;FILE     use this user configuration file
  &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt;, &lt;span class=&quot;nt&quot;&gt;--debug&lt;/span&gt;                emit debugging messages
  &lt;span class=&quot;nt&quot;&gt;-D&lt;/span&gt;, &lt;span class=&quot;nt&quot;&gt;--default&lt;/span&gt;              reset all options to their default values
      &lt;span class=&quot;nt&quot;&gt;--warnings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[=&lt;/span&gt;WARNINGS]  &lt;span class=&quot;nb&quot;&gt;enable &lt;/span&gt;warnings from groff
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;file-commands&quot;&gt;File commands&lt;/h3&gt;

&lt;p&gt;There are some operations that you can perform with files, like text files, comma separated files, and others.&lt;/p&gt;

&lt;h4 id=&quot;ls&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ls&lt;/code&gt;&lt;/h4&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ls&lt;/code&gt; command &lt;em&gt;list directory contents&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The syntax is: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ls [OPTION(s)] [FILE(s)]&lt;/code&gt;&lt;/p&gt;

&lt;h5 id=&quot;list-directory-contents&quot;&gt;List directory contents&lt;/h5&gt;

&lt;p&gt;The simples action is to list the contents in the current directory by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ls&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ &lt;span class=&quot;nb&quot;&gt;ls
&lt;/span&gt;a-folder  characters-folder        got-characters.txt           __init__.py
b-folder  got-characters-copy.txt  harry-potter-characters.txt  unix_shell.md
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;list-directory-contents-including-files-starting-with-&quot;&gt;List directory contents including files starting with ‘.’&lt;/h5&gt;

&lt;p&gt;Hidden files usually begin with a dot in UNIX-bases systems. To list them along with the visible files you need to add the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-a&lt;/code&gt; option. Here I’m adding &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;./characters-folder&lt;/code&gt; to list the contents in that directory instead of the current one.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ &lt;span class=&quot;nb&quot;&gt;ls&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-a&lt;/span&gt; ./characters-folder
&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;   got-characters-copy.txt  harry-potter-characters.txt
..  got-characters.txt       .hidden-file.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;list-directory-contents-in-long-format&quot;&gt;List directory contents in long format&lt;/h5&gt;

&lt;p&gt;Listing in long format reveals detailed information about each file. To do this we add the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-l&lt;/code&gt; option&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ &lt;span class=&quot;nb&quot;&gt;ls&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-l&lt;/span&gt;
drwxr-xr-x 3 pablo pablo  4096 Feb  8 15:33 b-folder
drwxr-xr-x 2 pablo pablo  4096 Feb  8 15:40 characters-folder
&lt;span class=&quot;nt&quot;&gt;-rw-r--r--&lt;/span&gt; 1 pablo pablo    18 Feb  8 15:23 got-characters-copy.txt
&lt;span class=&quot;nt&quot;&gt;-rw-rw-r--&lt;/span&gt; 1 pablo pablo    18 Feb  8 12:53 got-characters.txt
&lt;span class=&quot;nt&quot;&gt;-rw-r--r--&lt;/span&gt; 1 pablo pablo    19 Feb  8 15:06 harry-potter-characters.txt
&lt;span class=&quot;nt&quot;&gt;-rw-r--r--&lt;/span&gt; 1 pablo pablo     0 Feb  8 12:43 __init__.py
&lt;span class=&quot;nt&quot;&gt;-rw-r--r--&lt;/span&gt; 1 pablo pablo 11515 Feb  8 15:46 unix_shell.md
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There is a lot of information here. Let’s examine this part by part:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The first character indicates the &lt;a href=&quot;https://en.wikipedia.org/wiki/Unix_file_types&quot;&gt;file type&lt;/a&gt;. In this example is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;d&lt;/code&gt; for directory or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-&lt;/code&gt; for regular file. There are more types that you can look up on-line.&lt;/li&gt;
  &lt;li&gt;The next nine characters indicate file permissions.
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;Characters 1-3&lt;/em&gt;, are for the &lt;em&gt;user&lt;/em&gt;. Here &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-rw&lt;/code&gt; means &lt;em&gt;reading&lt;/em&gt; and &lt;em&gt;writing&lt;/em&gt; permissions&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Characters 4-6&lt;/em&gt; are for the &lt;em&gt;group&lt;/em&gt;. Here &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-r-&lt;/code&gt; means &lt;em&gt;reading&lt;/em&gt; permission only&lt;/li&gt;
      &lt;li&gt;Characters 7-9 are for &lt;em&gt;others&lt;/em&gt;. Here &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-r-&lt;/code&gt; means &lt;em&gt;reading&lt;/em&gt; permission only&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The number after the permission string indicates the the number of &lt;a href=&quot;https://en.wikipedia.org/wiki/Hard_link&quot;&gt;hard links&lt;/a&gt; to the file&lt;/li&gt;
  &lt;li&gt;Next, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pablo pablo&lt;/code&gt; indicates the &lt;em&gt;file owner&lt;/em&gt; and the &lt;em&gt;group&lt;/em&gt;, respectively.&lt;/li&gt;
  &lt;li&gt;The number after the group name, indicates the file size in bytes.&lt;/li&gt;
  &lt;li&gt;Next, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Feb  8 15:33&lt;/code&gt; indicates the &lt;em&gt;date&lt;/em&gt; and &lt;em&gt;time&lt;/em&gt; of the &lt;em&gt;last modification&lt;/em&gt; to the file.&lt;/li&gt;
  &lt;li&gt;Finally, the last column display the file name.&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;list-directory-contents-by-custom-order&quot;&gt;List directory contents by custom order&lt;/h5&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ls&lt;/code&gt; command will sort files alphabetically by default.  You can use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--sort=KEYWORD&lt;/code&gt; to sort files by &lt;em&gt;size&lt;/em&gt;, &lt;em&gt;time&lt;/em&gt;, &lt;em&gt;version&lt;/em&gt;, &lt;em&gt;extension&lt;/em&gt;.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# sort by size&lt;/span&gt;
➜ &lt;span class=&quot;nb&quot;&gt;ls&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-l&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--sort&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;size
total 40
&lt;span class=&quot;nt&quot;&gt;-rw-r--r--&lt;/span&gt; 1 pablo pablo 13396 Feb  8 16:09 unix_shell.md
drwxr-xr-x 2 pablo pablo  4096 Feb  8 15:25 a-folder
drwxr-xr-x 3 pablo pablo  4096 Feb  8 15:33 b-folder
drwxr-xr-x 2 pablo pablo  4096 Feb  8 15:40 characters-folder
&lt;span class=&quot;nt&quot;&gt;-rw-r--r--&lt;/span&gt; 1 pablo pablo    19 Feb  8 15:06 harry-potter-characters.txt
&lt;span class=&quot;nt&quot;&gt;-rw-r--r--&lt;/span&gt; 1 pablo pablo    18 Feb  8 15:23 got-characters-copy.txt
&lt;span class=&quot;nt&quot;&gt;-rw-rw-r--&lt;/span&gt; 1 pablo pablo    18 Feb  8 12:53 got-characters.txt
&lt;span class=&quot;nt&quot;&gt;-rw-r--r--&lt;/span&gt; 1 pablo pablo     0 Feb  8 12:43 __init__.py
&lt;span class=&quot;c&quot;&gt;# sor by time&lt;/span&gt;
➜ &lt;span class=&quot;nb&quot;&gt;ls&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-l&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--sort&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;time
&lt;/span&gt;total 40
&lt;span class=&quot;nt&quot;&gt;-rw-r--r--&lt;/span&gt; 1 pablo pablo 13396 Feb  8 16:09 unix_shell.md
drwxr-xr-x 2 pablo pablo  4096 Feb  8 15:40 characters-folder
drwxr-xr-x 3 pablo pablo  4096 Feb  8 15:33 b-folder
drwxr-xr-x 2 pablo pablo  4096 Feb  8 15:25 a-folder
&lt;span class=&quot;nt&quot;&gt;-rw-r--r--&lt;/span&gt; 1 pablo pablo    18 Feb  8 15:23 got-characters-copy.txt
&lt;span class=&quot;nt&quot;&gt;-rw-r--r--&lt;/span&gt; 1 pablo pablo    19 Feb  8 15:06 harry-potter-characters.txt
&lt;span class=&quot;nt&quot;&gt;-rw-rw-r--&lt;/span&gt; 1 pablo pablo    18 Feb  8 12:53 got-characters.txt
&lt;span class=&quot;nt&quot;&gt;-rw-r--r--&lt;/span&gt; 1 pablo pablo     0 Feb  8 12:43 __init__.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;cat&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cat&lt;/code&gt;&lt;/h4&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cat&lt;/code&gt; command &lt;em&gt;concatenates and print contents of a file to standard output&lt;/em&gt; (I know, confusingly, it has nothing to do with cats).&lt;/p&gt;

&lt;p&gt;The syntax is: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cat [OPTION(s)] [FILE(s)]&lt;/code&gt;&lt;/p&gt;

&lt;h5 id=&quot;display-files-contents&quot;&gt;Display file’s contents&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ &lt;span class=&quot;nb&quot;&gt;cat &lt;/span&gt;harry-potter-characters.txt
Harry
Hermione
Ron
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;display-multiple-files-contents&quot;&gt;Display multiple file’s contents&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ &lt;span class=&quot;nb&quot;&gt;cat &lt;/span&gt;got-characters.txt
Harry
Hermione
Ron
Jon
Arya
Daenerys
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;display-numerated-files-contents&quot;&gt;Display numerated file’s contents&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ &lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; got-characters.txt
1	Jon
2	Arya
3	Daenerys
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;create-a-new-file-concatenating-standard-input-text&quot;&gt;Create a new file concatenating standard input text&lt;/h5&gt;

&lt;p&gt;This will prompt you to enter input text. You type some text and press &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;enter&lt;/code&gt;. Once you are done, press &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Control + D&lt;/code&gt; to exit.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ &lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;new-characters.txt
Dumbledore
Voldemort
Jaime
Tyrion
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, if you type &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cat new-characters.txt&lt;/code&gt; the list of names will be printed in the terminal.&lt;/p&gt;

&lt;h4 id=&quot;cp&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cp&lt;/code&gt;&lt;/h4&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cp&lt;/code&gt; command &lt;em&gt;copy files and directories&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The syntax is: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cp [OPTION(s)][SOURCE] [DESTINY]&lt;/code&gt;&lt;/p&gt;

&lt;h5 id=&quot;copy-contents-of-a-file-into-another&quot;&gt;Copy contents of a file into another&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cp &lt;/span&gt;got-characters.txt got-characters-copy.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cat&lt;/code&gt; the contents of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;got-characters-copy.txt&lt;/code&gt; you will see the duplicated contents&lt;/p&gt;

&lt;h5 id=&quot;copy-a-file-from-one-directory-into-another&quot;&gt;Copy a file from one directory into another&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cp &lt;/span&gt;got-characters.txt ./a-folder
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;copy-a-directory-into-another&quot;&gt;Copy a directory into another&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cp&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-r&lt;/span&gt; /a-foler ./b-folder
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-r&lt;/code&gt; option stand for &lt;em&gt;recursive&lt;/em&gt;. This option is required to copy directories.&lt;/p&gt;

&lt;h4 id=&quot;touch&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;touch&lt;/code&gt;&lt;/h4&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;touch&lt;/code&gt; command changes files timestamps. However, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;touch&lt;/code&gt; is commonly used to create new empty files.&lt;/p&gt;

&lt;p&gt;To change timestamps of an &lt;em&gt;existing&lt;/em&gt; &lt;em&gt;file&lt;/em&gt; the syntax is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;touch [OPTION(s)] [FILE(s)]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;To create an &lt;em&gt;new empty file&lt;/em&gt; the syntax is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;touch [FILE(s)]&lt;/code&gt;&lt;/p&gt;

&lt;h5 id=&quot;change-the-time-stamp-of-an-existing-file&quot;&gt;Change the time-stamp of an existing file&lt;/h5&gt;

&lt;p&gt;First, I’ll review the access and modification times using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stat&lt;/code&gt; command, to then use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;touch&lt;/code&gt; command to change those times, and finally &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stat&lt;/code&gt; again to verify the changes.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# review access time and modification time&lt;/span&gt;
➜ &lt;span class=&quot;nb&quot;&gt;stat &lt;/span&gt;harry-potter-characters.txt 
  File: harry-potter-characters.txt
  Size: 19        	Blocks: 8          IO Block: 4096   regular file
Device: 10307h/66311d	Inode: 3440036     Links: 1
Access: &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;0644/-rw-r--r--&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;  Uid: &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt; 1000/   pablo&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;   Gid: &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt; 1000/   pablo&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
Access: 2020-02-08 15:39:41.039475209 &lt;span class=&quot;nt&quot;&gt;-0600&lt;/span&gt;
Modify: 2020-02-08 15:06:40.426141133 &lt;span class=&quot;nt&quot;&gt;-0600&lt;/span&gt;
Change: 2020-02-08 15:06:40.426141133 &lt;span class=&quot;nt&quot;&gt;-0600&lt;/span&gt;
 Birth: -

&lt;span class=&quot;c&quot;&gt;# change access and modficiation time&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;touch&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-am&lt;/span&gt; harry-potter-characters.txt 

&lt;span class=&quot;c&quot;&gt;# review access time and modification time after change&lt;/span&gt;
➜ &lt;span class=&quot;nb&quot;&gt;stat &lt;/span&gt;harry-potter-characters.txt     
  File: harry-potter-characters.txt
  Size: 19        	Blocks: 8          IO Block: 4096   regular file
Device: 10307h/66311d	Inode: 3440036     Links: 1
Access: &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;0644/-rw-r--r--&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;  Uid: &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt; 1000/   pablo&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;   Gid: &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt; 1000/   pablo&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
Access: 2020-02-08 17:37:05.054635614 &lt;span class=&quot;nt&quot;&gt;-0600&lt;/span&gt;
Modify: 2020-02-08 17:37:05.054635614 &lt;span class=&quot;nt&quot;&gt;-0600&lt;/span&gt;
Change: 2020-02-08 17:37:05.054635614 &lt;span class=&quot;nt&quot;&gt;-0600&lt;/span&gt;
 Birth: -
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-a&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-m&lt;/code&gt; options change the &lt;em&gt;access time&lt;/em&gt; and &lt;em&gt;modification time&lt;/em&gt;, respectively. If you compare the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Access&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Modify&lt;/code&gt; entries from the top and the bottom of the code block, you’ll see the changes.&lt;/p&gt;

&lt;h5 id=&quot;create-a-new-empty-file&quot;&gt;Create a new empty file&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ &lt;span class=&quot;nb&quot;&gt;touch &lt;/span&gt;my-empty-file.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is the most common use of the touch command. You can create multiple empty files by:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ &lt;span class=&quot;nb&quot;&gt;touch &lt;/span&gt;my-empty-file-1.txt my-empty-file-2.txt my-empty-file-3.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;mv&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mv&lt;/code&gt;&lt;/h4&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mv&lt;/code&gt; command moves files around your file system, and it is also used to rename files.&lt;/p&gt;

&lt;p&gt;The syntax to move files is: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mv [OPTION(s)]]  [SOURCE] [DESTINY]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The syntax to rename a file is: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mv [OPTION(s)] [CURRENT NAME] [WANTED NAME]&lt;/code&gt;&lt;/p&gt;

&lt;h5 id=&quot;moving-a-file&quot;&gt;Moving a file&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ &lt;span class=&quot;nb&quot;&gt;mv &lt;/span&gt;my-empty-file.txt ./a-folder
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Notice that here you can use either absolute or relative paths.&lt;/p&gt;

&lt;h5 id=&quot;renaming-a-file&quot;&gt;Renaming a file&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ &lt;span class=&quot;nb&quot;&gt;mv &lt;/span&gt;my-empty-file-3.txt my-empty-file-4.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;grep&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grep&lt;/code&gt;&lt;/h4&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grep&lt;/code&gt; command print lines matching a desired pattern given some standard input. By standard input, we mean content printed to the terminal or a file. This is one of the most handy command line tools in bash, particularly when combined with other tools like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ls&lt;/code&gt; , &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;find&lt;/code&gt;, and &lt;a href=&quot;&apos;https://en.wikipedia.org/wiki/Regular_expression&apos;&quot;&gt;regular expressions&lt;/a&gt;. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grep&lt;/code&gt; documentation is very extensive, and it can be treated as a topic in itself one combined with regular expression. Here we will review a few of the main &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grep&lt;/code&gt; functions.&lt;/p&gt;

&lt;p&gt;The syntax for basic pattern matching is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grep [OPTION(s)] [PATTERN] [FILE(s)]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The syntax for pattern matching as extended regular expressions is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grep [OPTION(s)] -e [PATTERN] [FILE(s)]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The syntax for pattern matching using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[PATTERN]&lt;/code&gt; as a list of fixed string (instead of regular expression) is  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grep [OPTION(s)] -f [STRING] [FILE(s)]&lt;/code&gt;&lt;/p&gt;

&lt;h5 id=&quot;basic-pattern-matching&quot;&gt;Basic pattern matching&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ &lt;span class=&quot;nb&quot;&gt;grep &lt;/span&gt;Harry harry-potter-characters.txt 
Harry
➜ &lt;span class=&quot;nb&quot;&gt;grep &lt;/span&gt;Jon got-characters.txt 
Jon
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;extended-regular-expression-pattern-matching&quot;&gt;Extended regular expression pattern matching&lt;/h5&gt;

&lt;p&gt;In Linux, there is no difference between basic and extended pattern matching. In other system, it may be the case that basic pattern matching is not as general and powerful as extended pattern matching. If you are in need of using complex regular expression patterns to search, you may need to add the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;e&lt;/code&gt; option for that to work. I’ll not cover this option now since regular expressions knowledge is required.&lt;/p&gt;

&lt;h5 id=&quot;string-pattern-matching&quot;&gt;String pattern matching&lt;/h5&gt;

&lt;p&gt;The difference between basic pattern matching and string pattern matching, is that the latter does not use regular expression rules to search. For instance, suppose you want to search something like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.*&lt;/code&gt; in a file:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# pattern matching with regular expressions&lt;/span&gt;
➜ &lt;span class=&quot;nb&quot;&gt;grep&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\.\*&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; filename
&lt;span class=&quot;c&quot;&gt;# pattern matching with strings&lt;/span&gt;
➜ &lt;span class=&quot;nb&quot;&gt;grep&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-F&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;.*&quot;&lt;/span&gt; filename
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the pattern matching case you have to add the backslash &lt;a href=&quot;https://en.wikipedia.org/wiki/Escape_character&quot;&gt;escape character&lt;/a&gt; to prevent the interpretation of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*&lt;/code&gt; as a &lt;a href=&quot;https://en.wikipedia.org/wiki/Wildcard_character&quot;&gt;wildcard character&lt;/a&gt;. In the string search case, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.*&lt;/code&gt; is interpreted literally, so no escape character is needed.&lt;/p&gt;

&lt;h5 id=&quot;using-the-output-of-another-command-as-grep-input&quot;&gt;Using the output of another command as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grep&lt;/code&gt; input&lt;/h5&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grep&lt;/code&gt; command is often used in combination with others bash tools. For instance, we can list all the files in the current directory with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ls&lt;/code&gt;, then pass the output as input to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grep&lt;/code&gt;, and search for all files containing the string “got”.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ &lt;span class=&quot;nb&quot;&gt;ls&lt;/span&gt; | &lt;span class=&quot;nb&quot;&gt;grep &lt;/span&gt;got                    
got-characters-copy.txt
got-characters.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wc&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wc&lt;/code&gt; command prints how many new lines, words, and bytes are in  file.&lt;/p&gt;

&lt;p&gt;The basic syntax is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wc [OPTION(s)] [FILE]&lt;/code&gt;&lt;/p&gt;

&lt;h5 id=&quot;basic-count&quot;&gt;Basic count&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ &lt;span class=&quot;nb&quot;&gt;wc &lt;/span&gt;got-characters.txt
3  3 18 got-characters.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In order, this is 3 words, 3 new lines, and 18 bytes.&lt;/p&gt;

&lt;h5 id=&quot;count-words-only&quot;&gt;Count words only&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ &lt;span class=&quot;nb&quot;&gt;wc&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-w&lt;/span&gt; got-characters.txt
3 got-characters.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;count-new-lines-only&quot;&gt;Count new lines only&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ &lt;span class=&quot;nb&quot;&gt;wc&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-w&lt;/span&gt; got-characters.txt
3 got-characters.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;count-the-number-of-characters&quot;&gt;Count the number of characters&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ &lt;span class=&quot;nb&quot;&gt;wc&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; got-characters.txt
18 got-characters.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;count-the-number-of-bytes&quot;&gt;Count the number of bytes&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ &lt;span class=&quot;nb&quot;&gt;wc&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; got-characters.txt
18 got-characters.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;head&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;head&lt;/code&gt;&lt;/h4&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;head&lt;/code&gt; command prints the first elements of a file, by default, the first 10 elements.&lt;/p&gt;

&lt;p&gt;The basic syntax is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;head [OPTION] [FILE]&lt;/code&gt;&lt;/p&gt;

&lt;h5 id=&quot;printing-the-first-10-lines-of-a-file&quot;&gt;Printing the first 10 lines of a file&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ &lt;span class=&quot;nb&quot;&gt;head &lt;/span&gt;fruits.txt
apple
pear
peach
grape
kiwi
melon
fig
cucumber
cherry
banana
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;printing-the-first-n-number-of-lines-of-a-file&quot;&gt;Printing the first n number of lines of a file&lt;/h5&gt;

&lt;p&gt;You can print an arbitrary number &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n&lt;/code&gt; of lines in a file by using th &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n&lt;/code&gt; option.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ &lt;span class=&quot;nb&quot;&gt;head&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; 5 fruits.txt
apple
pear
peach
grape
kiwi
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;printing-the-first-n-number-of-bytes-of-a-file&quot;&gt;Printing the first n number of bytes of a file&lt;/h5&gt;

&lt;p&gt;You can print an arbitrary number &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n&lt;/code&gt; of bytes in a file by using th &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-c&lt;/code&gt; option.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ &lt;span class=&quot;nb&quot;&gt;head&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; 20 fruits.txt
apple
pear
peach
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;tail&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tail&lt;/code&gt;&lt;/h5&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tail&lt;/code&gt; command prints the last elements of a file, by default, the last 10 elements.&lt;/p&gt;

&lt;p&gt;The basic syntax is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tail [OPTION] [FILE]&lt;/code&gt;&lt;/p&gt;

&lt;h5 id=&quot;printing-the-last-10-lines-of-a-file&quot;&gt;Printing the last 10 lines of a file&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ &lt;span class=&quot;nb&quot;&gt;tail &lt;/span&gt;fruits.txt
cucumber
cherry
banana
avocado
coconut
orange
papaya
watermelon
mango
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;printing-the-last-n-lines-of-a-file&quot;&gt;Printing the last n lines of a file&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ &lt;span class=&quot;nb&quot;&gt;tail&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; 5 fruits.txt
coconut
orange
papaya
watermelon
mango
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;printing-the-last-n-number-of-bytes-of-a-file&quot;&gt;Printing the last n number of bytes of a file&lt;/h5&gt;

&lt;p&gt;You can print an arbitrary number &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n&lt;/code&gt; of bytes in a file by using th &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-c&lt;/code&gt; option.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ &lt;span class=&quot;nb&quot;&gt;tail&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; 20 fruits.txt
ya
watermelon
mango
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;rm&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rm&lt;/code&gt;&lt;/h4&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rm&lt;/code&gt; command removes files or directories. This another very important and useful shell tool.&lt;/p&gt;

&lt;p&gt;The basic syntax is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rm [OPTION(s)] [FILE]&lt;/code&gt;&lt;/p&gt;

&lt;h5 id=&quot;removing-a-single-file&quot;&gt;Removing a single file&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ &lt;span class=&quot;nb&quot;&gt;rm &lt;/span&gt;my-empty-file-4.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You won’t see any printed output but you can check the file has been removed with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ls&lt;/code&gt;.&lt;/p&gt;

&lt;h5 id=&quot;removing-multiple-files&quot;&gt;Removing multiple files&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ &lt;span class=&quot;nb&quot;&gt;rm &lt;/span&gt;my-empty-file-1.txt my-empty-file-2.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;removing-a-directory&quot;&gt;Removing a directory&lt;/h5&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rm&lt;/code&gt; alone won’t work this time. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-r&lt;/code&gt; flag (“recursively”) must be added.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# create an empty folder as example&lt;/span&gt;
➜ &lt;span class=&quot;nb&quot;&gt;mkdir &lt;/span&gt;empty-dir
➜ &lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-r&lt;/span&gt; empt-dir
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;directory-commands&quot;&gt;Directory commands&lt;/h3&gt;

&lt;p&gt;Directory commands are actions you can perform with folders or directories.&lt;/p&gt;

&lt;h4 id=&quot;pwd&quot;&gt;pwd&lt;/h4&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pwd&lt;/code&gt; prints the current workind directory to the terminal.&lt;/p&gt;

&lt;p&gt;The basic syntax is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pwd [OPTION(s)]&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# you will see your own full path printed out to the terminal&lt;/span&gt;
➜ &lt;span class=&quot;nb&quot;&gt;pwd&lt;/span&gt;
/mnt/c/Users/pablo/Desktop/projects/unix_shell
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;cd&quot;&gt;cd&lt;/h4&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cd&lt;/code&gt; command changes the current working directory to another.&lt;/p&gt;

&lt;p&gt;The basic syntax is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cd [OPTION(s)] [DIRECTORY]&lt;/code&gt;&lt;/p&gt;

&lt;h5 id=&quot;move-to-an-specific-folder-down&quot;&gt;Move to an specific folder “down”&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# move from current directory to characrers-folder/&lt;/span&gt;
➜ &lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;characters-folder/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;move-one-folder-up&quot;&gt;Move one folder up&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# move back from characrers-folder/ to unix_shell/&lt;/span&gt;
➜ &lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; ..
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;move-two-folders-up&quot;&gt;Move two folders up&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# go back to unix_shell/ first&lt;/span&gt;
➜ &lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;unix_shell/
&lt;span class=&quot;c&quot;&gt;# move back from unix_shell/ to repo-directory&lt;/span&gt;
➜ &lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; ../..
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can follow the patten of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;../&lt;/code&gt; to move several directories up.&lt;/p&gt;

&lt;h5 id=&quot;move-the-home-directory&quot;&gt;Move the home directory&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# this is no mistake, cd alone would change the directory to home&lt;/span&gt;
➜ &lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;mkdir&quot;&gt;mkdir&lt;/h4&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mkdir&lt;/code&gt; command creates a new directory.&lt;/p&gt;

&lt;p&gt;The basic syntax is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mkdir [OPTION(s)] [DIRECTORY-NAME]&lt;/code&gt;&lt;/p&gt;

&lt;h5 id=&quot;make-a-single-directory&quot;&gt;Make a single directory&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ &lt;span class=&quot;nb&quot;&gt;mkdir &lt;/span&gt;my-folder
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;make-multiple-directories&quot;&gt;Make multiple directories&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ &lt;span class=&quot;nb&quot;&gt;mkdir &lt;/span&gt;my-folder-2 my-folder-3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can create as many directories as you need “below” the current working directory by following the same patter&lt;/p&gt;

&lt;h3 id=&quot;system-commands&quot;&gt;System commands&lt;/h3&gt;

&lt;h4 id=&quot;ps&quot;&gt;ps&lt;/h4&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ps&lt;/code&gt; command prints a snapshot of the corrent active processes to the terminal. This command has many options which are useful for system administrators and othes. For our purposes, just knowing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ps&lt;/code&gt; does that is enough.&lt;/p&gt;

&lt;p&gt;The basic syntax is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ps [OPTION(s)]&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# your output will differ depending on your active processes&lt;/span&gt;
➜ ps
PID   TTY          TIME CMD
  8   pts/0    00:00:01 zsh
 1042 pts/0    00:00:00 ps
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;kill&quot;&gt;kill&lt;/h4&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kill&lt;/code&gt; command terminates an active process.&lt;/p&gt;

&lt;p&gt;The basic syntax is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kill PID&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kill -s signalName PID&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The PID is the process identification number. You can find the PID of a process with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ps&lt;/code&gt; command.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;kill &lt;/span&gt;PID
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here is an example of when I have used &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kill&lt;/code&gt;: I closed a Jupyter Notebook session by typying &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Ctrl&lt;/code&gt; + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Z&lt;/code&gt; in the terminal, instead of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Ctrl&lt;/code&gt; + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;C&lt;/code&gt;. Sometimes happens that if you try launch Jupyter again won’t work because the default port is in use. Then you have to terminate the process Jupyter process, which is still running in the background, to be able to launch Jupyter again.&lt;/p&gt;

&lt;h4 id=&quot;top&quot;&gt;top&lt;/h4&gt;

&lt;h4 id=&quot;whoami&quot;&gt;whoami&lt;/h4&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;whoami&lt;/code&gt; command prints the current user id to the terminal.&lt;/p&gt;

&lt;p&gt;The basic syntax is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;whoami [OPTION&apos;s]&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# here you will see your used id&lt;/span&gt;
➜ &lt;span class=&quot;nb&quot;&gt;whoami
&lt;/span&gt;pablo
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;inputoutput-redirection-commands&quot;&gt;Input/Output redirection commands&lt;/h3&gt;

&lt;p&gt;As I explained before, the unix shell has three streams of data: input, output, and error messages. Such information can be redirected before execution. An example is the pipeline &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;|&lt;/code&gt; that we reviewd in the composed commands section.&lt;/p&gt;

&lt;h4 id=&quot;redirection-operators&quot;&gt;Redirection operators&lt;/h4&gt;

&lt;p&gt;Redirection operators are special characters used for change the direction in which streams of data flow. They can be located before or in between commands. Here is a list of several common operators.&lt;/p&gt;

&lt;h5 id=&quot;redirecting-input&quot;&gt;Redirecting input&lt;/h5&gt;

&lt;p&gt;To redirect input we use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;&lt;/code&gt; operator as:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ &lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; &amp;lt; got-characters.txt
Jon
Arya
Daenerys
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cat&lt;/code&gt; command is “fed” with the text file as input.&lt;/p&gt;

&lt;h5 id=&quot;redirecting-output&quot;&gt;Redirecting output&lt;/h5&gt;

&lt;p&gt;To redirect output we use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;gt;&lt;/code&gt; operator as:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;catbug&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; catbug.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This will cause the outut of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;echo &quot;catbug&quot;&lt;/code&gt; to be directed towards the catbug.txt. You can check that printing the catbug.txt contents.&lt;/p&gt;

&lt;h5 id=&quot;appending-redirected-output&quot;&gt;Appending redirected output&lt;/h5&gt;

&lt;p&gt;To append output we use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;gt;&amp;gt;&lt;/code&gt; operator as:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜ &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Plum&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; catbug.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This will cause to append (i.e., print at the end) the output of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;echo &quot;Plum&quot;&lt;/code&gt; , to the  catbug.txt file. If you use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;gt;&lt;/code&gt; instead, the contents will be replaced rather than appended. This is you will get only:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Plum
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Instead of:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;catbug
Plum
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;redirecting-standard-output-and-error&quot;&gt;Redirecting standard output and error&lt;/h5&gt;

&lt;p&gt;There are instances where in addition to the output of a process, you may want to print any error messages generated for something that went wrong. An example of this is running long processes, which may take hours, which you leave unattended. if something goes wrong while you are away, having the error messages printed to a text file may be very useful to fix your code later.&lt;/p&gt;

&lt;p&gt;This action is done with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;amp;&amp;gt;&lt;/code&gt; operator as:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cat &lt;/span&gt;nonexistent-file.txt &amp;amp;&amp;gt; empty.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The empty.txt file now must contain the “cat: nonexistent-file.txt: No such file or directory” message. If you use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;gt;&lt;/code&gt; instead, the error message will be printed to the terminal, and the empty.txt file will be empty.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;#table-of-contents&quot;&gt;back to the top&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;future-sections&quot;&gt;Future sections&lt;/h2&gt;

&lt;p&gt;This tutorial is not complete. I was not planning to release this yet, but it became necessary to help out some students to learn shell basics. These are the topics I hope to cover later.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Shell functions basics&lt;/li&gt;
  &lt;li&gt;Shell variables basics&lt;/li&gt;
  &lt;li&gt;Shell flow control basics&lt;/li&gt;
  &lt;li&gt;Shell pattern matching basics&lt;/li&gt;
  &lt;li&gt;Shell scripting basics&lt;/li&gt;
  &lt;li&gt;Alternative shells&lt;/li&gt;
  &lt;li&gt;Additional resources to learn&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;#table-of-contents&quot;&gt;back to the top&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 18 Jun 2020 00:00:00 +0800</pubDate>
        <link>//intro-unix-shell</link>
        <link href="/intro-unix-shell"/>
        <guid isPermaLink="true">/intro-unix-shell</guid>
      </item>
    
      <item>
        <title>Introduction to Jupyter Notebooks - set-up, user-guide, and best practices</title>
        <description>&lt;iframe src=&quot;https://github.com/sponsors/pabloinsente/card&quot; title=&quot;Sponsor pabloinsente&quot; height=&quot;225&quot; width=&quot;600&quot; style=&quot;border: 0;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;This tutorial contains video-lessons at the end of each section&lt;/li&gt;
  &lt;li&gt;The Jupyter Notebook version can be found in my GitHub &lt;a href=&quot;https://github.com/pabloinsente/intro-sc-python/blob/master/notebooks/intro-jupyter-ide.ipynb&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;ides-integrated-development-environments&quot;&gt;IDEs: Integrated Development Environments&lt;/h2&gt;

&lt;p&gt;There are several ways in which we can interact with Python:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Via the terminal in Python interactive mode&lt;/li&gt;
  &lt;li&gt;Via the terminal by running Python scripts written in a text editor&lt;/li&gt;
  &lt;li&gt;Via an Integrated Development Environment (IDE)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this mini-workshop I’ll focus on IDEs, as they are more commonly used alternative for data analysis and scientific computing with Python. Pretty much all educational content and examples you will find on-line about the subject are created in and for IDEs.&lt;/p&gt;

&lt;p&gt;Popular IDEs for Python are Spyder, PyCharm, VSCode, and Jupyter Notebooks. This are all valid options with advantages and disadvantages. I’ll focus in Jupyter Notebooks for a couple of reasons:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;They can be run in cloud-based computing environments easily&lt;/li&gt;
  &lt;li&gt;They allow to create interactive documents, combining text, code, and graphics, which is great for educational content&lt;/li&gt;
  &lt;li&gt;Most on-line examples and educational materials in scientific computing and data analysis are created in Jupyter Notebooks&lt;/li&gt;
  &lt;li&gt;It is beginner friendly and intuitive to use&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Jupyter Notebooks do have several drawbacks that can become a problem for more advance users, which may prefer to work in text-based IDEs like PyCharm or VSCode. I will briefly mention such drawbacks later, so you are aware of them. It is also worth notice that VSCode and PyCharm allow for Jupyter-like interfaces, which you can check it our &lt;a href=&quot;https://code.visualstudio.com/docs/python/jupyter-support&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://www.jetbrains.com/help/pycharm/jupyter-notebook-support.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;jupyter-notebooks&quot;&gt;Jupyter Notebooks&lt;/h2&gt;

&lt;p&gt;Jupyter is an &lt;a href=&quot;https://github.com/jupyterlab/jupyterlab&quot;&gt;open-source software&lt;/a&gt; for interactive computing for a variety of programming languages like Python, Julia, R, Ruby, Haskell, Javascript, Scala, and &lt;a href=&quot;https://github.com/jupyter/jupyter/wiki/Jupyter-kernels&quot;&gt;many others&lt;/a&gt;. The document you are reading right now is an example of a Jupyter Notebook.&lt;/p&gt;

&lt;p&gt;Jupyter Notebooks have become popular among researchers and data scientists because of several convenient characteristics:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Allows for the combination of a narrative, code, and the results of computations in one place&lt;/li&gt;
  &lt;li&gt;Easy to use intuitive interface&lt;/li&gt;
  &lt;li&gt;Flexibility on supporting multiple programming languages&lt;/li&gt;
  &lt;li&gt;Integration with cloud computing environments&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Such characteristics permit to effortlessly reproduce the workflow of most researchers and data scientists: the description of a research problem and methods in prose, plus the code to run the analysis and models to analyze datasets, and the presentation of results in tables and charts. You can even export your computational Notebook into PDF, Markdown, HTML, and others easy to share formats.&lt;/p&gt;

&lt;p&gt;Jupyter Notebooks evolved from a project called &lt;a href=&quot;https://ipython.org/&quot;&gt;IPython&lt;/a&gt;, which was created by the Colombian Berkeley Professor &lt;a href=&quot;https://bids.berkeley.edu/people/fernando-p%C3%A9rez&quot;&gt;Fernando Pérez&lt;/a&gt;, who at the time was a graduate student in Physics at CU Boulder. &lt;a href=&quot;https://www.youtube.com/watch?v=xuNj5paMuow&quot;&gt;Here&lt;/a&gt; you can watch a Fernando’s presentation about his journey creating Jupyter Notebooks.&lt;/p&gt;

&lt;p&gt;Although nowadays Jupyter is the most widely used IDE for scientific computing and data science worldwide, the idea of computational notebooks predates Project Jupyter. The first computational notebooks, today named &lt;a href=&quot;https://www.wolfram.com/notebooks/&quot;&gt;Wolfram Notebooks&lt;/a&gt;, were introduced by &lt;a href=&quot;https://www.stephenwolfram.com/&quot;&gt;Stephen Wolfram&lt;/a&gt; for the &lt;a href=&quot;https://www.wolfram.com/mathematica/&quot;&gt;Wolfram Mathematica&lt;/a&gt; programming language. The issue was that Wolfram Mathematica and Wolfram Notebooks are a closed-sources proprietary software, i.e., you have to pay for it.&lt;/p&gt;

&lt;p&gt;Fortunately, today we have access to Jupyter Notebooks for free, which is developed and maintained primarily by a large community of users from all over the world.&lt;/p&gt;

&lt;h2 id=&quot;jupyterlab&quot;&gt;JupyterLab&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/jupyterlab/jupyterlab&quot;&gt;JupyterLab&lt;/a&gt; is the next-generation interface for Jupyter Notebook. Essentially, they are an extension build on top the classic Jupyter Notebook, but with improved capabilities and features. I will use the JupyterLab interface for this mini-workshop, as it is the most up to date version of Project Jupyter, and is expected to fully replace the classic Jupyter Notebook in the short term. Project Jupyter developers advise the use of JupyterLab as they are investing they efforts on maintaining and developing this platform.&lt;/p&gt;

&lt;p&gt;Note that there some minor but important differences between the interface Jupyter Notebooks and JupyterLab, so you are advised to search for JupyterLab specific extensions and tutorials, since functionality may differ.&lt;/p&gt;

&lt;h2 id=&quot;jupyterlab-basics&quot;&gt;JupyterLab basics&lt;/h2&gt;

&lt;h3 id=&quot;installing-jupyterlab&quot;&gt;Installing JupyterLab&lt;/h3&gt;

&lt;p&gt;Before installing JupyterLab, I am assuming you have a recent version of Python 3 installed. Any Python version greater than 3.6 should work. Utilizing a virtual environment it is also recommended to isolate your JupyterLab installation.&lt;/p&gt;

&lt;p&gt;It is also good idea to update &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip&lt;/code&gt; before installing jupyterLab by running:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; pip &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--upgrade&lt;/span&gt; pip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;JupyterLab can be installed with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip&lt;/code&gt; in the terminal as:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# is recommended to run this in a virtual environment&lt;/span&gt;
pip3 &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;jupyterlab
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;or as:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python3 &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; pip &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;jupyterlab
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;A second option is with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# is recommended to run this in a conda virtual environment&lt;/span&gt;
conda &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; conda-forge jupyterlab
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I personally prefer to use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip&lt;/code&gt; as I find it’s simpler, cleaner, and works out-of-the-box with your Python installation.&lt;/p&gt;

&lt;p&gt;Also take into account that the above instructions will install the latest stable release of JupyterLab.&lt;/p&gt;

&lt;p&gt;To check you installation was succesful, run:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;jupyterlab &lt;span class=&quot;nt&quot;&gt;--version&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;launching-jupyterlab&quot;&gt;Launching JupyterLab&lt;/h3&gt;

&lt;p&gt;To launch JupyterLab open the terminal, navigate to your working directory, and run:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;jupyter lab
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;JupyterLab will launch a session in your default browser. If you want to launch JupyterLab in a different browser, you can either change your default browser or to copy-past the Notebook address in your desired browser.&lt;/p&gt;

&lt;h3 id=&quot;jupyterlab-interface&quot;&gt;JupyterLab interface&lt;/h3&gt;

&lt;p&gt;The JupyterLab interface consists of:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;a &lt;strong&gt;main work area&lt;/strong&gt; containing tabs for the notebooks, terminals, and text files&lt;/li&gt;
  &lt;li&gt;a &lt;strong&gt;collapsible left sidebar&lt;/strong&gt; containing a file browser, the running kernels and terminals, the command palette, the cell inspector, the list of open tabs, and any other extension you may have activated.&lt;/li&gt;
  &lt;li&gt;a top &lt;strong&gt;menu bar&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;creating-and-renaming-a-notebook&quot;&gt;Creating and renaming a Notebook&lt;/h3&gt;

&lt;p&gt;JupyterLab will open the Launcher tab as default, where you can select what kind of instance you want to run. By default, JupyterLab will allow for a Notebook with Python 3 kernel, a IPython console, a bash terminal, a Markdown file, and a text editor.&lt;/p&gt;

&lt;p&gt;To create a Notebook click under “Notebook” in the Launcher, and select a Python 3 kernel when prompted. Alternatively, you can create new Notebooks by clicking “File” in the top menu bar, then “New”, and then “Notebook.&lt;/p&gt;

&lt;p&gt;It is important to rename the Notebook as JupyterLab will give an “Untitled.ipynb” name as default to all new Notebooks.&lt;/p&gt;

&lt;h2 id=&quot;video-i---introduction-and-set-up&quot;&gt;Video I - Introduction and set-up&lt;/h2&gt;

&lt;div class=&quot;embed-container&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/U5Hg1Anxy7g&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h3 id=&quot;interacting-with-files&quot;&gt;Interacting with files&lt;/h3&gt;

&lt;p&gt;JupyterLab will load up directory and subdirectories from which you open it up in the terminal. As JupyterLab is very flexible, you will be able to open pretty much any file by double clicking on it: notebooks, images, text files, json files, csv files, and much more.&lt;/p&gt;

&lt;p&gt;One of the improved capabilities of JupyterLab is its ability to open large csv files in a excel-like nice looking interface.&lt;/p&gt;

&lt;p&gt;Another great JupyterLab feature is the possibility of rearranging your workspace with multiple windows at once, which is great for when you want to work with multiple files side by side.&lt;/p&gt;

&lt;p&gt;Additionally, files that can be open in more than one format, can be put side by side, and the changes you make in one view/format of the file will reflect in the other view/format. For instance, Markdown files can be open as rendered versions or as raw text. Notice that you have to save the changes before they are reflected in the other view/format.&lt;/p&gt;

&lt;p&gt;A common task in data manipulation is to open files like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;csv&lt;/code&gt; datasets and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;png&lt;/code&gt; images. To import such files into your Notebook, you will need to know the “path” to the file. File paths can be easily obtained by right-clicking in the desired file and selecting “Copy path”.&lt;/p&gt;

&lt;p&gt;Uploading files can be done by clicking the “Upload Files” arrow-icon in the left sidebar or by dragging and dropping the files onto the file navigator in the browser.&lt;/p&gt;

&lt;p&gt;Downloading files can be done by right-clicking in the desired file, and selecting the “Download” option.&lt;/p&gt;

&lt;h2 id=&quot;interacting-with-notebooks&quot;&gt;Interacting with Notebooks&lt;/h2&gt;

&lt;p&gt;As I mentioned earlier, Notebooks main advantage is its capacity to combine a narrative (text), with code to run analysis and models, and the analysis outcomes as tables and plots, all in one place. As a bonus, Notebooks allow to render LaTeX output, which is fabulous if you need to introduce equations.&lt;/p&gt;

&lt;p&gt;Notebooks are made out of collections of &lt;strong&gt;cells&lt;/strong&gt;. A cell is simply a rectangular box in which you can type and visualize stuff. There are three types of cells:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Markdown cells&lt;/strong&gt;: this are used to write your document, and to organize the content in titles, sub-titles, and so on&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Code cells&lt;/strong&gt;: this are used to run code&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Raw cells&lt;/strong&gt;: this are essentially plain text&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The easiest way to change between cell types, is by clicking on the drop-down tab at the top of the Notebook and selecting the one you want. You can also use the keyboard shortcuts which are &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Esc&lt;/code&gt; + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;M&lt;/code&gt; for Markdown, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Esc&lt;/code&gt; + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Y&lt;/code&gt; for Code, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Esc&lt;/code&gt; + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;R&lt;/code&gt; for Raw.&lt;/p&gt;

&lt;p&gt;To run a cell, you can either go to the top menu bar, select “Run”, and then click in “Run Selected Cell”. In practice, most people uses the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Ctrl&lt;/code&gt; + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Enter&lt;/code&gt; keyboard shortcut. You can also use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Shift&lt;/code&gt; + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Enter&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Alt&lt;/code&gt; + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Enter&lt;/code&gt; to run the current cell and insert a cell below.&lt;/p&gt;

&lt;p&gt;You can create new cells by clicking in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;+&lt;/code&gt; symbol in the top bar of the Notebook, or with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Esc&lt;/code&gt; + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;B&lt;/code&gt; keyboard shortcut.&lt;/p&gt;

&lt;p&gt;To delete cells, you can right-click on the cell and select “Delete Cell”, or you can use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Esc&lt;/code&gt; + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DD&lt;/code&gt; shortcut.&lt;/p&gt;

&lt;p&gt;To copy cells, you can right-click on the cell and select “Copy Cell”, or to use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Esc&lt;/code&gt; + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;C&lt;/code&gt; keyboard shortcut. To past a copied cell, you can right-click on the cell and select “Paste Cell Below” to insert the cell below th active cell, or to use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Esc&lt;/code&gt; + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt; keyboard shortcut.&lt;/p&gt;

&lt;p&gt;To cut cells, you can ither use the “scissors” icon at the top of the workspace bar, or to use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Esc&lt;/code&gt; + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X&lt;/code&gt; keyboard shortcut.&lt;/p&gt;

&lt;p&gt;When you insert new cells, you won’t be able to type content into the cell immediately. You need to enter in “command” or “enter mode”. To do this you can double-click on the cell, or to use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Enter&lt;/code&gt; keyboard shortcut.&lt;/p&gt;

&lt;p&gt;Accidentally deleting cells is something may happen often in Notebooks. To recover a deleted cell, you can right-click on the cell that became active after deletion, and select “Redo Cell Operation”, or to use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Shift&lt;/code&gt; + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Z&lt;/code&gt; keyboard shortcut.&lt;/p&gt;

&lt;p&gt;Remembering Python syntax, commands, and library methods, can be thought at the beginning. Notebooks offer a few useful tools to help with this. When typing  a Python command, you can click &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Tab&lt;/code&gt; and JupyterLab will generate a drop-down menu where you can search for commands and methods to complete your code. This is the so-called &lt;em&gt;Tab completion&lt;/em&gt;. Tab completion is usually very powerful in IDEs like VSCode and Pycharm, but a bit slow and incomplete for JupyterLab.&lt;/p&gt;

&lt;p&gt;A second Notebook tool to remember commands and methods is the “Contextual Help”. The contextual help is activated by clicking on “Help” in the top bar menu, and selecting “Show Contextual Help”, or wit the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Ctrl&lt;/code&gt; + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;I&lt;/code&gt; shortcut. The Contextual Help will open as a new tab, so you can rearrange your workspace to have the Contextual Help tab side by side your Notebook. Contextual Help works by searching the documentation for a function when you select it. Keep in mind it won’t work for all functions.&lt;/p&gt;

&lt;p&gt;To move cells around you can hover over the left-side of the cell, and click to drag and drop the cell in a new location. Although this is a nice feature, I strongly advise against it for reasons I will review later.&lt;/p&gt;

&lt;p&gt;To save changes made to your Notebook, you can click on “File” at the top bar menu and select “Save Notebook” or “Save All”, or to simply use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Ctrl&lt;/code&gt; + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;S&lt;/code&gt; keyboard shortcut. Fortunately, JupyterLab routinely save your progress in the background so it can be recovered if your browser closes for any reason.&lt;/p&gt;

&lt;h2 id=&quot;video-ii---intercting-with-files-and-notebooks&quot;&gt;Video II - Intercting with files and Notebooks&lt;/h2&gt;

&lt;div class=&quot;embed-container&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/pQsDd0N2kNQ&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h2 id=&quot;interacting-with-the-kernel&quot;&gt;Interacting with the Kernel&lt;/h2&gt;

&lt;p&gt;The first time I heard about a “Kernel” I was deeply confused (I’m still a bit confused to be honest). Put simply, a Kernel is an “instance” of the Python interpreter that runs in the background and process the code instructions you type in Notebook cells. Same for any other dynamic programming language like R, Scala, or Julia. This is why “Kernels” of multiple programming languagues can be added and run in parallel in JupyterLab.&lt;/p&gt;

&lt;p&gt;You can see what Kernels are being run in your session by clicking on the “Terminals and Kernels” icon in the left sidebar. Such menu also allows to shut down Kernels. Notice thay closing a Notebook or Console will not shut down the running Kernel. Shutting down a Kernel must be explicitly done in the Kernel session menu or the terminal where you open up JupyterLab.&lt;/p&gt;

&lt;p&gt;The “Kernel” tab in the top bar menu also allow to do Actions on Kernels, like shutting down the current Kernel or all running Kernels.&lt;/p&gt;

&lt;p&gt;Common Kernel operations are Interrupting and Restarting. Interrupting is sometimes necessary to stop computations have been running for too long and may be corrupted. Restarting is sometimes necessary to generate a clean environment to work with the same Notebook. Interrupting can be done in the “Kernel” menu or with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Esc&lt;/code&gt; + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;II&lt;/code&gt; keyboard shortcut. Restarting can also be done in the “Kernel” menu or with he &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Esc&lt;/code&gt; + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OO&lt;/code&gt; menu.&lt;/p&gt;

&lt;p&gt;Another important operation is to “Restart the Kernel and Run All Cells”. This is often done because you want to make sure the code in the Notebook run without problems from top-to-bottom, which can be a problem given how Notebooks work. This can only be done in the “Kernel” menu.&lt;/p&gt;

&lt;h2 id=&quot;jupyterlab-extensions&quot;&gt;JupyterLab Extensions&lt;/h2&gt;

&lt;p&gt;JupyterLab has the flexibility of incorporating extensions, this is, additional functionality which is not available “out-of-the-box”. Such extensions are primarily created by the community of users and developers in the Jupyter community, and are free to use.&lt;/p&gt;

&lt;p&gt;Under the hood, JupyterLab is essentially a bunch of JavaScipt code, which is the dominant programming languague in the web development sphere. This means that JupyterLab extensions are developed in JavaScript, which implies to will need Node.js in your machine to install JupyterLab extensions.&lt;/p&gt;

&lt;p&gt;There are two ways to install extensions: with the Extension Manager and in the terminal. The Extension Manager it is also an extension (very meta), which provides a graphic user interface within JupyterLab to install extensions. You won’t find it at first because it is disable by default. To enable it, go to the search bar in the command pallet, and search for “Extension Manager”. Once enabled, you will see a puzzle-shape icon in the left sidebar.&lt;/p&gt;

&lt;p&gt;You can search and install extensions by searching by name in the search bar in the Extension Manager. Now, before you jumping to install extensions, beware that extensions allow for the execution of unchecked code in your Kernel and browser instance. JupyterLab partially address this issue by adding a JupyterLab Icon on the extensions “verified” by Project Jupyter. I have to acknowledge that I have never paying attention to who created the extension or if its verified or not before installing it. You may be wiser than me and check that out before installing extensions.&lt;/p&gt;

&lt;p&gt;Installing extension in the Extension Manager is as simple as to click on the “Install” icon. Once the installation is done, JupyterLab will prompt you to reload your workspace to make the extension available for use. Disabling extensions can be done in the Extension Manager as well, by simply searching the extension in the search bar and clicking on “Disable”.&lt;/p&gt;

&lt;p&gt;You can also install extension in the command line. To do this, you have to run :&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;jupyter labextension &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;my-extension
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Where “my-extension” stands for the extension name in the &lt;a href=&quot;https://www.npmjs.com/&quot;&gt;npm package repository&lt;/a&gt;. For instance, to install the “@jupyterlab/toc” extension:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;jupyter labextension &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; @jupyterlab/toc
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can search for extensions &lt;a href=&quot;https://www.npmjs.com/&quot;&gt;here&lt;/a&gt; by searching for “jupyterlab” in the search bar.&lt;/p&gt;

&lt;p&gt;Note that if you are using a virtual environment to run JupyterLab, you must open the terminal, activate the virtual environment, and then install the extension. Once the installation process is done, you will have to reload JupyterLab by clicking in the reload icon in your browser.&lt;/p&gt;

&lt;h2 id=&quot;cool-and-useful-jupyterlab-extensions&quot;&gt;Cool and useful JupyterLab extensions&lt;/h2&gt;

&lt;p&gt;JupyterLab benefits from a constantly growing library of extension to enhance functionality and user experience. Here I’ll just mention five of my favorites that I found significantly improve my productivity and enjoyment.&lt;/p&gt;

&lt;h3 id=&quot;table-of-contents-extension-toc&quot;&gt;Table of contents extension (TOC)&lt;/h3&gt;

&lt;p&gt;The TOC extension allows to organize sections in a Table of Contents, which is auto-generated following markdown heading conventions. You can click and navigate the document with the TOC in the left sidebar.&lt;/p&gt;

&lt;p&gt;To install &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;toc&lt;/code&gt; extension the terminal run:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;jupyter labextension &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; @jupyterlab/toc
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Or search for it in the Extension Manager&lt;/p&gt;

&lt;h3 id=&quot;spell-checker-extension&quot;&gt;Spell checker extension&lt;/h3&gt;

&lt;p&gt;I happen to write a lot in JupyterLab, and one of my major complaints is the lack of an spell checker as traditional text editors. The jupyterlab_spellchecker extension partially address this by highlighting misspelled words. It won’t suggest the correct spelling, but at least it will let you know you misspelled something.&lt;/p&gt;

&lt;p&gt;Note that this extension will also highlight any unrecognized word according to the &lt;a href=&quot;https://github.com/cfinke/Typo.js&quot;&gt;Typo.js&lt;/a&gt; library, meaning that words like “JupyterLab” will be highlighted. It also can be a bit annoying at first to have misspelled words highlighted as it does it while you write, instead of after. Yet, as non-native speaker, I prefer to tolerate such annoyance to produce better writing.&lt;/p&gt;

&lt;p&gt;To install in the terminal &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jupyterlab_spellchecker&lt;/code&gt; run:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;jupyter labextension &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; @ijmbarr/jupyterlab_spellchecker
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Or search for it in the Extension Manager.&lt;/p&gt;

&lt;h3 id=&quot;collapsible-headings&quot;&gt;Collapsible headings&lt;/h3&gt;

&lt;p&gt;Sometimes Notebooks may get very long, which makes navigation annoying or slow. There are sections you may not even need to work anymore and that would be better to have out of your sight. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;collapsible_headings&lt;/code&gt; extension allows to collapse / uncollapse sections by clicking on the caret icon on the top left corner of the cell defining a section.&lt;/p&gt;

&lt;p&gt;To install the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;collapsible_headings&lt;/code&gt; with the terminal run:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;jupyter labextension &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; @aquirdturtle/collapsible_headings
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Or search for it in the Extension Manager.&lt;/p&gt;

&lt;h3 id=&quot;shortcut-manager&quot;&gt;Shortcut manager&lt;/h3&gt;

&lt;p&gt;As you may have notice by now, JupyterLab has a wide variety of keyword shortcuts to improve productivity. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;shortcutui&lt;/code&gt; allows to both explore the current available shortcuts in your system, and to define new ones, in a simple graphic interface.&lt;/p&gt;

&lt;p&gt;To access the keyboard shortcuts editor click on the “Settings” or “Help” tabs in the top menu bar, and select “Keyboard Shortcut Editor”.&lt;/p&gt;

&lt;p&gt;To install &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;shortcutui&lt;/code&gt; with the terminal run:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;jupyter labextension &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; @jupyterlab/shortcutui
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Or search for it in the Extension Manager.&lt;/p&gt;

&lt;h3 id=&quot;jupyterlab-neon-theme&quot;&gt;Jupyterlab Neon Theme&lt;/h3&gt;

&lt;p&gt;Some people like light-themes (white background) editors, some like dark-theme editors, and some, like me, like shinny neon purple editors. It just easier on my eyes and pleasant to look at. The Jupyter Neon Theme extension provides exactly that.&lt;/p&gt;

&lt;p&gt;To install the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jupyterlab_neon_theme&lt;/code&gt; extension in the terminal run:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;jupyter labextension &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; @yeebc/jupyterlab_neon_theme
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To actually use it, click on “Settings” in the top bar menu, then in “JupyterLab Theme”, and then in “JupyterLab Neon Theme” (or any theme you have available and want to try).&lt;/p&gt;

&lt;p&gt;Or search for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jupyterlab_neon_theme&lt;/code&gt; in the Extension Manager.&lt;/p&gt;

&lt;h2 id=&quot;video-iii---kernel-and-extensions&quot;&gt;Video III - Kernel and Extensions&lt;/h2&gt;

&lt;div class=&quot;embed-container&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/lYFusU11RbY&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h2 id=&quot;exporting-notebooks&quot;&gt;Exporting Notebooks&lt;/h2&gt;

&lt;p&gt;Notebooks have the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.ipynb&lt;/code&gt; extension, which stands for interactive python notebook. However, Notebooks can be exported in a wide variety of formats:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Asciidoc .asciidoc&lt;/li&gt;
  &lt;li&gt;HTML .html&lt;/li&gt;
  &lt;li&gt;Latex .tex&lt;/li&gt;
  &lt;li&gt;Markdown .md&lt;/li&gt;
  &lt;li&gt;PDF .pdf&lt;/li&gt;
  &lt;li&gt;ReStructured Text .rst&lt;/li&gt;
  &lt;li&gt;Executable Script .py&lt;/li&gt;
  &lt;li&gt;Reveal.js Slides .html&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To access to the export options click on “File” -&amp;gt; “Export Notebook As” and select the desired format.&lt;/p&gt;

&lt;h3 id=&quot;exporting-to-pdf-and-latex&quot;&gt;Exporting to PDF and LaTeX&lt;/h3&gt;

&lt;p&gt;To export Notebooks to PDF and LaTeX formats you will a need couple of packages first. In particular, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pandoc&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nbconvert&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TeX&lt;/code&gt;. The installation process for those packages will differ depending on your Operating System, i.e., whether you are running Linux, macOS, or Windows.&lt;/p&gt;

&lt;p&gt;Instructions to install &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pandoc&lt;/code&gt; can be found &lt;a href=&quot;https://pandoc.org/installing.html&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Instructions to install &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nbconvert&lt;/code&gt; can be found &lt;a href=&quot;https://nbconvert.readthedocs.io/en/latest/install.html&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Instructions to install &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TeX&lt;/code&gt; can be found &lt;a href=&quot;https://nbconvert.readthedocs.io/en/latest/install.html#installing-tex&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;recommended-formart-to-export-and-share-with-others&quot;&gt;Recommended formart to export and share with others&lt;/h3&gt;

&lt;p&gt;When I started using Jupyter and wanted to share a non-interactive version of my Notebook, my first instinct was to generate a PDF. Turns our, that since Notebooks are all JavaScript under the hood, it is pretty simple to generate a nice and consistent looking HTML file. Any person you send a HTML file will be able to open the file by just double clicking on it as long they have a browser, which every computer has.&lt;/p&gt;

&lt;p&gt;I highly recommend to export Notebooks to share in HTML format as they may save you a lot of pain trying to export Notebooks in PDF properly formatted.&lt;/p&gt;

&lt;h2 id=&quot;running-notebooks-in-vscode&quot;&gt;Running Notebooks in VSCode&lt;/h2&gt;

&lt;p&gt;VSCode is a free and open-source general purpose multi-platform (i.e., Linux, Windows, and macOS compatible) text IDE developed by Microsoft. Nowadays is one of the most popular IDEs and text-editors among programmers and data scientist.&lt;/p&gt;

&lt;p&gt;VSCode has recently incorporated the capacity to open &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.ipybn&lt;/code&gt; extension files, i.e., Notebooks, providing a Notebook-like interface. This is certainly a good alternative to try out if you are not fully comfortable with the web browser interface. I personally do not use it as I found out that rendering text, LaTeX formulas, and pictures is cleaner and easier in the browser interface.&lt;/p&gt;

&lt;p&gt;To install VSCode go to the &lt;a href=&quot;https://code.visualstudio.com/&quot;&gt;official website&lt;/a&gt; and follow the installation instructions for your system.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://code.visualstudio.com/docs/python/jupyter-support&quot;&gt;Here&lt;/a&gt; is the official VSCode guide to run Jupyter Notebooks within VSCode.&lt;/p&gt;

&lt;h2 id=&quot;video-iv---exporting-and-vscode&quot;&gt;Video IV - Exporting and VSCode&lt;/h2&gt;

&lt;div class=&quot;embed-container&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/-jzEB34a0RE&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h2 id=&quot;notebooks-weaknesses&quot;&gt;Notebooks weaknesses&lt;/h2&gt;

&lt;p&gt;Until this point, we have highlighted the many awesome capabilities of JupyterLab and Notebooks. However, Notebooks have a series of weaknesses to be mindful about and to consider, such that you can do two things:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Decide when is a good idea to use a Notebook&lt;/li&gt;
  &lt;li&gt;If you decide to use, to take the proper precautions to prevent problems&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Here is a brief list of issues that myself and others &lt;a href=&quot;https://www.youtube.com/watch?v=7jiPeIFXb6U&quot;&gt;on the Internet&lt;/a&gt; have mentioned about Notebooks:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Out of order execution&lt;/li&gt;
  &lt;li&gt;Source control is hard&lt;/li&gt;
  &lt;li&gt;Dependency management is hard&lt;/li&gt;
  &lt;li&gt;Modularization is hard&lt;/li&gt;
  &lt;li&gt;Testing is hard&lt;/li&gt;
  &lt;li&gt;Code reviews are hard&lt;/li&gt;
  &lt;li&gt;Code is hard to extend&lt;/li&gt;
  &lt;li&gt;Refactoring code is hard&lt;/li&gt;
  &lt;li&gt;Maintaining code is hard&lt;/li&gt;
  &lt;li&gt;Code collaboration is hard&lt;/li&gt;
  &lt;li&gt;Encourage poor programming practices&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I will not delve into all of this topics in consideration of time and space, and because many of this issues are actually problematic for large scientific projects rather than for small data analysis or modeling procedures. I will focus in just three issues: &lt;strong&gt;out of order execution&lt;/strong&gt;, &lt;strong&gt;modularization&lt;/strong&gt;, and &lt;strong&gt;refactoring&lt;/strong&gt; and &lt;strong&gt;debugging&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;out-of-order-execution&quot;&gt;Out of order execution&lt;/h3&gt;

&lt;p&gt;When you write code for an analysis, the code must run in the exact same sequence to yield the same results or to even run at all. Traditional code written in plain text-editors and then run in the terminal, will always do this by definition. Notebooks, on the other hand, make very easy to run cells out of order. It has happened to me dozens of times during the ~3 years I have been using Notebooks, and it is a common complaint on the Internet among data scientists.&lt;/p&gt;

&lt;p&gt;Most of the time, out of order execution will not matter that much. You will realize early and fix it. But once in a while, you may introduce a “bug” or mistake in your code than can cost you hours of detective work. That is the best case scenario. Worst case scenario, you never realize your mistake and your results will be wrong without you knowing it.&lt;/p&gt;

&lt;p&gt;For instance, imagine you have to run these three steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Get rid of NA in your data&lt;/li&gt;
  &lt;li&gt;Computer the weighted average of two variables&lt;/li&gt;
  &lt;li&gt;Create a new variable based on those two variables&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Several things may go wrong here. First, you may forgot to run the cell cleaning the NA values. Second, you may compute the average first, and run the cell to eliminate the NA values second.&lt;/p&gt;

&lt;p&gt;Although making such mistakes may sound unlikely, at least in mat experience, is not. Here is a typical scenario: you write the code for cleaning the NA and computing the average on different cells, so everything is nicely organized. Then you go for a coffee and get distracted. Then you come back to your computer, your cursor is on the cell to compute the average, so run that, and keep working. Afterwards, you may spend a good couple of hours writing and running more code, and writing analysis from your results, just to realize several hours later that you forgot to run the cell to clean the NA values. Too bad.&lt;/p&gt;

&lt;p&gt;Here is another one: you suddenly realize you made a mistake in the cell to clean the NA values. You go back, change the cell, and rerun the cell. But ops! you forgot to rerun the previous cell to reload the data into your Notebook, so now your cleaning process is mess up. You go back and rerun that cell. Then you get distracted with your dog barking. Then you go an run the average cell. You get distracted again with an email. Then you go back and remember to run the cell to clean the NA values. There you go: you ran the cells out of order. Again, this simply can’t happen with a script, since the script will rerun everything in order from top to bottom each time.&lt;/p&gt;

&lt;p&gt;Even though you may think that you focus is too good to make such mistakes, it is often the case that people get chronically distracted with working on computers with Internet access. &lt;strong&gt;So, be careful&lt;/strong&gt;: if you really need to run long sequences of code, you may consider to switch to a text-file instead of a Notebook.&lt;/p&gt;

&lt;h3 id=&quot;modularization-is-hard&quot;&gt;Modularization is hard&lt;/h3&gt;

&lt;p&gt;When your code start to grow, a common recommended best practice is to divide your code into &lt;strong&gt;modules&lt;/strong&gt;. This is simply having separate text-files with code to do &lt;strong&gt;one task&lt;/strong&gt;. For instance, a large project may have scripts to: (1) load the data, (2) pre-process the data, (3) run descriptive statistics and plots, (4) run statistical models and generate the output.&lt;/p&gt;

&lt;p&gt;Doing all of this with Notebooks is not impossible, but hard. It is just way too easy to get trapped in the flow of writing everything in a single Notebook.&lt;/p&gt;

&lt;h3 id=&quot;refactoring-and-debugging-is-hard&quot;&gt;Refactoring and debugging is hard&lt;/h3&gt;

&lt;p&gt;Refactoring refers to the practice to rewrite and reorganize your code for readability and performance. Debugging refers to the exercise of searching for mistakes in your code.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;When code modularization is hard, refactoring and debugging are hard&lt;/strong&gt;. When code is modularized into single-purpose isolated scripts, it is relatively easy to focus in ways to rewrite such code to be clearer and faster to run. This is hard with Notebooks. Debugging code is also relatively easy when code is modularized, and refactored. If everything is dumped into a Notebook with 100 cells, finding where you made an error in your code can be a nightmare.&lt;/p&gt;

&lt;h2 id=&quot;video-v---notebooks-weaknesses&quot;&gt;Video V - Notebooks weaknesses&lt;/h2&gt;

&lt;div class=&quot;embed-container&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/lMlN-2W1rxE&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h2 id=&quot;when-to-use-notebooks&quot;&gt;When to use Notebooks&lt;/h2&gt;

&lt;p&gt;The purpose of make Notebooks weaknesses visible, is to encourage you to think carefully about when to use Notebooks, and to take precautions.&lt;/p&gt;

&lt;p&gt;In my view, Notebooks are a good option, sometimes the best option, in the following scenarios:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Relatively short data analysis or modeling task&lt;/li&gt;
  &lt;li&gt;Task that rely more in the narrative than in the code itself, which is short&lt;/li&gt;
  &lt;li&gt;Task that are primarily about data visualization, as Notebooks can render plots and images easily&lt;/li&gt;
  &lt;li&gt;Tutorials and educational content&lt;/li&gt;
  &lt;li&gt;Analysis that entail lots of equations&lt;/li&gt;
  &lt;li&gt;Task that require relatively “small” or “medium” size datasets&lt;/li&gt;
  &lt;li&gt;Task that are heavily dependent on constant interaction with the results of running your code, i.e., highly interactive development&lt;/li&gt;
  &lt;li&gt;Prototyping code that will be eventually transfered to a script&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;People differ in their appreciation of Notebooks. Only you and your experience using them can inform what is best for you.&lt;/p&gt;

&lt;h2 id=&quot;basic-good-practices-to-work-with-notebooks&quot;&gt;Basic good practices to work with Notebooks&lt;/h2&gt;

&lt;p&gt;Now that we covered Notebook weaknesses, it is a good idea to review a few “good” practices when working with Notebooks such that you can prevent as many mistakes as possible.&lt;/p&gt;

&lt;p&gt;My recommendations are mix of my own experience, things I have learned from other Notebook users, and &lt;a href=&quot;https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007007#sec004&quot;&gt;this article&lt;/a&gt; about 10 rules to work with computational Notebooks.&lt;/p&gt;

&lt;p&gt;Here is the list:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Cells should do one task at the time&lt;/li&gt;
  &lt;li&gt;Document your coding process&lt;/li&gt;
  &lt;li&gt;Restart the Kernel and rerun all the cells often&lt;/li&gt;
  &lt;li&gt;Save and use version control often&lt;/li&gt;
  &lt;li&gt;Modularize your code as much as possible&lt;/li&gt;
  &lt;li&gt;Document your dependencies&lt;/li&gt;
  &lt;li&gt;Switch to a script if your Notebook grows too long&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;cells-should-do-one-task-at-the-time&quot;&gt;Cells should do one task at the time&lt;/h3&gt;

&lt;p&gt;It is tempting to write code cells that run multiple lines of code and tasks all at once. This will probably make hard to read, document, and debug code cells if something goes wrong. Ideally, you want cells to do one task at the time. One task may involve multiple lines of code, but it is still one task.&lt;/p&gt;

&lt;p&gt;For instance, computing the mean between two list of integers is a perfectly fine task to have in one cell as:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;statistics&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;list_a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;list_b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;list_c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;list_a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;list_b&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;list_c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;mean of list-a + list-b = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mean of list-a + list-b = 3.5
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, it is not always clear what “a task” entails. What if you define “the task” as computing descriptive statistics as the mean, median, and mode? Should all three go into a single cell or three separate cells? Since computing those is so simple, I would put everything in once cell, and move the print statements onto a separate cell as:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;statistics&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;median&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;list_a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;list_b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;list_c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;list_a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;list_b&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;list_c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;median&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;median&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;list_c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;max_v&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;list_c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;mean of list-a + list-b = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;median of list-a + list-b = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;median&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;max value of list-a + list-b = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_v&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mean of list-a + list-b = 3.5
median of list-a + list-b = 3.5
max value of list-a + list-b = 6
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If instead of simple calculations, the task were to run several statistical models like Principal Component analysis and Logistic Regression, I would definitely separate everything.&lt;/p&gt;

&lt;h3 id=&quot;document-your-coding-process&quot;&gt;Document your coding process&lt;/h3&gt;

&lt;p&gt;Documenting code is a whole art in itself. Yet, there a few things you can implement right away to make code more reliable. Take the code from out previous example. There are two things we can do to improve upon it: (1) to use markdown cells to describe what are doing, (2) to add comments in along with the code.&lt;/p&gt;

&lt;p&gt;Now, for such a short and simple task is often recommended to not add in-line commentaries as the code is self-descriptive. However, I’ll add comments just for the sake of example.&lt;/p&gt;

&lt;p&gt;Something like:&lt;/p&gt;

&lt;h4 id=&quot;descriptive-statistics&quot;&gt;Descriptive statistics&lt;/h4&gt;

&lt;p&gt;Here we compute the mean, median, and max value for our lists.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;list_a, list_b = [1, 2, 3],[4, 5, 6] # assign list to variables for further computation
list_c = list_a + list_b # join list to compute descriptive stats 
mean = mean(list_c) 
median = median(list_c)
max_v = max(list_c)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In general, you want to comment on the “why” instead of only the “what” of a line of code.&lt;/p&gt;

&lt;h3 id=&quot;restart-the-kernel-and-rerun-all-the-cells-often&quot;&gt;Restart the Kernel and rerun all the cells often&lt;/h3&gt;

&lt;p&gt;Just restarting your Kernel and rerun cells often it is a tremendously effective strategy to avoid the out-of-order execution problem. Since i started to do this the mistakes in my code where reduced dramatically. Quick and simple.&lt;/p&gt;

&lt;h3 id=&quot;save-and-use-version-control-often&quot;&gt;Save and use version control often&lt;/h3&gt;

&lt;p&gt;Although JupyterLab will constantly save your progress, clicking &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Ctrl&lt;/code&gt; + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;S&lt;/code&gt; often and saving your progress in a remote version control system as Git/GitHub is a life saver for when things go wrong with Notebooks. Version control system is a topic in itself, but in short, it is a way to save copies of your work at different points in time in the cloud. Such “images” or “clones” of your work can be accessed and recovered easily in case you made some serious mistake in your code.&lt;/p&gt;

&lt;h3 id=&quot;modularize-your-code-as-much-as-possible&quot;&gt;Modularize your code as much as possible&lt;/h3&gt;

&lt;p&gt;True, I previously argued that modularization was hard in Notebooks. But hard does not mean equal impossible. As you become a more proficient coder, you will fine ways to refactor and modularize your code in manner that will make work with Notebooks more reliable and cleaner.&lt;/p&gt;

&lt;h3 id=&quot;document-your-dependencies&quot;&gt;Document your dependencies&lt;/h3&gt;

&lt;p&gt;Dependencies are all the software and libraries you used in your project. For instance, your software stack for a project may look as:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;python==3.6.8&lt;/li&gt;
  &lt;li&gt;jupyterlab==2.0.1&lt;/li&gt;
  &lt;li&gt;numpy==1.18.2&lt;/li&gt;
  &lt;li&gt;pandas==1.0.3&lt;/li&gt;
  &lt;li&gt;pip==20.0.2&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Documenting that at the top of your Notebook is a good way to make clear to your future self and others potential users of your code, what are the requirements to reproduce your analysis.&lt;/p&gt;

&lt;p&gt;People have created utilities for that like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;whatermark&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load_ext&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;watermark&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;watermark&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;2020-06-11T15:58:30-05:00

CPython 3.6.8
IPython 7.15.0

compiler   : GCC 7.5.0
system     : Linux
release    : 4.19.104-microsoft-standard
machine    : x86_64
processor  : x86_64
CPU cores  : 16
interpreter: 64bit
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;switch-to-a-script-if-your-notebook-grows-too-long&quot;&gt;Switch to a script if your Notebook grows too long&lt;/h3&gt;

&lt;p&gt;No matter how many precautions you take, you will reach a point where using Notebooks it is not the most functional alternative. You can insist on using a Notebook for large projects with many moving pieces, but you will probably start to experience the problems mentioned above. In such cases, a IDE like VSCode, Pycharm or Atom are probable your best option.&lt;/p&gt;

&lt;h2 id=&quot;video-vi---when-to-use-notebooks-and-best-practices&quot;&gt;Video VI - When to use Notebooks and Best Practices&lt;/h2&gt;

&lt;div class=&quot;embed-container&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/_M2rbm_zh50&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h2 id=&quot;final-jupyterlab-tips&quot;&gt;Final JupyterLab Tips&lt;/h2&gt;

&lt;p&gt;Here a few additional final tips to use Notebooks effectively:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Command Palette&lt;/strong&gt;: if you do &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Ctrl&lt;/code&gt; + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Shift&lt;/code&gt; + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;C&lt;/code&gt; you will have access to the Command Palette, which is a centralized command system to search and use all JupyterLab utilities.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Create console for Notebooks&lt;/strong&gt;: if you right-click onto a cell a select “New Console for Editor”, JupyterLab will open a Python interactive terminal at the bottom of the screen. Such instance shares the variables and information you have run already in your Notebook. It is a great way to try out code without having to change the Notebook, and to get richer output when running code.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Use the Help menu&lt;/strong&gt;: the “Help” tab at the top menu bar has the official documentation for JupyterLab which comes in handy when you are trying to learn something new or fix something does not work. Googling of course will always help, but sometimes answers to your questions will be outdated and plainly wrong.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;resources-to-learn-more&quot;&gt;Resources to learn more&lt;/h2&gt;

&lt;p&gt;Here you can find a list of resources to learn more about JupyterLab and become and effective user:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007007#sec004&quot;&gt;Ten simple rules for writing and sharing computational analyses in Jupyter Notebooks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://jupyterlab.readthedocs.io/en/stable/index.html&quot;&gt;Jupyterab official documentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=ctOM-Gza04Y&quot;&gt;JupyterLab: The Next Generation Jupyter Web Interface&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=NSiPeoDpwuI&quot;&gt;JupyterLab: The Evolution of the Jupyter Notebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 12 Jun 2020 00:00:00 +0800</pubDate>
        <link>//intro-jupyter-ide</link>
        <link href="/intro-jupyter-ide"/>
        <guid isPermaLink="true">/intro-jupyter-ide</guid>
      </item>
    
      <item>
        <title>Introduction to Linear Algebra for Applied Machine Learning with Python</title>
        <description>&lt;p&gt;Linear algebra is to machine learning as flour to bakery: &lt;strong&gt;every machine learning model is based in linear algebra, as every cake is based in flour&lt;/strong&gt;. It is not the only ingredient, of course. Machine learning models need vector calculus, probability, and optimization, as cakes need sugar, eggs, and butter. Applied machine learning, like bakery, is essentially about combining these mathematical ingredients in clever ways to create useful (tasty?) models.&lt;/p&gt;

&lt;p&gt;This document contains &lt;strong&gt;introductory level linear algebra notes for applied machine learning&lt;/strong&gt;. It is meant as a reference rather than a comprehensive review. If you ever get confused by matrix multiplication, don’t remember what was the $L_2$ norm, or the conditions for linear independence, this can serve as a quick reference. It also a good introduction for people that don’t need a deep understanding of linear algebra, but still want to learn about the fundamentals to read about machine learning or to use pre-packaged machine learning solutions. Further, it is a good source for people that learned linear algebra a while ago and need a refresher.&lt;/p&gt;

&lt;p&gt;These notes are based in a series of (mostly) freely available textbooks, video lectures, and classes I’ve read, watched and taken in the past. If you want to obtain a deeper understanding or to find exercises for each topic, you may want to consult those sources directly.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Free resources&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Mathematics for Machine Learning&lt;/strong&gt; by Deisenroth, Faisal, and Ong. 1st Ed. &lt;a href=&quot;https://mml-book.github.io/&quot;&gt;Book link&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Introduction to Applied Linear Algebra&lt;/strong&gt; by Boyd and Vandenberghe. 1sr Ed. &lt;a href=&quot;http://vmls-book.stanford.edu/&quot;&gt;Book link&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Linear Algebra Ch. in Deep Learning&lt;/strong&gt; by Goodfellow, Bengio, and Courville. 1st Ed. &lt;a href=&quot;https://www.deeplearningbook.org/contents/linear_algebra.html&quot;&gt;Chapter link&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Linear Algebra Ch. in Dive into Deep Learning&lt;/strong&gt; by Zhang, Lipton, Li, And Smola. &lt;a href=&quot;https://d2l.ai/chapter_preliminaries/linear-algebra.html&quot;&gt;Chapter link&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Prof. Pavel Grinfeld’s Linear Algebra Lectures&lt;/strong&gt; at Lemma. &lt;a href=&quot;https://www.lem.ma/books/AIApowDnjlDDQrp-uOZVow/landing&quot;&gt;Videos link&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Prof. Gilbert Strang’s Linear Algebra Lectures&lt;/strong&gt; at MIT. &lt;a href=&quot;https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/&quot;&gt;Videos link&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Salman Khan’s Linear Algebra Lectures&lt;/strong&gt; at Khan Academy. &lt;a href=&quot;https://www.khanacademy.org/math/linear-algebra&quot;&gt;Videos link&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;3blue1brown’s Linear Algebra Series&lt;/strong&gt; at YouTube. &lt;a href=&quot;https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&quot;&gt;Videos link&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Not-free resources&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Introduction to Linear Algebra&lt;/strong&gt; by Gilbert Strang. 5th Ed. &lt;a href=&quot;https://www.amazon.com/Introduction-Linear-Algebra-Gilbert-Strang/dp/0980232775&quot;&gt;Book link&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;No Bullshit Guide to Linear Algebra&lt;/strong&gt; by Ivan Savov. 2nd Ed. &lt;a href=&quot;https://www.amazon.com/No-bullshit-guide-linear-algebra/dp/0992001021&quot;&gt;Book Link&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I’ve consulted all these resources at one point or another. Pavel Grinfeld’s lectures are my absolute favorites. Salman Khan’s lectures are really good for absolute beginners (they are long though). The famous 3blue1brown series in linear algebra is delightful to watch and to get a solid high-level view of linear algebra.&lt;/p&gt;

&lt;p&gt;If you have to pic one book, I’d pic &lt;strong&gt;Boyd’s and Vandenberghe’s Intro to applied linear algebra&lt;/strong&gt;, as it is the most beginner friendly book on linear algebra I’ve encounter. Every aspect of the notation is clearly explained and pretty much all the key content for applied machine learning is covered. The Linear Algebra Chapter in Goodfellow et al is a nice and concise introduction, but it may require some previous exposure to linear algebra concepts. Deisenroth et all book is probably the best and most comprehensive source for linear algebra for machine learning I’ve found, although it assumes that you are good at reading math (and at math more generally). Savov’s book it’s also great for beginners but requires time to digest. Professor Strang lectures are great too but I won’t recommend it for absolute beginners.&lt;/p&gt;

&lt;p&gt;I’ll do my best to keep notation consistent. Nevertheless, learning to adjust to changing or inconsistent notation is a useful skill, since most authors will use their own preferred notation, and everyone seems to think that its/his/her own notation is better.&lt;/p&gt;

&lt;p&gt;To make everything more dynamic and practical, I’ll introduce bits of Python code to exemplify each mathematical operation (when possible) with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;, which is the facto standard package for scientific computing in Python.&lt;/p&gt;

&lt;p&gt;Finally, keep in mind this is created by a non-mathematician for (mostly) non-mathematicians. I wrote this as if I were talking to myself or a dear friend, which explains why my writing is sometimes conversational and informal.&lt;/p&gt;

&lt;p&gt;If you find any mistake in notes feel free to reach me out at pcaceres@wisc.edu and to https://pablocaceres.org/ so I can correct the issue.&lt;/p&gt;

&lt;h1 id=&quot;table-of-contents&quot;&gt;Table of contents&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; &lt;em&gt;underlined sections&lt;/em&gt; are the newest sections and/or corrected ones.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;#preliminary-concepts&quot;&gt;Preliminary concepts&lt;/a&gt;&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#sets&quot;&gt;Sets&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#belonging-and-inclusion&quot;&gt;Belonging and inclusion&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#set-specification&quot;&gt;Set specification&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#ordered-pairs&quot;&gt;Ordered pairs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#relations&quot;&gt;Relations&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#functions&quot;&gt;Functions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;#vectors&quot;&gt;Vectors&lt;/a&gt;&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#types-of-vectors&quot;&gt;Types of vectors&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#geometric-vectors&quot;&gt;Geometric vectors&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#polynomials&quot;&gt;Polynomials&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#elements-of-r&quot;&gt;Elements of R&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#zero-vector-unit-vector-and-sparse-vector&quot;&gt;Zero vector, unit vector, and sparse vector&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#vector-dimensions-and-coordinate-system&quot;&gt;Vector dimensions and coordinate system&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#basic-vector-operations&quot;&gt;Basic vector operations&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#vector-vector-addition&quot;&gt;Vector-vector addition&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#vector-scalar-multiplication&quot;&gt;Vector-scalar multiplication&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#linear-combinations-of-vectors&quot;&gt;Linear combinations of vectors&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#vector-vector-multiplication-dot-product&quot;&gt;Vector-vector multiplication: dot product&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#vector-space-span-and-subspace&quot;&gt;Vector space, span, and subspace&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#vector-space&quot;&gt;Vector space&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#vector-span&quot;&gt;Vector span&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#vector-subspaces&quot;&gt;Vector subspaces&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#linear-dependence-and-independence&quot;&gt;Linear dependence and independence&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#vector-null-space&quot;&gt;Vector null space&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#vector-norms&quot;&gt;Vector norms&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#euclidean-norm&quot;&gt;Euclidean norm: $L_2$&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#manhattan-norm&quot;&gt;Manhattan norm: $L_1$&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#max-norm&quot;&gt;Max norm: $L_\infty$&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#vector-inner-product-length-and-distance&quot;&gt;Vector inner product, length, and distance&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#vector-angles-and-orthogonality&quot;&gt;Vector angles and orthogonality&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#systems-of-linear-equations&quot;&gt;Systems of linear equations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;#matrices&quot;&gt;Matrices&lt;/a&gt;&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#basic-matrix-operations&quot;&gt;Basic matrix operations&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#matrix-matrix-addition&quot;&gt;Matrix-matrix addition&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#matrix-scalar-multiplication&quot;&gt;Matrix-scalar multiplication&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#matrix-vector-multiplication-dot-product&quot;&gt;Matrix-vector multiplication: dot product&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#matrix-matrix-multiplication&quot;&gt;Matrix-matrix multiplication&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#matrix-identity&quot;&gt;Matrix identity&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#matrix-inverse&quot;&gt;Matrix inverse&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#matrix-transpose&quot;&gt;Matrix transpose&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#hadamard-product&quot;&gt;Hadamard product&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#special-matrices&quot;&gt;Special matrices&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#rectangular-matrix&quot;&gt;Rectangular matrix&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#square-matrix&quot;&gt;Square matrix&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#diagonal-matrix&quot;&gt;Diagonal matrix&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#upper-triangular-matrix&quot;&gt;Upper triangular matrix&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#lower-triangular-matrix&quot;&gt;Lower triangular matrix&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#symmetric-matrix&quot;&gt;Symmetric matrix&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#identity-matrix&quot;&gt;Identity matrix&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#scalar-matrix&quot;&gt;Scalar matrix&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#null-or-zero-matrix&quot;&gt;Null or zero matrix&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#echelon-matrix&quot;&gt;Echelon matrix&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#antidiagonal-matrix&quot;&gt;Antidiagonal matrix&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#design-matrix&quot;&gt;Design matrix&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#matrices-as-systems-of-linear-equations&quot;&gt;Matrices as systems of linear equations&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#the-four-fundamental-matrix-subsapces&quot;&gt;The four fundamental matrix subsapces&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#the-column-space&quot;&gt;The column space&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#the-row-space&quot;&gt;The row space&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#the-null-space&quot;&gt;The null space&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#the-null-space-of-the-transpose&quot;&gt;The null space of the transpose&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#solving-systems-of-linear-equations-with-matrices&quot;&gt;Solving systems of linear equations with matrices&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#gaussian-elimination&quot;&gt;Gaussian Elimination&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#gauss-jordan-elimination&quot;&gt;Gauss-Jordan Elimination&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#matrix-basis-and-rank&quot;&gt;Matrix basis and rank&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#matrix-norm&quot;&gt;Matrix norm&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;#linear-and-affine-mappings&quot;&gt;Linear and affine mappings&lt;/a&gt;&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#linear-mappings&quot;&gt;Linear mappings&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#examples-of-linear-mappings&quot;&gt;Examples of linear mappings&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#negation-matrix&quot;&gt;Negation matrix&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#reversal-matrix&quot;&gt;Reversal matrix&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#examples-of-nonlinear-mappings&quot;&gt;Examples of nonlinear mappings&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#norms&quot;&gt;Norms&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#translation&quot;&gt;Translation&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#affine-mappings&quot;&gt;Affine mappings&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#affine-combination-of-vectors&quot;&gt;Affine combination of vectors&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#affine-span&quot;&gt;Affine span&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#affine-space-and-subspace&quot;&gt;Affine space and subspace&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#affine-mappings-using-the-augmented-matrix&quot;&gt;Affine mappings using the augmented matrix&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#special-linear-mappings&quot;&gt;Special linear mappings&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#scaling&quot;&gt;Scaling&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#reflection&quot;&gt;Reflection&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#shear&quot;&gt;Shear&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#rotation&quot;&gt;Rotation&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#projections&quot;&gt;Projections&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#projections-onto-lines&quot;&gt;Projections onto lines&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#projections-onto-general-subspaces&quot;&gt;Projections onto general subspaces&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#projections-as-approximate-solutions-to-systems-of-linear-equations&quot;&gt;Projections as approximate solutions to systems of linear equations&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;#matrix-decompositions&quot;&gt;Matrix decompositions&lt;/a&gt;&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#lu-decomposition&quot;&gt;LU decomposition&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#elementary-matrices&quot;&gt;Elementary matrices&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#the-inverse-of-elementary-matrices&quot;&gt;The inverse of elementary matrices&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#lu-decomposition-as-gaussian-elimination&quot;&gt;LU decomposition as Gaussian Elimination&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#lu-decomposition-with-pivoting&quot;&gt;LU decomposition with pivoting&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#qr-decomposition&quot;&gt;QR decomposition&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#orthonormal-basis&quot;&gt;Orthonormal basis&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#orthonormal-basis-transpose&quot;&gt;Orthonormal basis transpose&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#gram-schmidt-orthogonalization&quot;&gt;Gram-Schmidt Orthogonalization &lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#qr-decomposition-as-gram-schmidt-orthogonalization&quot;&gt;QR decomposition as Gram-Schmidt Orthogonalization&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#determinant&quot;&gt;Determinant&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#determinant-as-measures-of-volume&quot;&gt;Determinant as measures of volume&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#the-2-x-2-determinant&quot;&gt;The 2X2 determinant&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#the-n-x-n-determinant&quot;&gt;The NXN determinant&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#determinants-as-scaling-factors&quot;&gt;Determinants as scaling factors&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#the-importance-of-determinants&quot;&gt;The importance of determinants&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#eigenthings&quot;&gt;Eigenthings&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#change-of-basis&quot;&gt;Change of basis&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#eigenvectors-eigenvalues-and-eigenspaces&quot;&gt;Eigenvectors, Eigenvalues, and Eigenspaces&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#trace-and-determinant-with-eigenvalues&quot;&gt;Trace and determinant with eigenvalues&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#eigendecomposition&quot;&gt;Eigendecomposition&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#eigenbasis-are-a-good-basis&quot;&gt;Eigenbasis are a good basis&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#geometric-interpretation-of-eigendecomposition&quot;&gt;Geometric interpretation of Eigendecomposition&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#the-problem-with-eigendecomposition&quot;&gt;The problem with Eigendecomposition&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#singular-value-decomposition&quot;&gt;Singular Value Decomposition&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#singular-value-decomposition-theorem&quot;&gt;Singular Value Decomposition Theorem&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#singular-value-decomposition-computation&quot;&gt;Singular Value Decomposition computation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#geometric-interpretation-of-the-singular-value-decomposition&quot;&gt;Geometric interpretation of the Singular Value Decomposition&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#singular-value-decomposition-vs-eigendecomposition&quot;&gt;Singular Value Decomposition vs Eigendecomposition&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#matrix-approximation&quot;&gt;Matrix Approximation&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#best-rank-k-approximation-with-svd&quot;&gt;Best rank-k approximation with SVD&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#best-low-rank-approximation-as-a-minimization-problem&quot;&gt;Best low-rank approximation as a minimization problem&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;#epilogue&quot;&gt;Epilogue&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&quot;preliminary-concepts&quot;&gt;Preliminary concepts&lt;/h1&gt;

&lt;p&gt;While writing about linear mappings, I realized the importance of having a basic understanding of a few concepts before approaching the study of linear algebra. If you are like me, you may not have formal mathematical training beyond high school. If so, I encourage you to read this section and spent some time wrapping your head around these concepts before going over the linear algebra content (otherwise, you might prefer to skip this part). I believe that reviewing these concepts is of great help to understand the &lt;em&gt;notation&lt;/em&gt;, which in my experience is one of the main barriers to understand mathematics for nonmathematicians: we are &lt;em&gt;non&lt;/em&gt;native speakers, so we are continuously building up our vocabulary. I’ll keep this section very short, as is not the focus of this mini-course.&lt;/p&gt;

&lt;p&gt;For this section, my notes are based on readings of:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Geometric transformations (Vol. 1)&lt;/strong&gt; (1966) by Modenov &amp;amp; Parkhomenko&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Naive Set Theory&lt;/strong&gt; (1960) by P.R. Halmos&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Abstract Algebra: Theory and Applications&lt;/strong&gt; (2016) by Judson &amp;amp; Beeer. &lt;a href=&quot;http://abstract.pugetsound.edu/download/aata-20160809.pdf&quot;&gt;Book link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;sets&quot;&gt;Sets&lt;/h2&gt;

&lt;p&gt;Sets are one of the most fundamental concepts in mathematics. They are so fundamental that they are not defined in terms of anything else. On the contrary, other branches of mathematics are defined in terms of sets, including linear algebra. Put simply, &lt;strong&gt;sets are well-defined collections of objects&lt;/strong&gt;. Such objects are called &lt;strong&gt;elements or members&lt;/strong&gt; of the set. The crew of a ship, a caravan of camels, and the LA Lakers roster, are all examples of sets. The captain of the ship, the first camel in the caravan, and LeBron James are all examples of “members” or “elements” of their corresponding sets. We denote a set with an upper case italic letter as $\textit{A}$. In the context of linear algebra, we say that a line is a set of points, and the set of all lines in the plane is a set of sets. Similarly, we can say that &lt;em&gt;vectors&lt;/em&gt; are sets of points, and &lt;em&gt;matrices&lt;/em&gt; sets of vectors.&lt;/p&gt;

&lt;h2 id=&quot;belonging-and-inclusion&quot;&gt;Belonging and inclusion&lt;/h2&gt;

&lt;p&gt;We build sets using the notion of &lt;strong&gt;belonging&lt;/strong&gt;. We denote that $a$ &lt;em&gt;belongs&lt;/em&gt; (or is an &lt;em&gt;element&lt;/em&gt; or &lt;em&gt;member&lt;/em&gt; of) to $\textit{A}$ with the Greek letter epsilon as:&lt;/p&gt;

\[a \in \textit{A}\]

&lt;p&gt;Another important idea is &lt;strong&gt;inclusion&lt;/strong&gt;, which allow us to build &lt;em&gt;subsets&lt;/em&gt;. Consider sets $\textit{A}$ and $\textit{B}$. When every element of $\textit{A}$ is an element of $\textit{B}$, we say that $\textit{A}$ is a &lt;em&gt;subset&lt;/em&gt; of $\textit{B}$, or that $\textit{B}$ &lt;em&gt;includes&lt;/em&gt; $\textit{A}$. The notation is:&lt;/p&gt;

\[\textit{A} \subset \textit{B}\]

&lt;p&gt;or&lt;/p&gt;

\[\textit{B} \supset \textit{A}\]

&lt;p&gt;Belonging and inclusion are derived from &lt;strong&gt;axion of extension&lt;/strong&gt;: &lt;em&gt;two sets are equal if and only if they have the same elements&lt;/em&gt;. This axiom may sound trivially obvious but is necessary to make belonging and inclusion rigorous.&lt;/p&gt;

&lt;h2 id=&quot;set-specification&quot;&gt;Set specification&lt;/h2&gt;

&lt;p&gt;In general, anything we assert about the elements of a set results in &lt;strong&gt;generating a subset&lt;/strong&gt;. In other words, asserting things about sets is a way to manufacture subsets. Take as an example the set of all dogs, that I’ll denote as $\textit{D}$. I can assert now “$d$ is black”. Such an assertion is true for some members of the set of all dogs and false for others. Hence, such a sentence, evaluated for &lt;em&gt;all&lt;/em&gt; member of $\textit{D}$, generates a subset: &lt;em&gt;the set of all black dogs&lt;/em&gt;. This is denoted as:&lt;/p&gt;

\[\textit{B} = \{ d \in \textit{D} : \text{d is black} \}\]

&lt;p&gt;or&lt;/p&gt;

\[\textit{B} = \{ d \in \textit{D} \vert \text{ d is black} \}\]

&lt;p&gt;The colon ($:$) or vertical bar ($\vert$) read as “such that”. Therefore, we can read the above expression as: &lt;em&gt;all elements of $d$ in $\textit{D}$ such that $d$ is black&lt;/em&gt;. And that’s how we obtain the set $\textit{B}$ from $\textit{A}$.&lt;/p&gt;

&lt;p&gt;Set generation, as defined before, depends on the &lt;strong&gt;axiom of specification&lt;/strong&gt;: &lt;em&gt;to every set $\textit{A}$ and to every condition $\textit{S}(x)$ there corresponds a set $\textit{B}$ whose elements are exactly those elements $a \in \textit{A}$ for which $\textit{S}(x)$ holds.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;A condition $\textit{S}(x)$ is any &lt;em&gt;sentence&lt;/em&gt; or &lt;em&gt;assertion&lt;/em&gt; about elements of $\textit{A}$. Valid sentences are either of &lt;em&gt;belonging&lt;/em&gt; or &lt;em&gt;equality&lt;/em&gt;. When we combine belonging and equality assertions with logic operators (not, if, and or, etc), we can build any legal set.&lt;/p&gt;

&lt;h2 id=&quot;ordered-pairs&quot;&gt;Ordered pairs&lt;/h2&gt;

&lt;p&gt;Pairs of sets come in two flavors: &lt;em&gt;unordered&lt;/em&gt; and &lt;em&gt;ordered&lt;/em&gt;. We care about pairs of sets as we need them to define a notion of relations and functions (from here I’ll denote sets with lower-case for convenience, but keep in mind we’re still talking about sets).&lt;/p&gt;

&lt;p&gt;Consider a pair of sets $\textit{x}$ and $\textit{y}$. An &lt;strong&gt;unordered pair&lt;/strong&gt; is a set whose elements are ${ \textit{x},\textit{y} }$, and ${ \textit{x},\textit{y} } = { \textit{y},\textit{x} } $. Therefore, presentation order does not matter, the set is the same.&lt;/p&gt;

&lt;p&gt;In machine learning, we usually do care about presentation order. For this, we need to define an &lt;strong&gt;ordered pair&lt;/strong&gt; (I’ll introduce this at an intuitive level, to avoid to introduce too many new concepts). An &lt;strong&gt;ordered pair&lt;/strong&gt; is denoted as $( \textit{x},\textit{y} )$, with $\textit{x}$ as the &lt;em&gt;first coordinate&lt;/em&gt; and $\textit{y}$ as the &lt;em&gt;second coordinate&lt;/em&gt;. A valid ordered pair has the property that $( \textit{x},\textit{y} ) \ne ( \textit{y},\textit{x} )$.&lt;/p&gt;

&lt;h2 id=&quot;relations&quot;&gt;Relations&lt;/h2&gt;

&lt;p&gt;From ordered pairs, we can derive the idea of &lt;strong&gt;relations&lt;/strong&gt; among sets or between elements and sets. Relations can be binary, ternary, quaternary, or N-ary. Here we are just concerned with binary relationships. In set theory, &lt;strong&gt;relations&lt;/strong&gt; are defined as &lt;em&gt;sets of ordered pairs&lt;/em&gt;, and denoted as $\textit{R}$. Hence, we can express the relation between $\textit{x}$ and $\textit{y}$ as:&lt;/p&gt;

\[\textit{x R y}\]

&lt;p&gt;Further, for any $\textit{z} \in \textit{R}$, there exist $\textit{x}$ and $\textit{y}$ such that $\textit{z} = (\textit{x}, \textit{y})$.&lt;/p&gt;

&lt;p&gt;From the definition of $\textit{R}$, we can obtain the notions of &lt;strong&gt;domain&lt;/strong&gt; and &lt;strong&gt;range&lt;/strong&gt;. The &lt;strong&gt;domain&lt;/strong&gt; is a set defined as:&lt;/p&gt;

\[\text{dom } \textit{R} = \{ \textit{x:  for some y } ( \textit{x R y)} \}\]

&lt;p&gt;This reads as: the values of $\textit{x}$ such that for at least one element of $\textit{y}$, $\textit{x}$ has a relation with $\textit{y}$.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;range&lt;/strong&gt; is a set defined as:&lt;/p&gt;

\[\text{ran } \textit{R} = \{ \textit{y:  for some x } ( \textit{x R y)} \}\]

&lt;p&gt;This reads: the set formed by the values of $\text{y}$ such that at least one element of $\textit{x}$, $\textit{x}$ has a relation with $\textit{y}$.&lt;/p&gt;

&lt;h2 id=&quot;functions&quot;&gt;Functions&lt;/h2&gt;

&lt;p&gt;Consider a pair of sets $\textit{X}$ and $\textit{Y}$. We say that a &lt;strong&gt;function&lt;/strong&gt; from $\textit{X}$ to $\textit{Y}$ is relation such that:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$dom \textit{ f} = \textit{X}$ and&lt;/li&gt;
  &lt;li&gt;such that for each $\textit{x} \in \textit{X}$ there is a unique element of $\textit{y} \in \textit{Y}$ with $(\textit{x}, \textit{y}) \in {f}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;More informally, we say that a function “&lt;em&gt;transform&lt;/em&gt;” or “&lt;em&gt;maps&lt;/em&gt;” or “&lt;em&gt;sends&lt;/em&gt;” $\textit{x}$ onto $\textit{y}$, and for each “&lt;em&gt;argument&lt;/em&gt;” $\textit{x}$ there is a unique value $\textit{y}$ that $\textit{f }$ “&lt;em&gt;assummes&lt;/em&gt;” or “&lt;em&gt;takes&lt;/em&gt;”.&lt;/p&gt;

&lt;p&gt;We typically denote a relation or function or transformation or mapping from X onto Y as:&lt;/p&gt;

\[\textit{f}: \textit{X} \rightarrow \textit{Y}\]

&lt;p&gt;or&lt;/p&gt;

\[\textit{f}(\textit{x}) = \textit{y}\]

&lt;p&gt;The simples way to see the effect of this definition of a function is with a chart. In &lt;strong&gt;Fig. 1&lt;/strong&gt;, the left-pane shows a valid function, i.e., each value $\textit{f}(\textit{x})$ &lt;em&gt;maps&lt;/em&gt; uniquely onto one value of $\textit{y}$. The right-pane is not a function, since each value $\textit{f}(\textit{x})$ &lt;em&gt;maps&lt;/em&gt; onto multiple values of $\textit{y}$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fig. 1: Functions&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-10/b-function.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For $\textit{f}: \textit{X} \rightarrow \textit{Y}$, the &lt;em&gt;domain&lt;/em&gt; of $\textit{f}$ equals to $\textit{X}$, but the &lt;em&gt;range&lt;/em&gt; does not necessarily equals to $\textit{Y}$. Just recall that the &lt;em&gt;range&lt;/em&gt; includes only the elements for which $\textit{Y}$ has a relation with $\textit{X}$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The ultimate goal of machine learning is learning functions from data&lt;/strong&gt;, i.e., transformations or mappings from the &lt;em&gt;domain&lt;/em&gt; onto the &lt;em&gt;range&lt;/em&gt; of a function. This may sound simplistic, but it’s true. The &lt;em&gt;domain&lt;/em&gt; $\textit{X}$ is usually a vector (or set) of &lt;em&gt;variables&lt;/em&gt; or &lt;em&gt;features&lt;/em&gt; mapping onto a vector of &lt;em&gt;target&lt;/em&gt; values. Finally, I want to emphasize that in machine learning the words transformation and mapping are used interchangeably, but both just mean function.&lt;/p&gt;

&lt;p&gt;This is all I’ll cover about sets and functions. My goals were just to introduce: (1) &lt;strong&gt;the concept of a set&lt;/strong&gt;, (2) &lt;strong&gt;basic set notation&lt;/strong&gt;, (3) &lt;strong&gt;how sets are generated&lt;/strong&gt;, (4) &lt;strong&gt;how sets allow the definition of functions&lt;/strong&gt;, (5) &lt;strong&gt;the concept of a function&lt;/strong&gt;. Set theory is a monumental field, but there is no need to learn everything about sets to understand linear algebra. Halmo’s &lt;strong&gt;Naive set theory&lt;/strong&gt; (not free, but you can find a copy for ~\$8-$10 US) is a fantastic book for people that just need to understand the most fundamental ideas in a relatively informal manner.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Libraries for this section
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;altair&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;themes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;enable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;dark&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ThemeRegistry.enable(&apos;dark&apos;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;vectors&quot;&gt;Vectors&lt;/h1&gt;

&lt;p&gt;Linear algebra is the study of vectors. At the most general level, vectors are &lt;strong&gt;ordered finite lists of numbers&lt;/strong&gt;. Vectors are the most fundamental mathematical object in machine learning. We use them to &lt;strong&gt;represent attributes of entities&lt;/strong&gt;: age, sex, test scores, etc. We represent vectors by a bold lower-case letter like $\bf{v}$ or as a lower-case letter with an arrow on top like $\vec{v}$.&lt;/p&gt;

&lt;p&gt;Vectors are a type of mathematical object that can be &lt;strong&gt;added together&lt;/strong&gt; and/or &lt;strong&gt;multiplied by a number&lt;/strong&gt; to obtain another object of &lt;strong&gt;the same kind&lt;/strong&gt;. For instance, if we have a vector $\bf{x} = \text{age}$ and a second vector $\bf{y} = \text{weight}$, we can add them together and obtain a third vector $\bf{z} = x + y$. We can also multiply $2 \times \bf{x}$ to obtain $2\bf{x}$, again, a vector. This is what we mean by &lt;em&gt;the same kind&lt;/em&gt;: the returning object is still a &lt;em&gt;vector&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;types-of-vectors&quot;&gt;Types of vectors&lt;/h2&gt;

&lt;p&gt;Vectors come in three flavors: (1) &lt;strong&gt;geometric vectors&lt;/strong&gt;, (2) &lt;strong&gt;polynomials&lt;/strong&gt;, (3) and &lt;strong&gt;elements of $\mathbb{R^n}$ space&lt;/strong&gt;. We will defined each one next.&lt;/p&gt;

&lt;h3 id=&quot;geometric-vectors&quot;&gt;Geometric vectors&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Geometric vectors are oriented segments&lt;/strong&gt;. Therse are the kind of vectors you probably learned about in high-school physics and geometry. Many linear algebra concepts come from the geometric point of view of vectors: space, plane, distance, etc.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fig. 2: Geometric vectors&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-10/b-geometric-vectors.svg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;polynomials&quot;&gt;Polynomials&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;A polynomial is an expression like $f(x) = x^2 + y + 1$&lt;/strong&gt;. This is, a expression adding multiple “terms” (nomials). Polynomials are vectors because they meet the definition of a vector: they can be added together to get another polynomial, and they can be multiplied together to get another polynomial.&lt;/p&gt;

\[\text{function addition is valid} \\
f(x) + g(x)\\\]

\[and\\\]

\[\text{multiplying by a scalar is valid} \\
5 \times f(x)\]

&lt;p&gt;&lt;strong&gt;Fig. 3: Polynomials&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-10/b-polynomials-vectors.svg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;elements-of-r&quot;&gt;Elements of R&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Elements of $\mathbb{R}^n$ are sets of real numbers&lt;/strong&gt;. This type of representation is arguably the most important for applied machine learning. It is how data is commonly represented in computers to build machine learning models. For instance, a vector in $\mathbb{R}^3$ takes the shape of:&lt;/p&gt;

\[\bf{x}=
\begin{bmatrix}
x_1 \\
x_2 \\
x_3
\end{bmatrix}
\in \mathbb{R}^3\]

&lt;p&gt;Indicating that it contains three dimensions.&lt;/p&gt;

\[\text{addition is valid} \\
\phantom{space}\\
\begin{bmatrix}
1 \\
2 \\
3
\end{bmatrix} +
\begin{bmatrix}
1 \\
2 \\
3
\end{bmatrix}=
\begin{bmatrix}
2 \\
4 \\
6
\end{bmatrix}\\\]

\[and\\\]

\[\text{multiplying by a scalar is valid} \\
\phantom{space}\\
5 \times
\begin{bmatrix}
1 \\
2 \\
3
\end{bmatrix} =
\begin{bmatrix}
5 \\
10 \\
15
\end{bmatrix}\]

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; vectors are represented as n-dimensional arrays. To create a vector in $\mathbb{R^3}$:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can inspect the vector shape by:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (3 dimensions, 1 element on each)
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(3, 1)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;A 3-dimensional vector:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;A 3-dimensional vector:
[[1]
 [2]
 [3]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;zero-vector-unit-vector-and-sparse-vector&quot;&gt;Zero vector, unit vector, and sparse vector&lt;/h2&gt;

&lt;p&gt;There are a couple of “special” vectors worth to remember as they will be mentioned frequently on applied linear algebra: (1) zero vector, (2) unit vector, (3) sparse vectors&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Zero vectors&lt;/strong&gt;, are vectors composed of zeros, and zeros only. It is common to see this vector denoted as simply $0$, regardless of the dimensionality. Hence, you may see a 3-dimensional or 10-dimensional with all entries equal to 0, refered as “the 0” vector. For instance:&lt;/p&gt;

\[\bf{0} =
\begin{bmatrix}
0\\
0\\
0
\end{bmatrix}\]

&lt;p&gt;&lt;strong&gt;Unit vectors&lt;/strong&gt;, are vectors composed of a single element equal to one, and the rest to zero. Unit vectors are important to understand applications like norms. For instance, $\bf{x_1}$, $\bf{x_2}$, and $\bf{x_3}$ are unit vectors:&lt;/p&gt;

\[\bf{x_1} =
\begin{bmatrix}
1\\
0\\
0
\end{bmatrix},
\bf{x_2} =
\begin{bmatrix}
0\\
1\\
0
\end{bmatrix},
\bf{x_3} =
\begin{bmatrix}
0\\
0\\
1
\end{bmatrix}\]

&lt;p&gt;&lt;strong&gt;Sparse vectors&lt;/strong&gt;, are vectors with most of its elements equal to zero. We denote the number of nonzero elements of a vector $\bf{x}$ as $nnz(x)$. The sparser possible vector is the zero vector. Sparse vectors are common in machine learning applications and often require some type of method to deal with them effectively.&lt;/p&gt;

&lt;h2 id=&quot;vector-dimensions-and-coordinate-system&quot;&gt;Vector dimensions and coordinate system&lt;/h2&gt;

&lt;p&gt;Vectors can have any number of dimensions. The most common are the 2-dimensional cartesian plane, and the 3-dimensional space. Vectors in 2 and 3 dimensions are used often for pedgagogical purposes since we can visualize them as geometric vectors. Nevetheless, most problems in machine learning entail more dimensions, sometiome hundreds or thousands of dimensions. The notation for a vector $\bf{x}$ of arbitrary dimensions, $n$ is:&lt;/p&gt;

\[\bf{x} =
\begin{bmatrix}
x_1 \\ x_2 \\ \vdots \\ x_n
\end{bmatrix}
\in \mathbb{R}^n\]

&lt;p&gt;Vectors dimensions map into &lt;strong&gt;coordinate systems or perpendicular axes&lt;/strong&gt;. Coordinate systems have an origin at $(0,0,0)$, hence, when we define a vector:&lt;/p&gt;

\[\bf{x} = \begin{bmatrix} 3 \\ 2 \\ 1 \end{bmatrix} \in \mathbb{R}^3\]

&lt;p&gt;we are saying: starting from the origin, move 3 units in the 1st perpendicular axis, 2 units in the 2nd perpendicular axis, and 1 unit in the 3rd perpendicular axis. We will see later that when we have a set of perpendicular axes we obtain the basis of a vector space.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fig. 4: Coordinate systems&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-10/b-coordinate-system.svg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;basic-vector-operations&quot;&gt;Basic vector operations&lt;/h2&gt;

&lt;h3 id=&quot;vector-vector-addition&quot;&gt;Vector-vector addition&lt;/h3&gt;

&lt;p&gt;We used vector-vector addition to define vectors without defining vector-vector addition. Vector-vector addition is an element-wise operation, only defined for vectors of the same size (i.e., number of elements). Consider two vectors of the same size, then:&lt;/p&gt;

\[\bf{x} + \bf{y} =
\begin{bmatrix}
x_1\\
\vdots\\
x_n
\end{bmatrix}+
\begin{bmatrix}
y_1\\
\vdots\\
y_n
\end{bmatrix} =
\begin{bmatrix}
x_1 + y_1\\
\vdots\\
x_n + y_n
\end{bmatrix}\]

&lt;p&gt;For instance:&lt;/p&gt;

\[\bf{x} + \bf{y} =
\begin{bmatrix}
1\\
2\\
3
\end{bmatrix}+
\begin{bmatrix}
1\\
2\\
3
\end{bmatrix} =
\begin{bmatrix}
1 + 1\\
2 + 2\\
3 + 3
\end{bmatrix} =
\begin{bmatrix}
2\\
4\\
6
\end{bmatrix}\]

&lt;p&gt;Vector addition has a series of &lt;strong&gt;fundamental properties&lt;/strong&gt; worth mentioning:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Commutativity: $x + y = y + x$&lt;/li&gt;
  &lt;li&gt;Associativity: $(x + y) + z = x + (y + z)$&lt;/li&gt;
  &lt;li&gt;Adding the zero vector has no effect: $x + 0 = 0 + x = x$&lt;/li&gt;
  &lt;li&gt;Substracting a vector from itself returns the zero vector: $x - x = 0$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;, we add two vectors of the same with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;+&lt;/code&gt; operator or the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt; method:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                  &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                  &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([[2],
       [4],
       [6]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([[2],
       [4],
       [6]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;vector-scalar-multiplication&quot;&gt;Vector-scalar multiplication&lt;/h3&gt;

&lt;p&gt;Vector-scalar multiplication is an element-wise operation. It’s defined as:&lt;/p&gt;

\[\alpha \bf{x} =
\begin{bmatrix}
\alpha \bf{x_1}\\
\vdots \\
\alpha \bf{x_n}
\end{bmatrix}\]

&lt;p&gt;Consider $\alpha = 2$ and $\bf{x} = \begin{bmatrix} 1 \ 2 \ 3 \end{bmatrix}$:&lt;/p&gt;

\[\alpha \bf{x} =
\begin{bmatrix}
2 \times 1\\
2 \times 2\\
2 \times 3
\end{bmatrix} =
\begin{bmatrix}
2\\
4\\
6
\end{bmatrix}\]

&lt;p&gt;Vector-scalar multiplication satisfies a series of important properties:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Associativity: $(\alpha \beta) \bf{x} = \alpha (\beta \bf{x})$&lt;/li&gt;
  &lt;li&gt;Left-distributive property: $(\alpha + \beta) \bf{x} = \alpha \bf{x} + \beta \bf{x}$&lt;/li&gt;
  &lt;li&gt;Right-distributive property: $\bf{x} (\alpha + \beta) = \bf{x} \alpha + \bf{x} \beta$&lt;/li&gt;
  &lt;li&gt;Right-distributive property for vector addition: $\alpha (\bf{x} + \bf{y}) = \alpha \bf{x} + \alpha \bf{y}$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;, we compute scalar-vector multiplication with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*&lt;/code&gt; operator:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
             &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
             &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([[2],
       [4],
       [6]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;linear-combinations-of-vectors&quot;&gt;Linear combinations of vectors&lt;/h3&gt;

&lt;p&gt;There are only two legal operations with vectors in linear algebra: &lt;strong&gt;addition&lt;/strong&gt; and &lt;strong&gt;multiplication by numbers&lt;/strong&gt;. When we combine those, we get a &lt;strong&gt;linear combination&lt;/strong&gt;.&lt;/p&gt;

\[\alpha \bf{x} + \beta \bf{y} =
\alpha
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix}+
\beta
\begin{bmatrix}
y_1 \\
y_2
\end{bmatrix}=
\begin{bmatrix}
\alpha x_1 + \alpha x_2\\
\beta y_1 + \beta y_2
\end{bmatrix}\]

&lt;p&gt;Consider $\alpha = 2$, $\beta = 3$, $\bf{x}=\begin{bmatrix}2 \ 3\end{bmatrix}$, and $\begin{bmatrix}4 \ 5\end{bmatrix}$.&lt;/p&gt;

&lt;p&gt;We obtain:&lt;/p&gt;

\[\alpha \bf{x} + \beta \bf{y} =
2
\begin{bmatrix}
2 \\
3
\end{bmatrix}+
3
\begin{bmatrix}
4 \\
5
\end{bmatrix}=
\begin{bmatrix}
2 \times 2 + 2 \times 4\\
2 \times 3 + 3 \times 5
\end{bmatrix}=
\begin{bmatrix}
10 \\
21
\end{bmatrix}\]

&lt;p&gt;Another way to express linear combinations you’ll see often is with summation notation. Consider a set of vectors $x_1, …, x_k$ and scalars $\beta_1, …, \beta_k \in \mathbb{R}$, then:&lt;/p&gt;

\[\sum_{i=1}^k \beta_i x_i := \beta_1x_1 + ... + \beta_kx_k\]

&lt;p&gt;Note that $:=$ means “&lt;em&gt;is defined as&lt;/em&gt;”.&lt;/p&gt;

&lt;p&gt;Linear combinations are the most fundamental operation in linear algebra. Everything in linear algebra results from linear combinations. For instance, linear regression is a linear combination of vectors. &lt;strong&gt;Fig. 2&lt;/strong&gt; shows an example of how adding two geometrical vectors looks like for intuition.&lt;/p&gt;

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;, we do linear combinations as:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([[16],
       [21]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;vector-vector-multiplication-dot-product&quot;&gt;Vector-vector multiplication: dot product&lt;/h3&gt;

&lt;p&gt;We covered vector addition and multiplication by scalars. Now I will define vector-vector multiplication, commonly known as a &lt;strong&gt;dot product&lt;/strong&gt; or &lt;strong&gt;inner product&lt;/strong&gt;. The dot product of $\bf{x}$ and $\bf{y}$ is defined as:&lt;/p&gt;

\[\bf{x} \cdot \bf{y} :=
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix}^T
\begin{bmatrix}
y_1 \\
y_2
\end{bmatrix} =
\begin{bmatrix}
x_1 &amp;amp; x_2
\end{bmatrix}
\begin{bmatrix}
y_1 \\
y_2
\end{bmatrix} =
x_1 \times y_1 + x_2 \times y_2\]

&lt;p&gt;Where the $T$ superscript denotes the transpose of the vector. Transposing a vector just means to “flip” the column vector to a row vector counterclockwise. For instance:&lt;/p&gt;

\[\bf{x} \cdot \bf{y} =
\begin{bmatrix}
-2 \\
2
\end{bmatrix}
\begin{bmatrix}
4 \\
-3
\end{bmatrix} =
\begin{bmatrix}
-2 &amp;amp; 2
\end{bmatrix}
\begin{bmatrix}
4 \\
-3
\end{bmatrix} =
-2 \times 4 + 2 \times -3 = (-8) + (-6) = -14\]

&lt;p&gt;Dot products are so important in machine learning, that after a while they become second nature for practitioners.&lt;/p&gt;

&lt;p&gt;To multiply two vectors with dimensions (rows=2, cols=1) in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Numpy&lt;/code&gt;, we need to transpose the first vector at using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;@&lt;/code&gt; operator:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([[-14]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;vector-space-span-and-subspace&quot;&gt;Vector space, span, and subspace&lt;/h2&gt;

&lt;h3 id=&quot;vector-space&quot;&gt;Vector space&lt;/h3&gt;

&lt;p&gt;In its more general form, a &lt;strong&gt;vector space&lt;/strong&gt;, also known as &lt;strong&gt;linear space&lt;/strong&gt;, is a collection of objects that follow the rules defined for vectors in $\mathbb{R}^n$. We mentioned those rules when we defined vectors: they can be added together and multiplied by scalars, and return vectors of the same type. More colloquially, a vector space is the set of proper vectors and all possible linear combinatios of the vector set. In addition, vector addition and multiplication must follow these eight rules:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;commutativity: $x + y = y + x$&lt;/li&gt;
  &lt;li&gt;associativity: $x + (y + x) = (y + x) + z$&lt;/li&gt;
  &lt;li&gt;unique zero vector such that: $x + 0 = x$ $\forall$ $x$&lt;/li&gt;
  &lt;li&gt;$\forall$ $x$ there is a unique vector $x$ such that $x + -x = 0$&lt;/li&gt;
  &lt;li&gt;identity element of scalar multiplication: $1x = x$&lt;/li&gt;
  &lt;li&gt;distributivity of scalar multiplication w.r.t vector addition: $x(y + z) = xz + zy$&lt;/li&gt;
  &lt;li&gt;$x(yz) = (xy)z$&lt;/li&gt;
  &lt;li&gt;$(y + z)x = yx + zx$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In my experience remembering these properties is not really important, but it’s good to know that such rules exist.&lt;/p&gt;

&lt;h3 id=&quot;vector-span&quot;&gt;Vector span&lt;/h3&gt;

&lt;p&gt;Consider the vectors $\bf{x}$ and $\bf{y}$ and the scalars $\alpha$ and $\beta$. If we take &lt;em&gt;all&lt;/em&gt; possible linear combinations of $\alpha \bf{x} + \beta \bf{y}$ we would obtain the &lt;strong&gt;span&lt;/strong&gt; of such vectors. This is easier to grasp when you think about geometric vectors. If our vectors $\bf{x}$ and $\bf{y}$ point into &lt;strong&gt;different directions&lt;/strong&gt; in the 2-dimensional space, we get that the $span(x,y)$ is equal to &lt;strong&gt;the entire 2-dimensional plane&lt;/strong&gt;, as shown in the middle-pane in &lt;strong&gt;Fig. 5&lt;/strong&gt;. Just imagine having an unlimited number of two types of sticks: one pointing vertically, and one pointing horizontally. Now, you can reach any point in the 2-dimensional space by simply combining the necessary number of vertical and horizontal sticks (including taking fractions of sticks).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fig. 5: Vector Span&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-10/b-vector-span.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;What would happen if the vectors point in the same direction? Now, if you combine them, you just can &lt;strong&gt;span a line&lt;/strong&gt;, as shown in the left-pane in &lt;strong&gt;Fig. 5&lt;/strong&gt;. If you have ever heard of the term “multicollinearity”, it’s closely related to this issue: when two variables are “colinear” they are pointing in the same direction, hence they provide redundant information, so can drop one without information loss.&lt;/p&gt;

&lt;p&gt;With three vectors pointing into different directions, we can span the entire 3-dimensional space or a &lt;strong&gt;hyper-plane&lt;/strong&gt;, as in the right-pane of &lt;strong&gt;Fig. 5&lt;/strong&gt;. Note that the sphere is just meant as a 3-D reference, not as a limit.&lt;/p&gt;

&lt;p&gt;Four vectors pointing into different directions will span the 4-dimensional space, and so on. From here our geometrical intuition can’t help us. This is an example of how linear algebra can describe the behavior of vectors beyond our basics intuitions.&lt;/p&gt;

&lt;h3 id=&quot;vector-subspaces&quot;&gt;Vector subspaces&lt;/h3&gt;

&lt;p&gt;A &lt;strong&gt;vector subspace (or linear subspace) is a vector space that lies within a larger vector space&lt;/strong&gt;. These are also known as linear subspaces. Consider a subspace $S$. For a vector to be a valid subspace it has to meet &lt;strong&gt;three conditions&lt;/strong&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Contains the zero vector, $\bf{0} \in S$&lt;/li&gt;
  &lt;li&gt;Closure under multiplication, $\forall \alpha \in \mathbb{R} \rightarrow  \alpha \times s_i \in S$&lt;/li&gt;
  &lt;li&gt;Closure under addition, $\forall s_i \in S \rightarrow  s_1 + s_2 \in S$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Intuitively, you can think in closure as being unable to “jump out” from space into another. A pair of vectors laying flat in the 2-dimensional space, can’t, by either addition or multiplication, “jump out” into the 3-dimensional space.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fig. 6: Vector subspaces&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-10/b-vector-subspace.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Consider the following questions: Is $\bf{x}=\begin{bmatrix} 1 \ 1 \end{bmatrix}$ a valid subspace of $\mathbb{R^2}$? Let’s evaluate $\bf{x}$ on the three conditions:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Contains the zero vector&lt;/strong&gt;: it does. Remember that the span of a vector are all linear combinations of such a vector. Therefore, we can simply multiply by $0$ to get $\begin{bmatrix}0 \ 0 \end{bmatrix}$:&lt;/p&gt;

\[\bf{x}\times 0=0
\begin{bmatrix}
1 \\
1
\end{bmatrix}
=
\begin{bmatrix}
0 \\
0
\end{bmatrix}\]

&lt;p&gt;&lt;strong&gt;Closure under multiplication&lt;/strong&gt; implies that if take any vector belonging to $\bf{x}$ and multiply by any real scalar $\alpha$, the resulting vector stays within the span of $\bf{x}$. Algebraically is easy to see that we can multiply $\begin{bmatrix} 1 \ 1 \end{bmatrix}$ by any scalar $\alpha$, and the resulting vector remains in the 2-dimensional plane (i.e., the span of $\mathbb{R}^2$).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Closure under addition&lt;/strong&gt; implies that if we add together any vectors belonging to $\bf{x}$, the resulting vector remains within the span of $\mathbb{R}^2$. Again, algebraically is clear that if we add $\bf{x}$ + $\bf{x}$, the resulting vector will remain in $\mathbb{R}^2$. There is no way to get to $\mathbb{R^3}$ or $\mathbb{R^4}$ or any space outside the two-dimensional plane by adding $\bf{x}$ multiple times.&lt;/p&gt;

&lt;h2 id=&quot;linear-dependence-and-independence&quot;&gt;Linear dependence and independence&lt;/h2&gt;

&lt;p&gt;The left-pane shows a triplet of &lt;strong&gt;linearly dependent&lt;/strong&gt; vectors, whereas the right-pane shows a triplet of &lt;strong&gt;linearly independent&lt;/strong&gt; vectors.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fig. 7: Linear dependence and independence&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-10/b-linear-independence.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A set of vectors is &lt;strong&gt;linearly dependent&lt;/strong&gt; if at least one vector can be obtained as a linear combination of other vectors in the set. As you can see in the left pane, we can combine vectors $x$ and $y$ to obtain $z$.&lt;/p&gt;

&lt;p&gt;There is more rigurous (but slightly harder to grasp) definition of linear dependence. Consider a set of vectors $x_1, …, x_k$ and scalars $\beta \in \mathbb{R}$. If there is a way to get $0 = \sum_{i=1}^k \beta_i x_i$ with at least one $\beta \ne 0$, we have linearly dependent vectors. In other words, if we can get the zero vector as a linear combination of the vectors in the set, with weights that &lt;em&gt;are not&lt;/em&gt; all zero, we have a linearly dependent set.&lt;/p&gt;

&lt;p&gt;A set of vectors is &lt;strong&gt;linearly independent&lt;/strong&gt; if none vector can be obtained as a linear combination of other vectors in the set. As you can see in the right pane, there is no way for us to combine vectors $x$ and $y$ to obtain $z$. Again, consider a set of vectors $x_1, …, x_k$ and scalars $\beta \in \mathbb{R}$. If the only way to get $0 = \sum_{i=1}^k \beta_i x_i$ requires all $\beta_1, …, \beta_k = 0$, the we have linearly independent vectors. In words, the only way to get the zero vectors in by multoplying each vector in the set by $0$.&lt;/p&gt;

&lt;p&gt;The importance of the concepts of linear dependence and independence will become clearer in more advanced topics. For now, the important points to remember are: linearly dependent vectors contain &lt;strong&gt;redundant information&lt;/strong&gt;, whereas linearly independent vectors do not.&lt;/p&gt;

&lt;h2 id=&quot;vector-null-space&quot;&gt;Vector null space&lt;/h2&gt;

&lt;p&gt;Now that we know what subspaces and linear dependent vectors are, we can introduce the idea of the &lt;strong&gt;null space&lt;/strong&gt;. Intuitively, the null space of a set of vectors are &lt;strong&gt;all linear combinations that “map” into the zero vector&lt;/strong&gt;. Consider a set of geometric vectors $\bf{w}$, $\bf{x}$, $\bf{y}$, and $\bf{z}$ as in &lt;strong&gt;Fig. 8&lt;/strong&gt;. By inspection, we can see that vectors $\bf{x}$ and $\bf{z}$ are parallel to each other, hence, independent. On the contrary, vectors $\bf{w}$ and $\bf{y}$ can be obtained as linear combinations of $\bf{x}$ and $\bf{z}$, therefore, dependent.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fig. 8: Vector null space&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-10/b-vector-null-space.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As result, with this four vectors, we can form the following two combinations that will “map” into the origin of the coordinate system, this is, the zero vector $(0,0)$:&lt;/p&gt;

\[\begin{matrix}
z - y + x = 0 \\
z - x + w = 0
\end{matrix}\]

&lt;p&gt;We will see how this idea of the null space extends naturally in the context of matrices later.&lt;/p&gt;

&lt;h2 id=&quot;vector-norms&quot;&gt;Vector norms&lt;/h2&gt;

&lt;p&gt;Measuring vectors is another important operation in machine learning applications. Intuitively, we can think about the &lt;strong&gt;norm&lt;/strong&gt; or the &lt;strong&gt;length&lt;/strong&gt; of a vector as the distance between its “origin” and its “end”.&lt;/p&gt;

&lt;p&gt;Norms “map” vectors to non-negative values. In this sense are functions that assign length $\lVert \bf{x} \rVert \in \mathbb{R^n}$ to a vector $\bf{x}$. To be valid, a norm has to satisfy these properties (keep in mind these properties are a bit abstruse to understand):&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Absolutely homogeneous&lt;/strong&gt;: $\forall \alpha \in \mathbb{R},  \lVert \alpha \bf{x} \rVert = \vert \alpha \Vert \lVert \bf{x} \rVert$. In words: for all real-valued scalars, the norm scales proportionally with the value of the scalar.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Triangle inequality&lt;/strong&gt;: $\lVert \bf{x} + \bf{y} \rVert \le \lVert \bf{x} \rVert + \lVert \bf{y} \rVert $. In words: in geometric terms, for any triangle the sum of any two sides must be greater or equal to the lenght of the third side. This is easy to see experimentally: grab a piece of rope, form triangles of different sizes, measure all the sides, and test this property.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Positive definite&lt;/strong&gt;: $\lVert \bf{x} \rVert \ge 0$ and $ \lVert \bf{x} \rVert = 0 \Longleftrightarrow \bf{x}= 0$. In words: the length of any $\bf{x}$ has to be a positive value (i.e., a vector can’t have negative length), and a length of $0$ occurs only of $\bf{x}=0$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Grasping the meaning of these three properties may be difficult at this point, but they probably become clearer as you improve your understanding of linear algebra.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fig. 9: Vector norms&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-10/b-l2-norm.svg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;euclidean-norm&quot;&gt;Euclidean norm&lt;/h3&gt;

&lt;p&gt;The Euclidean norm is one of the most popular norms in machine learning. It is so widely used that sometimes is refered simply as “the norm” of a vector. Is defined as:&lt;/p&gt;

\[\lVert \bf{x} \rVert_2 := \sqrt{\sum_{i=1}^n x_i^2} = \sqrt{x^Tx}\]

&lt;p&gt;Hence, in &lt;strong&gt;two dimensions&lt;/strong&gt; the $L_2$ norm is:&lt;/p&gt;

\[\lVert \bf{x} \rVert_2 \in \mathbb{R}^2 = \sqrt {x_1^2  \cdot x_2^2 }\]

&lt;p&gt;Which is equivalent to the formula for the hypotenuse a triangle with sides $x_1^2$ and $x_2^2$.&lt;/p&gt;

&lt;p&gt;The same pattern follows for higher dimensions of $\mathbb{R^n}$&lt;/p&gt;

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;, we can compute the $L_2$ norm as:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;5.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you remember the first “Pythagorean triple”, you can confirm that the norm is correct.&lt;/p&gt;

&lt;h3 id=&quot;manhattan-norm&quot;&gt;Manhattan norm&lt;/h3&gt;

&lt;p&gt;The Manhattan or $L_1$ norm gets its name in analogy to measuring distances while moving in Manhattan, NYC. Since Manhattan has a grid-shape, the distance between any two points is measured by moving in vertical and horizontals lines (instead of diagonals as in the Euclidean norm). It is defined as:&lt;/p&gt;

\[\lVert \bf{x} \rVert_1 := \sum_{i=1}^n \vert x_i \vert\]

&lt;p&gt;Where $\vert x_i \vert$ is the absolute value. The $L_1$ norm is preferred when discriminating between elements that are exactly zero and elements that are small but not zero.&lt;/p&gt;

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; we compute the $L_1$ norm as&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;7.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Is easy to confirm that the sum of the absolute values of $3$ and $-4$ is $7$.&lt;/p&gt;

&lt;h3 id=&quot;max-norm&quot;&gt;Max norm&lt;/h3&gt;

&lt;p&gt;The max norm or infinity norm is simply the absolute value of the largest element in the vector. It is defined as:&lt;/p&gt;

\[\lVert \bf{x} \rVert_\infty := max_i \vert x_i \vert\]

&lt;p&gt;Where $\vert x_i \vert$ is the absolute value. For instance, for a vector with elements $\bf{x} = \begin{bmatrix} 1 &amp;amp; 2 &amp;amp; 3 \end{bmatrix}$, the $\lVert \bf{x} \rVert_\infty = 3$&lt;/p&gt;

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; we compute the $L_\infty$ norm as:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;4.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;vector-inner-product-length-and-distance&quot;&gt;Vector inner product, length, and distance.&lt;/h2&gt;

&lt;p&gt;For practical purposes, inner product and length are used as equivalent to dot product and norm, although technically are not the same.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Inner products&lt;/strong&gt; are a more general concept that dot products, with a series of additional properties (see &lt;a href=&quot;https://en.wikipedia.org/wiki/Inner_product_space#Elementary_properties&quot;&gt;here&lt;/a&gt;). In other words, every dot product is an inner product, but not every inner product is a dot product. The notation for the inner product is usually a pair of angle brackets as $\langle  .,. \rangle$ as. For instance, the scalar inner product is defined as:&lt;/p&gt;

\[\langle x,y \rangle := x\cdot y\]

&lt;p&gt;In $\mathbb{R}^n$ the inner product is a dot product defined as:&lt;/p&gt;

\[\Bigg \langle
\begin{bmatrix}
x_1 \\
\vdots\\
x_n
\end{bmatrix},
\begin{bmatrix}
y_1 \\
\vdots\\
y_n
\end{bmatrix}
\Bigg \rangle :=
x \cdot y = \sum_{i=1}^n x_iy_i\]

&lt;p&gt;&lt;strong&gt;Length&lt;/strong&gt; is a concept from geometry. We say that geometric vectors have length and that vectors in $\mathbb{R}^n$ have norm. In practice, many machine learning textbooks use these concepts interchangeably. I’ve found authors saying things like “we use the $l_2$ norm to compute the &lt;em&gt;length&lt;/em&gt; of a vector”. For instance, we can compute the length of a directed segment (i.e., geometrical vector) $\bf{x}$ by taking the square root of the inner product with itself as:&lt;/p&gt;

\[\lVert x \rVert = \sqrt{\langle x,x \rangle} = \sqrt{x\cdot y} = x^2 + y^2\]

&lt;p&gt;&lt;strong&gt;Distance&lt;/strong&gt; is a relational concept. It refers to the length (or norm) of the difference between two vectors. Hence, we use norms and lengths to measure the distance between vectors. Consider the vectors $\bf{x}$ and $\bf{y}$, we define the distance $d(x,y)$ as:&lt;/p&gt;

\[d(x,y) := \lVert x - y \rVert = \sqrt{\langle x - y, x - y \rangle}\]

&lt;p&gt;When the inner product $\langle x - y, x - y \rangle$ is the dot product, the distance equals to the Euclidean distance.&lt;/p&gt;

&lt;p&gt;In machine learning, unless made explicit, we can safely assume that an inner product refers to the dot product. We already reviewed how to compute the dot product in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([[-14]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As with the inner product, usually, we can safely assume that &lt;strong&gt;distance&lt;/strong&gt; stands for the Euclidean distance or $L_2$ norm unless otherwise noted. To compute the $L_2$ distance between a pair of vectors:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;L_2 distance : &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;L_2 distance : 7.810249675906656
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;vector-angles-and-orthogonality&quot;&gt;Vector angles and orthogonality&lt;/h2&gt;

&lt;p&gt;The concepts of angle and orthogonality are also related to geometrical vectors. We saw that inner products allow for the definition of length and distance. In the same manner, inner products are used to define &lt;strong&gt;angles&lt;/strong&gt; and &lt;strong&gt;orthogonality&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;In machine learning, the &lt;strong&gt;angle&lt;/strong&gt; between a pair of vectors is used as a &lt;strong&gt;measure of vector similarity&lt;/strong&gt;. To understand angles let’s first look at the &lt;strong&gt;Cauchy–Schwarz inequality&lt;/strong&gt;. Consider a pair of non-zero vectors $\bf{x}$ and $\bf{y}$ $\in \mathbb{R}^n$. The Cauchy–Schwarz inequality states that:&lt;/p&gt;

\[\vert \langle x, y \rangle \vert \leq \Vert x \Vert \Vert y \Vert\]

&lt;p&gt;In words: &lt;em&gt;the absolute value of the inner product of a pair of vectors is less than or equal to the products of their length&lt;/em&gt;. The only case where both sides of the expression are &lt;em&gt;equal&lt;/em&gt; is when vectors are colinear, for instance, when $\bf{x}$ is a scaled version of $\bf{y}$. In the 2-dimensional case, such vectors would lie along the same line.&lt;/p&gt;

&lt;p&gt;The definition of the angle between vectors can be thought as a generalization of the &lt;strong&gt;law of cosines&lt;/strong&gt; in trigonometry, which defines for a triangle with sides $a$, $b$, and $c$, and an angle $\theta$ are related as:&lt;/p&gt;

\[c^2 = a^2 + b^2 - 2ab \cos \theta\]

&lt;p&gt;&lt;strong&gt;Fig. 10: Law of cosines and Angle between vectors&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-10/b-vector-angle.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can replace this expression with vectors lengths as:&lt;/p&gt;

\[\Vert x - y \Vert^2 = \Vert x \Vert^2 + \Vert y \Vert^2 - 2(\Vert x \Vert \Vert y \Vert) \cos \theta\]

&lt;p&gt;With a bit of algebraic manipulation, we can clear the previous equation to:&lt;/p&gt;

\[\cos \theta = \frac{\langle x, y \rangle}{\Vert x \Vert \Vert y \Vert}\]

&lt;p&gt;And there we have a &lt;strong&gt;definition for (cos) angle $\theta$&lt;/strong&gt;. Further, from the Cauchy–Schwarz inequality we know that $\cos \theta$ must be:&lt;/p&gt;

\[-1 \leq \frac{\langle x, y \rangle}{\Vert x \Vert \Vert y \Vert} \leq 1\]

&lt;p&gt;This is a necessary conclusion (range between ${-1, 1}$) since the numerator in the equation always is going to be smaller or equal to the denominator.&lt;/p&gt;

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;, we can compute the $\cos \theta$ between a pair of vectors as:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# here we translate the cos(theta) definition
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cos_theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;cos of the angle = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cos_theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cos of the angle = [[0.988]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We get that $\cos \theta \approx 0.988$. Finally, to know the exact value of $\theta$ we need to take the trigonometric inverse of the cosine function as:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;cos_inverse&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arccos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cos_theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;angle in radiants = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cos_inverse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;angle in radiants = [[0.157]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We obtain $\theta \approx 0.157 $. To fo from radiants to degrees we can use the following formula:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;degrees&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cos_inverse&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;180&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;angle in degrees = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;degrees&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;angle in degrees = [[8.973]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We obtain $\theta \approx 8.973^{\circ}$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Orthogonality&lt;/strong&gt; is often used interchangeably with “independence” although they are mathematically different concepts. Orthogonality can be seen as a generalization of perpendicularity to vectors in any number of dimensions.&lt;/p&gt;

&lt;p&gt;We say that a pair of vectors $\bf{x}$ and $\bf{y}$ are &lt;strong&gt;orthogonal&lt;/strong&gt; if their inner product is zero, $\langle x,y \rangle = 0$. The notation for a pair of orthogonal vectors is $\bf{x} \perp \bf{y}$. In the 2-dimensional plane, this equals to a pair of vectors forming a $90^{\circ}$ angle.&lt;/p&gt;

&lt;p&gt;Here is an example of orthogonal vectors&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fig. 11: Orthogonal vectors&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-10/b-orthogonal-vectors.svg&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;cos_theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;cos of the angle = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cos_theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cos of the angle = [[0.]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We see that this vectors are &lt;strong&gt;orthogonal&lt;/strong&gt; as $\cos \theta=0$. This is equal to $\approx 1.57$ radiants and $\theta = 90^{\circ}$&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;cos_inverse&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arccos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cos_theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;degrees&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cos_inverse&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;180&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;angle in radiants = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cos_inverse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;angle in degrees =&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;degrees&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;angle in radiants = [[1.571]]
angle in degrees =[[90.]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;systems-of-linear-equations&quot;&gt;Systems of linear equations&lt;/h2&gt;

&lt;p&gt;The purpose of linear algebra as a tool is to &lt;strong&gt;solve systems of linear equations&lt;/strong&gt;. Informally, this means to figure out the right combination of linear segments to obtain an outcome. Even more informally, think about making pancakes: In what proportion ($w_i \in \mathbb{R}$) we have to mix ingredients to make pancakes? You can express this as a linear equation:&lt;/p&gt;

\[f_\text{flour} \times w_1 + b_\text{baking powder}  \times w_2 + e_\text{eggs}  \times w_3 + m_\text{milk} \times w_4 = P_\text{pancakes}\]

&lt;p&gt;The above expression describe &lt;em&gt;a&lt;/em&gt; linear equation. A &lt;em&gt;system&lt;/em&gt; of linear equations involve multiple equations that have to be solved &lt;strong&gt;simultaneously&lt;/strong&gt;. Consider:&lt;/p&gt;

\[x + 2y = 8 \\
5x - 3y = 1\]

&lt;p&gt;Now we have a system with two unknowns, $x$ and $y$. We’ll see general methods to solve systems of linear equations later. For now, I’ll give you the answer: $x=2$ and $y=3$. Geometrically, we can see that both equations produce a straight line in the 2-dimensional plane. The point on where both lines encounter is the solution to the linear system.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;y2&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]})&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;equation1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mark_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;equation2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mark_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;red&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;y2&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;equation1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;equation2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div id=&quot;altair-viz-a98fc3f32a7644beb047a3d0ab4bef27&quot;&gt;&lt;/div&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
  (function(spec, embedOpt){
    let outputDiv = document.currentScript.previousElementSibling;
    if (outputDiv.id !== &quot;altair-viz-a98fc3f32a7644beb047a3d0ab4bef27&quot;) {
      outputDiv = document.getElementById(&quot;altair-viz-a98fc3f32a7644beb047a3d0ab4bef27&quot;);
    }
    const paths = {
      &quot;vega&quot;: &quot;https://cdn.jsdelivr.net/npm//vega@5?noext&quot;,
      &quot;vega-lib&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-lib?noext&quot;,
      &quot;vega-lite&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext&quot;,
      &quot;vega-embed&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-embed@6?noext&quot;,
    };

    function loadScript(lib) {
      return new Promise(function(resolve, reject) {
        var s = document.createElement(&apos;script&apos;);
        s.src = paths[lib];
        s.async = true;
        s.onload = () =&gt; resolve(paths[lib]);
        s.onerror = () =&gt; reject(`Error loading script: ${paths[lib]}`);
        document.getElementsByTagName(&quot;head&quot;)[0].appendChild(s);
      });
    }

    function showError(err) {
      outputDiv.innerHTML = `&lt;div class=&quot;error&quot; style=&quot;color:red;&quot;&gt;${err}&lt;/div&gt;`;
      throw err;
    }

    function displayChart(vegaEmbed) {
      vegaEmbed(outputDiv, spec, embedOpt)
        .catch(err =&gt; showError(`Javascript Error: ${err.message}&lt;br&gt;This usually means there&apos;s a typo in your chart specification. See the javascript console for the full traceback.`));
    }

    if(typeof define === &quot;function&quot; &amp;&amp; define.amd) {
      requirejs.config({paths});
      require([&quot;vega-embed&quot;], displayChart, err =&gt; showError(`Error loading script: ${err.message}`));
    } else if (typeof vegaEmbed === &quot;function&quot;) {
      displayChart(vegaEmbed);
    } else {
      loadScript(&quot;vega&quot;)
        .then(() =&gt; loadScript(&quot;vega-lite&quot;))
        .then(() =&gt; loadScript(&quot;vega-embed&quot;))
        .catch(showError)
        .then(() =&gt; displayChart(vegaEmbed));
    }

})({&quot;usermeta&quot;: {&quot;embedOptions&quot;: {&quot;theme&quot;: &quot;dark&quot;}}, &quot;config&quot;: {&quot;view&quot;: {&quot;continuousWidth&quot;: 400, &quot;continuousHeight&quot;: 300}}, &quot;layer&quot;: [{&quot;mark&quot;: &quot;line&quot;, &quot;encoding&quot;: {&quot;x&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;x1&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;y1&quot;}}}, {&quot;mark&quot;: {&quot;type&quot;: &quot;line&quot;, &quot;color&quot;: &quot;red&quot;}, &quot;encoding&quot;: {&quot;x&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;x2&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;y2&quot;}}}], &quot;data&quot;: {&quot;name&quot;: &quot;data-57ffab6a26a928c2ff17e40b29b8a272&quot;}, &quot;$schema&quot;: &quot;https://vega.github.io/schema/vega-lite/v4.8.1.json&quot;, &quot;datasets&quot;: {&quot;data-57ffab6a26a928c2ff17e40b29b8a272&quot;: [{&quot;x1&quot;: 0, &quot;y1&quot;: 8, &quot;x2&quot;: 0.5, &quot;y2&quot;: 0}, {&quot;x1&quot;: 2, &quot;y1&quot;: 3, &quot;x2&quot;: 2.0, &quot;y2&quot;: 3}]}}, {&quot;mode&quot;: &quot;vega-lite&quot;});
&lt;/script&gt;

&lt;h1 id=&quot;matrices&quot;&gt;Matrices&lt;/h1&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Libraries for this section
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;altair&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Matrices are as fundamental as vectors in machine learning. With vectors, we can represent single variables as sets of numbers or instances. With matrices, we can represent sets of variables. In this sense, a matrix is simply an ordered &lt;strong&gt;collection of vectors&lt;/strong&gt;. Conventionally, column vectors, but it’s always wise to pay attention to the authors’ notation when reading matrices. Since computer screens operate in two dimensions, matrices are the way in which we interact with data in practice.&lt;/p&gt;

&lt;p&gt;More formally, we represent a matrix with a italicized upper-case letter like $\textit{A}$. In two dimensions, we say the matrix $\textit{A}$ has $m$ rows and $n$ columns. Each entry of $\textit{A}$ is defined as $a_{ij}$, $i=1,…, m,$ and $j=1,…,n$. A matrix $\textit{A} \in \mathbb{R^{m\times n}}$ is defines as:&lt;/p&gt;

\[A :=
\begin{bmatrix}
a_{11} &amp;amp; a_{12} &amp;amp; \cdots &amp;amp; a_{1n}\\
a_{21} &amp;amp; a_{22} &amp;amp; \cdots &amp;amp; a_{2n}\\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\
a_{m1} &amp;amp; a_{m2} &amp;amp; \cdots &amp;amp; a_{mn}
\end{bmatrix},
a_{ij} \in \mathbb{R}\]

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Numpy&lt;/code&gt;, we construct matrices with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;array&lt;/code&gt; method:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 1st row
&lt;/span&gt;              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 2nd row
&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;a 2x2 Matrix:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;a 2x2 Matrix:
[[0 2]
 [1 4]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;basic-matrix-operations&quot;&gt;Basic Matrix operations&lt;/h2&gt;

&lt;h3 id=&quot;matrix-matrix-addition&quot;&gt;Matrix-matrix addition&lt;/h3&gt;

&lt;p&gt;We add matrices in a element-wise fashion. The sum of $\textit{A} \in \mathbb{R}^{m\times n}$ and $\textit{B} \in \mathbb{R}^{m\times n}$ is defined as:&lt;/p&gt;

\[\textit{A} + \textit{B} :=
\begin{bmatrix}
a_{11} + b_{11} &amp;amp; \cdots &amp;amp; a_{1n} + b_{1n} \\
\vdots &amp;amp; \ddots &amp;amp; \vdots \\
a_{m1} + b_{m1} &amp;amp; \cdots &amp;amp; a_{mn} + b_{mn}
\end{bmatrix}
\in \mathbb{R^{m\times n}}\]

&lt;p&gt;For instance:&lt;/p&gt;

\[\textit{A} =
\begin{bmatrix}
0 &amp;amp; 2 \\
1 &amp;amp; 4
\end{bmatrix} +
\textit{B} =
\begin{bmatrix}
3 &amp;amp; 1 \\
-3 &amp;amp; 2
\end{bmatrix}=
\begin{bmatrix}
0+3 &amp;amp; 2+1 \\
3+(-3) &amp;amp; 2+2
\end{bmatrix}=
\begin{bmatrix}
3 &amp;amp; 3 \\
-2 &amp;amp; 6
\end{bmatrix}\]

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Numpy&lt;/code&gt;, we add matrices with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;+&lt;/code&gt; operator or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt; method:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([[ 3,  3],
       [-2,  6]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([[ 3,  3],
       [-2,  6]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;matrix-scalar-multiplication&quot;&gt;Matrix-scalar multiplication&lt;/h3&gt;

&lt;p&gt;Matrix-scalar multiplication is an element-wise operation. Each element of the matrix $\textit{A}$ is multiplied by the scalar $\alpha$. Is defined as:&lt;/p&gt;

\[a_{ij} \times \alpha, \text{such that } (\alpha \textit{A})_{ij} = \alpha(\textit{A})_{ij}\]

&lt;p&gt;Consider $\alpha=2$ and $\textit{A}=\begin{bmatrix}1 &amp;amp; 2 \ 3 &amp;amp; 4\end{bmatrix}$, then:&lt;/p&gt;

\[\alpha \textit{A} =
2
\begin{bmatrix}
1 &amp;amp; 2 \\
3 &amp;amp; 4
\end{bmatrix}=
\begin{bmatrix}
2\times 1 &amp;amp; 2\times 2 \\
2\times 3 &amp;amp; 2 \times4
\end{bmatrix}=
\begin{bmatrix}
2 &amp;amp; 4 \\
6 &amp;amp; 8
\end{bmatrix}\]

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;, we compute matrix-scalar multiplication with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*&lt;/code&gt; operator or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;multiply&lt;/code&gt; method:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([[2, 4],
       [6, 8]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;multiply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([[2, 4],
       [6, 8]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;matrix-vector-multiplication-dot-product&quot;&gt;Matrix-vector multiplication: dot product&lt;/h3&gt;

&lt;p&gt;Matrix-vector multiplication equals to taking the dot product of each column $n$ of a $\textit{A}$ with each element $\bf{x}$ resulting in a vector $\bf{y}$. Is defined as:&lt;/p&gt;

\[\textit{A}\cdot\bf{x}:=
\begin{bmatrix}
a_{11} &amp;amp; \cdots &amp;amp; a_{1n}\\
\vdots &amp;amp; \ddots &amp;amp; \vdots\\
a_{m1} &amp;amp; \cdots &amp;amp; a_{mn}
\end{bmatrix}
\begin{bmatrix}
x_1\\
\vdots\\
x_n
\end{bmatrix}=
x_1
\begin{bmatrix}
a_{11}\\
\vdots\\
a_{m1}
\end{bmatrix}+
x_2
\begin{bmatrix}
a_{12}\\
\vdots\\
a_{m2}
\end{bmatrix}+
x_n
\begin{bmatrix}
a_{1n}\\
\vdots\\
a_{mn}
\end{bmatrix}=
\begin{bmatrix}
y_1\\
\vdots\\
y_{mn}
\end{bmatrix}\]

&lt;p&gt;For instance:&lt;/p&gt;

\[\textit{A}\cdot\bf{x}=
\begin{bmatrix}
0 &amp;amp; 2\\
1 &amp;amp; 4
\end{bmatrix}
\begin{bmatrix}
1\\
2
\end{bmatrix}=
1
\begin{bmatrix}
0 \\
1
\end{bmatrix}+
2
\begin{bmatrix}
2 \\
4
\end{bmatrix}=
\begin{bmatrix}
1\times0 + 2\times2 \\
1\times1 + 2\times4
\end{bmatrix}=
\begin{bmatrix}
4 \\
9
\end{bmatrix}\]

&lt;p&gt;In numpy, we compute the matrix-vector product with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;@&lt;/code&gt; operator or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dot&lt;/code&gt; method:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([[4],
       [9]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([[4],
       [9]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;matrix-matrix-multiplication&quot;&gt;Matrix-matrix multiplication&lt;/h3&gt;

&lt;p&gt;Matrix-matrix multiplication is a dot produt as well. To work, the number of columns in the first matrix $\textit{A}$ has to be equal to the number of rows in the second matrix $\textit{B}$. Hence, $\textit{A} \in \mathbb{R^{m\times n}}$ times $\textit{B} \in \mathbb{R^{n\times p}}$ to be valid. One way to see matrix-matrix multiplication is by taking a series of dot products: the 1st column of $\textit{A}$ times the 1st row of $\textit{B}$, the 2nd column of $\textit{A}$ times the 2nd row of $\textit{B}$, until the $n_{th}$ column of $\textit{A}$ times the $n_{th}$ row of $\textit{B}$.&lt;/p&gt;

&lt;p&gt;We define $\textit{A} \in \mathbb{R^{n\times p}} \cdot \textit{B} \in \mathbb{R^{n\times p}} = \textit{C} \in \mathbb{R^{m\times p}}$:&lt;/p&gt;

\[\textit{A}\cdot\textit{B}:=
\begin{bmatrix}
a_{11} &amp;amp; \cdots &amp;amp; a_{1n}\\
\vdots &amp;amp; \ddots &amp;amp; \vdots\\
a_{m1} &amp;amp; \cdots &amp;amp; a_{mn}
\end{bmatrix}
\begin{bmatrix}
b_{11} &amp;amp; \cdots &amp;amp; b_{1p}\\
\vdots &amp;amp; \ddots &amp;amp; \vdots\\
b_{n1} &amp;amp; \cdots &amp;amp; b_{np}
\end{bmatrix}=
\begin{bmatrix}
c_{11} &amp;amp; \cdots &amp;amp; c_{1p}\\
\vdots &amp;amp; \ddots &amp;amp; \vdots\\
c_{m1} &amp;amp; \cdots &amp;amp; c_{mp}
\end{bmatrix}\]

&lt;p&gt;A compact way to define the matrix-matrix product is:&lt;/p&gt;

\[c_{ij} := \sum_{l=1}^n a_{il}b_{lj}, \text{  with   }i=1,...m, \text{ and}, j=1,...,p\]

&lt;p&gt;For instance&lt;/p&gt;

\[\textit{A}\cdot\textit{B}=
\begin{bmatrix}
0 &amp;amp; 2\\
1 &amp;amp; 4
\end{bmatrix}
\begin{bmatrix}
1 &amp;amp; 3\\
2 &amp;amp; 1
\end{bmatrix}=
\begin{bmatrix}
1\times0 + 2\times2 &amp;amp; 3\times0 + 1\times2 \\
1\times1 + 2\times4 &amp;amp; 3\times1 + 1\times4
\end{bmatrix}=
\begin{bmatrix}
4 &amp;amp; 2\\
9 &amp;amp; 7
\end{bmatrix}\]

&lt;p&gt;Matrix-matrix multiplication has a series of important properties:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Associativity: $(\textit{A}\textit{B}) \textit{C} = \textit{A}(\textit{B}\textit{C})$&lt;/li&gt;
  &lt;li&gt;Associativity with scalar multiplication: $\alpha (\textit{A}\textit{B}) = (\alpha \textit{A}) \textit{B}$&lt;/li&gt;
  &lt;li&gt;Distributivity with addition: $\textit{A}(\textit{B}+\textit{C}) = A+B + AC$&lt;/li&gt;
  &lt;li&gt;Transpose of product: $(\textit{A}\textit{B})^T = \textit{B}^T\textit{A}^T$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It’s also important to remember that &lt;strong&gt;matrix-matrix multiplication orders matter&lt;/strong&gt;, this is, it is &lt;strong&gt;not commutative&lt;/strong&gt;. Hence, in general, $AB \ne BA$.&lt;/p&gt;

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;, we obtan the matrix-matrix product with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;@&lt;/code&gt; operator or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dot&lt;/code&gt; method:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([[4, 2],
       [9, 7]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([[4, 2],
       [9, 7]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;matrix-identity&quot;&gt;Matrix identity&lt;/h3&gt;

&lt;p&gt;An identity matrix is a square matrix with ones on the diagonal from the upper left to the bottom right, and zeros everywhere else. We denote the identity matrix as $\textit{I}_n$. We define $\textit{I} \in \mathbb{R}^{n \times n}$ as:&lt;/p&gt;

\[\textit{I}_n :=
\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 1 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; \ddots &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 1 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; 1
\end{bmatrix}
\in \mathbb{R}^{n \times n}\]

&lt;p&gt;For example:&lt;/p&gt;

\[\textit{I}_3 =
\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 1 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}\]

&lt;p&gt;You can think in the inverse as playing the same role than $1$ in operations with real numbers. The inverse matrix does not look very interesting in itself, but it plays an important role in some proofs and for the inverse matrix (which can be used to solve system of linear equations).&lt;/p&gt;

&lt;h3 id=&quot;matrix-inverse&quot;&gt;Matrix inverse&lt;/h3&gt;

&lt;p&gt;In the context of real numbers, the &lt;em&gt;multiplicative inverse (or reciprocal)&lt;/em&gt; of a number $x$, is the number that when multiplied by $x$ yields $1$. We denote this by $x^{-1}$ or $\frac{1}{x}$. Take the number $5$. Its multiplicative inverse equals to $5 \times \frac{1}{5} = 1$.&lt;/p&gt;

&lt;p&gt;If you recall the matrix identity section, we said that the identity plays a similar role than the number one but for matrices. Again, by analogy, we can see the &lt;em&gt;inverse&lt;/em&gt; of a matrix as playing the same role than the multiplicative inverse for numbers but for matrices. Hence, the &lt;em&gt;inverse matrix&lt;/em&gt; is a matrix than when multiplies another matrix &lt;em&gt;from either the right or the left side&lt;/em&gt;, returns the identity matrix.&lt;/p&gt;

&lt;p&gt;More formally, consider the square matrix $\textit{A} \in \mathbb{R}^{n \times n}$. We define $\textit{A}^{-1}$ as matrix with the property:&lt;/p&gt;

\[\textit{A}^{-1}\textit{A} = \textit{I}_n = \textit{A}\textit{A}^{-1}\]

&lt;p&gt;The main reason we care about the inverse, is because it allows to &lt;strong&gt;solve systems of linear equations&lt;/strong&gt; in certain situations. Consider a system of linear equations as:&lt;/p&gt;

\[\textit{A}\bf{x} = \bf{y}\]

&lt;p&gt;Assuming $\textit{A}$ has an inverse, we can multiply by the inverse on both sides:&lt;/p&gt;

\[\textit{A}^{-1}\textit{A}\bf{x} = \textit{A}^{-1}\bf{y}\]

&lt;p&gt;And get:&lt;/p&gt;

\[\textit{I}\bf{x} = \textit{A}^{-1}\bf{y}\]

&lt;p&gt;Since the $\textit{I}$ does not affect $\bf{x}$ at all, our final expression becomes:&lt;/p&gt;

\[\bf{x} = \textit{A}^{-1}\bf{y}\]

&lt;p&gt;This means that we just need to know the inverse of $\textit{A}$, multiply by the target vector $\bf{y}$, and we obtain the solution for our system. I mentioned that this works only in &lt;em&gt;certain situations&lt;/em&gt;. By this I meant: &lt;strong&gt;if and only if $\textit{A}$ happens to have an inverse&lt;/strong&gt;. Not all matrices have an inverse. When $\textit{A}^{-1}$ exist, we say $\textit{A}$ is &lt;em&gt;nonsingular&lt;/em&gt; or &lt;em&gt;invertible&lt;/em&gt;, otherwise, we say it is &lt;em&gt;noninvertible&lt;/em&gt; or &lt;em&gt;singular&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The lingering question is how to find the inverse of a matrix. We can do it by reducing $\textit{A}$ to its &lt;em&gt;reduced row echelon form&lt;/em&gt; by using Gauss-Jordan Elimination. If $\textit{A}$ has an inverse, we will obtain the identity matrix as the row echelon form of $\textit{A}$. I haven’t introduced either just yet. You can jump to the &lt;em&gt;Solving systems of linear equations with matrices&lt;/em&gt; if you are eager to learn about it now. For now, we relie on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;, we can compute the inverse of a matrix with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.linalg.inv&lt;/code&gt; method:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A_i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;A inverse:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A_i&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;A inverse:
[[-7. -7.  6.]
 [ 2.  1. -1.]
 [ 4.  5. -4.]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can check the $\textit{A}^{-1}$ is correct by multiplying. If so, we should obtain the identity $\textit{I}_3$&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;I&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A_i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;A_i times A resulsts in I_3:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;I&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;A_i times A resulsts in I_3:
[[ 1.  0.  0.]
 [ 0.  1. -0.]
 [ 0. -0.  1.]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;matrix-transpose&quot;&gt;Matrix transpose&lt;/h3&gt;

&lt;p&gt;Consider a matrix $\textit{A} \in \mathbb{R}^{m \times n}$. The &lt;strong&gt;transpose&lt;/strong&gt; of $\textit{A}$ is denoted as $\textit{A}^T \in \mathbb{R}^{m \times n}$. We obtain $\textit{A}^T$ as:&lt;/p&gt;

\[(\textit{A}^T)_{ij} = \textit{A}_ji\]

&lt;p&gt;In other words, we get the $\textit{A}^T$ by switching the columns by the rows of $\textit{A}$. For instance:&lt;/p&gt;

\[\begin{bmatrix}
1 &amp;amp; 2 \\
3 &amp;amp; 4 \\
5 &amp;amp; 6
\end{bmatrix}^T
=
\begin{bmatrix}
1 &amp;amp; 3 &amp;amp; 5 \\
2 &amp;amp; 4 &amp;amp; 6
\end{bmatrix}\]

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;, we obtain the transpose with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;T&lt;/code&gt; method:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([[1, 3, 5],
       [2, 4, 6]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;hadamard-product&quot;&gt;Hadamard product&lt;/h3&gt;

&lt;p&gt;It is tempting to think in matrix-matrix multiplication as an element-wise operation, as multiplying each overlapping element of $\textit{A}$ and $\textit{B}$. &lt;em&gt;It is not&lt;/em&gt;. Such operation is called &lt;strong&gt;Hadamard product&lt;/strong&gt;. I’m introducing this to avoid confusion. The Hadamard product is defined as&lt;/p&gt;

\[a_{ij} \cdot b_{ij} := c_{ij}\]

&lt;p&gt;For instance:&lt;/p&gt;

\[\textit{A}\odot\textit{B}=
\begin{bmatrix}
0 &amp;amp; 2\\
1 &amp;amp; 4
\end{bmatrix}
\begin{bmatrix}
1 &amp;amp; 3\\
2 &amp;amp; 1
\end{bmatrix}=
\begin{bmatrix}
0\times1 &amp;amp; 2\times3\\
1\times2 &amp;amp; 4\times 1\\
\end{bmatrix}=
\begin{bmatrix}
0 &amp;amp; 6\\
2 &amp;amp; 4\\
\end{bmatrix}\]

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;numpy&lt;/code&gt;, we compute the Hadamard product with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*&lt;/code&gt; operator or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;multiply&lt;/code&gt; method:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([[0, 6],
       [2, 4]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;multiply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([[0, 6],
       [2, 4]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;special-matrices&quot;&gt;Special matrices&lt;/h2&gt;

&lt;p&gt;There are several matrices with special names that are commonly found in machine learning theory and applications. Knowing these matrices beforehand can improve your linear algebra fluency, so we will briefly review a selection of 12 common matrices. For an extended list of special matrices see &lt;a href=&quot;https://en.wikipedia.org/wiki/List_of_named_matrices&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;http://www.ee.ic.ac.uk/hp/staff/dmb/matrix/special.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;rectangular-matrix&quot;&gt;Rectangular matrix&lt;/h3&gt;

&lt;p&gt;Matrices are said to be &lt;em&gt;rectangular&lt;/em&gt; when the number of rows is $\ne$ to the number of columns, i.e., $\textit{A}^{m \times n}$ with $m \ne n$. For instance:&lt;/p&gt;

\[\begin{bmatrix}
1 &amp;amp; 2 &amp;amp; 3 \\
4 &amp;amp; 5 &amp;amp; 6
\end{bmatrix}\]

&lt;h3 id=&quot;square-matrix&quot;&gt;Square matrix&lt;/h3&gt;

&lt;p&gt;Matrices are said to be &lt;strong&gt;square&lt;/strong&gt; when the number of rows $=$ the number of columns, i.e., $\textit{A}^{n \times n}$. For instance:&lt;/p&gt;

\[\begin{bmatrix}
1 &amp;amp; 2 &amp;amp; 3 \\
4 &amp;amp; 5 &amp;amp; 6 \\
7 &amp;amp; 8 &amp;amp; 9
\end{bmatrix}\]

&lt;h3 id=&quot;diagonal-matrix&quot;&gt;Diagonal matrix&lt;/h3&gt;

&lt;p&gt;Square matrices are said to be &lt;strong&gt;diagonal&lt;/strong&gt; when each of its non-diagonal elements is zero, i.e., For $\textit{D} = (d_{i,j})$, we have $\forall i,j \in n, i \ne j \implies d_{i,j} = 0$. For instance:&lt;/p&gt;

\[\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 5 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 9
\end{bmatrix}\]

&lt;h3 id=&quot;upper-triangular-matrix&quot;&gt;Upper triangular matrix&lt;/h3&gt;

&lt;p&gt;Square matrices are said to be &lt;strong&gt;upper triangular&lt;/strong&gt; when the elements below the main diagonal are zero, i.e., For $\textit{D} = (d_{i,j})$, we have $d_{i,j} = 0, \text{for } i&amp;gt;j$. For instance:&lt;/p&gt;

\[\begin{bmatrix}
1 &amp;amp; 2 &amp;amp; 3 \\
0 &amp;amp; 5 &amp;amp; 6 \\
0 &amp;amp; 0 &amp;amp; 9
\end{bmatrix}\]

&lt;h3 id=&quot;lower-triangular-matrix&quot;&gt;Lower triangular matrix&lt;/h3&gt;

&lt;p&gt;Square matrices are said to be &lt;strong&gt;lower triangular&lt;/strong&gt; when the elements above the main diagonal are zero, i.e., For $\textit{D} = (d_{i,j})$, we have $d_{i,j} = 0, \text{for } i&amp;lt;j$. For instance:&lt;/p&gt;

\[\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 \\
4 &amp;amp; 5 &amp;amp; 0 \\
7 &amp;amp; 8 &amp;amp; 9
\end{bmatrix}\]

&lt;h3 id=&quot;symmetric-matrix&quot;&gt;Symmetric matrix&lt;/h3&gt;

&lt;p&gt;Square matrices are said to be symmetric its equal to its transpose, i.e., $\textit{A} = \textit{A}^T$. For instance:&lt;/p&gt;

\[\begin{bmatrix}
1 &amp;amp; 2 &amp;amp; 3 \\
2 &amp;amp; 1 &amp;amp; 6 \\
3 &amp;amp; 6 &amp;amp; 1
\end{bmatrix}\]

&lt;h3 id=&quot;identity-matrix&quot;&gt;Identity matrix&lt;/h3&gt;

&lt;p&gt;A diagonal matrix is said to be the identity when the elements along its main diagonal are equal to one. For instance:&lt;/p&gt;

\[\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 1 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}\]

&lt;h3 id=&quot;scalar-matrix&quot;&gt;Scalar matrix&lt;/h3&gt;

&lt;p&gt;Diagonal matrices are said to be scalar when all the elements along its main diaonal are equal, i.e., $\textit{D} = \alpha\textit{I}$. For instance:&lt;/p&gt;

\[\begin{bmatrix}
2 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 2 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 2
\end{bmatrix}\]

&lt;h3 id=&quot;null-or-zero-matrix&quot;&gt;Null or zero matrix&lt;/h3&gt;

&lt;p&gt;Matrices are said to be null or zero matrices when all its elements equal to zero, wich is denoted as $0_{m \times n}$. For instance:&lt;/p&gt;

\[\begin{bmatrix}
0 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 0
\end{bmatrix}\]

&lt;h3 id=&quot;echelon-matrix&quot;&gt;Echelon matrix&lt;/h3&gt;

&lt;p&gt;Matrices are said to be on &lt;strong&gt;echelon form&lt;/strong&gt; when it has undergone the process of Gaussian elimination. More specifically:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Zero rows are at the bottom of the matrix&lt;/li&gt;
  &lt;li&gt;The leading entry (pivot) of each nonzero row is to the right of the leading entry of the row above it&lt;/li&gt;
  &lt;li&gt;Each leading entry is the only nonzero entry in its column&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For instance:&lt;/p&gt;

\[\begin{bmatrix}
1 &amp;amp; 3 &amp;amp; 5 \\
2 &amp;amp; 2 &amp;amp; -1 \\
1 &amp;amp; 3 &amp;amp; 2
\end{bmatrix}\]

&lt;p&gt;In echelon form after Gaussian Elimination becomes:&lt;/p&gt;

\[\begin{bmatrix}
1 &amp;amp; 3 &amp;amp; 5 \\
0 &amp;amp; -4 &amp;amp; -11 \\
0 &amp;amp; 0 &amp;amp; -3
\end{bmatrix}\]

&lt;h3 id=&quot;antidiagonal-matrix&quot;&gt;Antidiagonal matrix&lt;/h3&gt;

&lt;p&gt;Matrices are said to be &lt;strong&gt;antidiagonal&lt;/strong&gt; when all the entries are zero but the antidiagonal (i.e., the diagonal starting from the bottom left corner to the upper right corner). For instance:&lt;/p&gt;

\[\begin{bmatrix}
0 &amp;amp; 0 &amp;amp; 3 \\
0 &amp;amp; 5 &amp;amp; 0 \\
7 &amp;amp; 0 &amp;amp; 0
\end{bmatrix}\]

&lt;h3 id=&quot;design-matrix&quot;&gt;Design matrix&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Design matrix&lt;/strong&gt; is a special name for matrices containing explanatory variables or features in the context of statistics and machine learning. Some authors favor this name to refer to the set of variables or features in a model.&lt;/p&gt;

&lt;h2 id=&quot;matrices-as-systems-of-linear-equations&quot;&gt;Matrices as systems of linear equations&lt;/h2&gt;

&lt;p&gt;I introduced the idea of systems of linear equations as a way to figure out the right combination of linear segments to obtain an outcome. I did this in the context of vectors, now we can extend this to the context of matrices.&lt;/p&gt;

&lt;p&gt;Matrices are ideal to represent systems of linear equations. Consider the matrix $\textit{M}$ and vectors $w$ and $y$ in $\in \mathbb{R}^3$. We can set up a system of linear equations as $\textit{M}w = y$ as:&lt;/p&gt;

\[\begin{bmatrix}
m_{11} &amp;amp; m_{12} &amp;amp; m_{13} \\
m_{21} &amp;amp; m_{22} &amp;amp; m_{23} \\
m_{31} &amp;amp; m_{32} &amp;amp; m_{33} \\
\end{bmatrix}
\begin{bmatrix}
w_{1} \\
w_{2} \\
w_{3}
\end{bmatrix}=
\begin{bmatrix}
y_{1} \\
y_{2} \\
y_{3}
\end{bmatrix}\]

&lt;p&gt;This is equivalent to:&lt;/p&gt;

\[\begin{matrix}
m_{11}w_{1} + m_{12}w_{2} + m_{13}w_{3} =y_{1} \\
m_{21}w_{1} + m_{22}w_{2} + m_{23}w_{3} =y_{2} \\
m_{31}w_{1} + m_{32}w_{2} + m_{33}w_{3} =y_{3}
\end{matrix}\]

&lt;p&gt;Geometrically, the solution for this representation equals to plot a &lt;strong&gt;set of planes in 3-dimensional space&lt;/strong&gt;, one for each equation, and to find the segment where the planes intersect.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fig. 12: Visualiation system of equations as planes&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-10/b-planes-intersection.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;An alternative way, which I personally prefer to use, is to represent the system as a &lt;strong&gt;linear combination of the column vectors times a scaling term&lt;/strong&gt;:&lt;/p&gt;

\[w_1
\begin{bmatrix}
m_{11}\\
m_{21}\\
m_{31}
\end{bmatrix}+
w_2
\begin{bmatrix}
m_{12}\\
m_{22}\\
m_{32}
\end{bmatrix}+
w_3
\begin{bmatrix}
m_{13}\\
m_{23}\\
m_{33}
\end{bmatrix}=
\begin{bmatrix}
y_{1} \\
y_{2} \\
y_{3}
\end{bmatrix}\]

&lt;p&gt;Geometrically, the solution for this representation equals to plot a set of &lt;strong&gt;vectors in 3-dimensional&lt;/strong&gt; space, one for each column vector, then scale them by $w_i$ and add them up, tip to tail, to find the resulting vector $y$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fig. 13: System of equations as linear combination of vectors&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-10/b-vectors-combination.svg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-four-fundamental-matrix-subsapces&quot;&gt;The four fundamental matrix subsapces&lt;/h2&gt;

&lt;p&gt;Let’s recall the definition of a subspace in the context of vectors:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Contains the zero vector, $\bf{0} \in S$&lt;/li&gt;
  &lt;li&gt;Closure under multiplication, $\forall \alpha \in \mathbb{R} \rightarrow  \alpha \times s_i \in S$&lt;/li&gt;
  &lt;li&gt;Closure under addition, $\forall s_i \in S \rightarrow  s_1 + s_2 \in S$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These conditions carry on to matrices since matrices are simply collections of vectors. Thus, now we can ask what are all possible subspaces that can be “covered” by a collection of vectors in a matrix. Turns out, there are four fundamental subspaces that can be “covered” by a matrix of valid vectors: (1) the column space, (2) the row space, (3) the null space, and (4) the left null space or null space of the transpose.&lt;/p&gt;

&lt;p&gt;These subspaces are considered fundamental because they express many important properties of matrices in linear algebra.&lt;/p&gt;

&lt;h3 id=&quot;the-column-space&quot;&gt;The column space&lt;/h3&gt;

&lt;p&gt;The column space of a matrix $\textit{A}$ is composed by &lt;strong&gt;all linear combinations of the columns of $\textit{A}$&lt;/strong&gt;. We denote the column space as $C(\textit{A})$. In other words, $C(\textit{A})$ equals to the &lt;strong&gt;span of the columns of $\textit{A}$&lt;/strong&gt;. This view of a matrix is what we represented in &lt;strong&gt;Fig. 12&lt;/strong&gt;: vectors in $\mathbb{R}^n$ scaled by real numbers.&lt;/p&gt;

&lt;p&gt;For a matrix $\textit{A} \in \mathbb{R}^{m\times n}$ and a vector $\bf{v} \in \mathbb{R}^m$, the column space is defined as:&lt;/p&gt;

\[C(\textit{A}) := \{ \bf{w} \in \mathbb{R}^n \vert \bf{w} = \textit{A}\bf{v} \text{ for some } \bf{v}\in \mathbb{R}^m \}\]

&lt;p&gt;In words: all linear combinations of the column vectors of $\textit{A}$ and entries of an $n$ dimensional vector $\bf{v}$.&lt;/p&gt;

&lt;h3 id=&quot;the-row-space&quot;&gt;The row space&lt;/h3&gt;

&lt;p&gt;The row space of a matrix $\textit{A}$ is composed of all linear combinations of the rows of a matrix. We denote
the row space as $R(\textit{A})$. In other words, $R(\textit{A})$ equals to the &lt;strong&gt;span of the rows&lt;/strong&gt; of $\textit{A}$. Geometrically, this is the way we represented a matrix in &lt;strong&gt;Fig. 11&lt;/strong&gt;: each row equation represented as planes. Now, a different way to see the row space, is by transposing $\textit{A}^T$. Now, we can define the row space simply as $R(\textit{A}^T)$&lt;/p&gt;

&lt;p&gt;For a matrix $\textit{A} \in \mathbb{R}^{m\times n}$ and a vector $\bf{w} \in \mathbb{R}^m$, the row space is defined as:&lt;/p&gt;

\[R(\textit{A}) := \{ \bf{v} \in \mathbb{R}^m \vert \bf{v} = \textit{A}\bf{w}^T \text{ for some } \bf{w}\in \mathbb{R}^n \}\]

&lt;p&gt;In words: all linear combinations of the row vectors of $\textit{A}$ and entries of an $m$ dimensional vector $\bf{w}$.&lt;/p&gt;

&lt;h3 id=&quot;the-null-space&quot;&gt;The null space&lt;/h3&gt;

&lt;p&gt;The null space of a matrix $\textit{A}$ is composed of all vectors that are map into the zero vector when multiplied by $\textit{A}$. We denote the null space as $N(\textit{A})$.&lt;/p&gt;

&lt;p&gt;For a matrix $\textit{A} \in \mathbb{R}^{m\times n}$ and a vector $\bf{v} \in \mathbb{R}^n$, the null space is defined as:&lt;/p&gt;

\[N(\textit{A}) := \{ \bf{v} \in \mathbb{R}^m \vert \textit{A}\bf{v} = 0\}\]

&lt;h3 id=&quot;the-null-space-of-the-transpose&quot;&gt;The null space of the transpose&lt;/h3&gt;

&lt;p&gt;The left null space of a matrix $\textit{A}$ is composed of all vectors that are map into the zero vector when multiplied by $\textit{A}$ from the left. By “from the left”, the vectors on the left of $\textit{A}$. We denote the left null space as $N(\textit{A}^T)$&lt;/p&gt;

&lt;p&gt;For a matrix $\textit{A} \in \mathbb{R}^{m\times n}$ and a vector $\bf{w} \in \mathbb{R}^m$, the null space is defined as:&lt;/p&gt;

\[N(\textit{A}^T) := \{ \bf{w} \in \mathbb{R}^n \vert \bf{v^T} \textit{A} = 0^T\}\]

&lt;h2 id=&quot;solving-systems-of-linear-equations-with-matrices&quot;&gt;Solving systems of linear equations with Matrices&lt;/h2&gt;

&lt;h3 id=&quot;gaussian-elimination&quot;&gt;Gaussian Elimination&lt;/h3&gt;

&lt;p&gt;When I was in high school, I learned to solve systems of two or three equations by the methods of elimination and substitution. Nevertheless, as systems of equations get larger and more complicated, such inspection-based methods become impractical. By inspection-based, I mean “just by looking at the equations and using common sense”. Thus, to approach such kind of systems we can use the method of &lt;strong&gt;Gaussian Elimination&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Gaussian Elimination&lt;/strong&gt;, is a robust algorithm to solve linear systems. We say is robust, because it works in general, it all possible circumstances. It works by &lt;em&gt;eliminating&lt;/em&gt; terms from a system of equations, such that it is simplified to the point where we obtain the &lt;strong&gt;row echelon form&lt;/strong&gt; of the matrix. A matrix is in row echelon form when all rows contain zeros at the bottom left of the matrix. For instance:&lt;/p&gt;

\[\begin{bmatrix}
p_1 &amp;amp; a &amp;amp; b \\
0 &amp;amp; p_2 &amp;amp; c \\
0 &amp;amp; 0 &amp;amp; p_3
\end{bmatrix}\]

&lt;p&gt;The $p$ values along the diagonal are the &lt;strong&gt;pivots&lt;/strong&gt; also known as basic variables of the matrix. An important remark about the pivots, is that they indicate which vectors are linearly independent in the matrix, once the matrix has been reduced to the row echelon form.&lt;/p&gt;

&lt;p&gt;There are three &lt;em&gt;elementary transformations&lt;/em&gt; in Gaussian Elimination that when combined, allow simplifying any system to its row echelon form:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Addition and subtraction of two equations (rows)&lt;/li&gt;
  &lt;li&gt;Multiplication of an equation (rows) by a number&lt;/li&gt;
  &lt;li&gt;Switching equations (rows)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Consider the following system $\textit{A} \bf{w} = \bf{y}$:&lt;/p&gt;

\[\begin{bmatrix}
1 &amp;amp; 3 &amp;amp; 5 \\
2 &amp;amp; 2 &amp;amp; -1 \\
1 &amp;amp; 3 &amp;amp; 2
\end{bmatrix}
\begin{bmatrix}
w_{1} \\
w_{2} \\
w_{3}
\end{bmatrix}=
\begin{bmatrix}
-1 \\
1 \\
2
\end{bmatrix}\]

&lt;p&gt;We want to know what combination of columns of $\textit{A}$ will generate the target vector $\bf{y}$. Alternatively, we can see this as a decomposition problem, as how can we decompose $\bf{y}$ into columns of $\textit{A}$. To aid the application of Gaussian Elimination, we can generate an &lt;strong&gt;augmented matrix&lt;/strong&gt; $(\textit{A} \vert \bf{y})$, this is, appending $\bf{y}$ to $\textit{A}$ on this manner:&lt;/p&gt;

\[\left[
\begin{matrix}
1 &amp;amp; 3 &amp;amp; 5 \\
2 &amp;amp; 2 &amp;amp; -1 \\
1 &amp;amp; 3 &amp;amp; 2
\end{matrix}
  \left|
    \,
\begin{matrix}
-1 \\
1 \\
2
\end{matrix}
  \right.
\right]\]

&lt;p&gt;We start by multiplying row 1 by and substracting it from row 2 as $R_2 - 2R_1$ to obtain:&lt;/p&gt;

\[\left[
\begin{matrix}
1 &amp;amp; 3 &amp;amp; 5 \\
0 &amp;amp; -4 &amp;amp; -11 \\
1 &amp;amp; 3 &amp;amp; 2
\end{matrix}
  \left|
    \,
\begin{matrix}
-1 \\
3 \\
2
\end{matrix}
  \right.
\right]\]

&lt;p&gt;If we substract row 1 from row 3 as $R_3 - R_1$ we get:&lt;/p&gt;

\[\left[
\begin{matrix}
1 &amp;amp; 3 &amp;amp; 5 \\
0 &amp;amp; -4 &amp;amp; -11 \\
0 &amp;amp; 0 &amp;amp; -3
\end{matrix}
  \left|
    \,
\begin{matrix}
-1 \\
3 \\
3
\end{matrix}
  \right.
\right]\]

&lt;p&gt;At this point, we have found the row echelon form of $\textit{A}$. If we divide row 3 by -3, We know that $w_3 = -1$. By &lt;strong&gt;backsubsitution&lt;/strong&gt;, we can solve for $w_2$ as:&lt;/p&gt;

\[\begin{matrix}
-4w_2 + -11(-1) = 3 \\
-4w_2 = -8 \\
w_2 = 2
\end{matrix}\]

&lt;p&gt;Again, taking $w_2=2$ and $w_3=-1$ we can solve for $w_1$ as:&lt;/p&gt;

\[w_1 + 3(2) + 5(-1) = -1 \\
w_1 + 6 - 5 = -1 \\
w_1 = -2\]

&lt;p&gt;In this manner, we have found that the solution for our system is $w_1 = -2$, $w_2=2$, and $w_3 = -1$.&lt;/p&gt;

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;, we can solve a system of equations with Gaussian Elimination with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;linalg.solve&lt;/code&gt; method as:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;solve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([[-2.],
       [ 2.],
       [-1.]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Which confirms our solution is correct.&lt;/p&gt;

&lt;h3 id=&quot;gauss-jordan-elimination&quot;&gt;Gauss-Jordan Elimination&lt;/h3&gt;

&lt;p&gt;The only difference between &lt;strong&gt;Gaussian Elimination&lt;/strong&gt; and &lt;strong&gt;Gauss-Jordan Elimination&lt;/strong&gt;, is that this time we “keep going” with the elemental row operations until we obtain the &lt;strong&gt;reduced row echelon form&lt;/strong&gt;. The &lt;em&gt;reduced&lt;/em&gt; part means two additionak things: (1) the pivots must be $1$, (2) and the entries above the pivots must be $0$. This is simplest form a system of linear equations can take. For instance, for a 3x3 matrix:&lt;/p&gt;

\[\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 1 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}\]

&lt;p&gt;Let’s retake from where we left Gaussian elimination in the above section. If we divide row 3 by -3 and row 2 by -4 as $\frac{R_3}{-3}$ and $\frac{R_2}{-4}$, we get:&lt;/p&gt;

\[\left[
\begin{matrix}
1 &amp;amp; 3 &amp;amp; 5 \\
0 &amp;amp; 1 &amp;amp; 2.75 \\
0 &amp;amp; 0 &amp;amp; 1
\end{matrix}
  \left|
    \,
\begin{matrix}
-1 \\
-0.75 \\
-1
\end{matrix}
  \right.
\right]\]

&lt;p&gt;Again, by this point we we know $w_3 = -1$. If we multiply row 2 by 3 and substract from row 1 as $R_1 - 3R_2$:&lt;/p&gt;

\[\left[
\begin{matrix}
1 &amp;amp; 0 &amp;amp; -3.25 \\
0 &amp;amp; 1 &amp;amp; 2.75 \\
0 &amp;amp; 0 &amp;amp; 1
\end{matrix}
  \left|
    \,
\begin{matrix}
1.25 \\
-0.75 \\
-1
\end{matrix}
  \right.
\right]\]

&lt;p&gt;Finally, we can add 3.25 times row 3 to row 1, and substract 2.75 times row 3 to row 2, as $R_1 + 3.25R_3$ and $R_2 - 2.75R_3$ to get the &lt;strong&gt;reduced row echelon form&lt;/strong&gt; as:&lt;/p&gt;

\[\left[
\begin{matrix}
1 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 1 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 1
\end{matrix}
  \left|
    \,
\begin{matrix}
-2 \\
2 \\
-1
\end{matrix}
  \right.
\right]\]

&lt;p&gt;Now, by simply following the rules of matrix-vector multiplication, we get =&lt;/p&gt;

\[w_1
\begin{bmatrix}
1\\
0\\
0
\end{bmatrix}+
w_2
\begin{bmatrix}
0\\
1\\
0
\end{bmatrix}+
w_3
\begin{bmatrix}
0\\
0\\
1
\end{bmatrix}=
\begin{bmatrix}
w_1\\
w_2\\
w_3
\end{bmatrix}=
\begin{bmatrix}
-2 \\
2 \\
-1
\end{bmatrix}\]

&lt;p&gt;There you go, we obtained that $w_1 = -2$, $w_2 = 2$, and $w_3 = -1$.&lt;/p&gt;

&lt;h2 id=&quot;matrix-basis-and-rank&quot;&gt;Matrix basis and rank&lt;/h2&gt;

&lt;p&gt;A set of $n$ linearly independent column vectors with $n$ elements forms a &lt;strong&gt;basis&lt;/strong&gt;. For instance, the column vectors of $\textit{A}$ are a basis:&lt;/p&gt;

\[\textit{A}=
\begin{bmatrix}
1 &amp;amp; 0 \\
0 &amp;amp; 1
\end{bmatrix}\]

&lt;p&gt;“A basis for what?” You may be wondering. In the case of $\textit{A}$, for any vector $\bf{y} \in \mathbb{R}^2$. On the contrary, the column vectors for $\textit{B}$ &lt;em&gt;do not&lt;/em&gt; form a basis for $\mathbb{R}^2$:&lt;/p&gt;

\[\textit{B}=
\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 1\\
0 &amp;amp; 1 &amp;amp; 1
\end{bmatrix}\]

&lt;p&gt;In the case of $\textit{B}$, the third column vector is a linear combination of first and second column vectors.&lt;/p&gt;

&lt;p&gt;The definition of a &lt;em&gt;basis&lt;/em&gt; depends on the &lt;strong&gt;independence-dimension inequality&lt;/strong&gt;, which states that a &lt;em&gt;linearly independent set of $n$ vectors can have at most $n$ elements&lt;/em&gt;. Alternatively, we say that any set of $n$ vectors with $n+1$ elements is, &lt;em&gt;necessarily&lt;/em&gt;, linearly dependent. Given that each vector in a &lt;em&gt;basis&lt;/em&gt; is linearly independent, we say that any vector $\bf{y}$ with $n$ elements, can be generated in a unique linear combination of the &lt;em&gt;basis&lt;/em&gt; vectors. Hence, any matrix more columns than rows (as in $\textit{B}$) will have dependent vectors. &lt;em&gt;Basis&lt;/em&gt; are sometimes referred to as the &lt;em&gt;minimal generating set&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;An important question is how to find the &lt;em&gt;basis&lt;/em&gt; for a matrix. Another way to put the same question is to found out which vectors are linearly independent of each other. Hence, we need to solve:&lt;/p&gt;

\[\sum_{i=1}^k \beta_i a_i = 0\]

&lt;p&gt;Where $a_i$ are the column vectors of $\textit{A}$. We can approach this by using &lt;strong&gt;Gaussian Elimination&lt;/strong&gt; or &lt;strong&gt;Gauss-Jordan Elimination&lt;/strong&gt; and reducing $\textit{A}$ to its &lt;strong&gt;row echelon form&lt;/strong&gt; or &lt;strong&gt;reduced row echelon form&lt;/strong&gt;. In either case, recall that the &lt;em&gt;pivots&lt;/em&gt; of the echelon form indicate the set of linearly independent vectors in a matrix.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; does not have a method to obtain the row echelon form of a matrix. But, we can use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Sympy&lt;/code&gt;, a Python library for symbolic mathematics that counts with a module for Matrices operations.&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SymPy&lt;/code&gt; has a method to obtain the reduced row echelon form and the pivots, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rref&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sympy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Matrix&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A_rref&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A_pivots&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;rref&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Reduced row echelon form of A:&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Reduced row echelon form of A:
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

\[\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 1\\
0 &amp;amp; 1 &amp;amp; 1
\end{bmatrix}\]

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Column pivots of A: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A_pivots&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Column pivots of A: (0, 1)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;B_rref&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B_pivots&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;rref&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Reduced row echelon form of B:&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Reduced row echelon form of B:
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

\[\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; -1 &amp;amp; 0\\
0 &amp;amp; 1 &amp;amp; 2 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0
\end{bmatrix}\]

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Column pivots of A: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B_pivots&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Column pivots of A: (0, 1, 3)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For $\textit{A}$, we found that the first and second column vectors are the &lt;em&gt;basis&lt;/em&gt;, whereas for $\textit{B}$ is the first, second, and fourth.&lt;/p&gt;

&lt;p&gt;Now that we know about a &lt;em&gt;basis&lt;/em&gt; and how to find it, understanding the concept of &lt;em&gt;rank&lt;/em&gt; is simpler. The &lt;strong&gt;rank&lt;/strong&gt; of a matrix $\textit{A}$ is the dimensionality of the vector space generated by its number of linearly independent column vectors. This happens to be identical to the dimensionality of the vector space generated by its row vectors. We denote the &lt;em&gt;rank&lt;/em&gt; of matrix as $rk(\textit{A})$ or $rank(\textit{A})$.&lt;/p&gt;

&lt;p&gt;For an square matrix $\mathbb{R}^{m\times n}$ (i.e., $m=n$), we say is &lt;strong&gt;full rank&lt;/strong&gt; when every column and/or row is linearly independent. For a non-square matrix with $m&amp;gt;n$ (i.e., more rows than columns), we say is &lt;strong&gt;full rank&lt;/strong&gt; when every row is linearly independent. When $m&amp;lt;n$ (i.e., more columns than rows), we say is &lt;strong&gt;full rank&lt;/strong&gt; when every column is linearly independent.&lt;/p&gt;

&lt;p&gt;From an applied machine learning perspective, the &lt;em&gt;rank&lt;/em&gt; of a matrix is relevant as a measure of the &lt;a href=&quot;https://math.stackexchange.com/questions/21100/importance-of-matrix-rank&quot;&gt;information content of the matrix&lt;/a&gt;. Take matrix $\textit{B}$ from the example above. Although the original matrix has 5 columns, we know is rank 4, hence, it has less information than it appears at first glance.&lt;/p&gt;

&lt;h2 id=&quot;matrix-norm&quot;&gt;Matrix norm&lt;/h2&gt;

&lt;p&gt;As with vectors, we can measure the size of a matrix by computing its &lt;strong&gt;norm&lt;/strong&gt;. There are multiple ways to define the norm for a matrix, as long it satisfies the same properties defined for vectors norms: (1) absolutely homogeneous, (2) triangle inequality, (3) positive definite (see vector norms section). For our purposes, I’ll cover two of the most commonly used norms in machine learning: (1) &lt;strong&gt;Frobenius norm&lt;/strong&gt;, (2) &lt;strong&gt;max norm&lt;/strong&gt;, (3) &lt;strong&gt;spectral norm&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: I won’t cover the spectral norm just yet, because it depends on concepts that I have not introduced at this point.&lt;/p&gt;

&lt;h3 id=&quot;frobenius-norm&quot;&gt;Frobenius norm&lt;/h3&gt;

&lt;p&gt;The &lt;strong&gt;Frobenius norm&lt;/strong&gt; is an element-wise norm named after the German mathematician Ferdinand Georg Frobenius. We denote this norm as $\Vert \textit{A} \Vert_F$. You can thing about this norm as flattening out the matrix into a long vector. For instance, a $3 \times 3$ matrix would become a vector with $n=9$ entries. We define the Frobenius norm as:&lt;/p&gt;

\[\Vert \textit{A} \Vert_F := \sqrt{\sum_{i=1}^m \sum_{j=1}^n a_{ij}^2}\]

&lt;p&gt;In words: square each entry of $\textit{A}$, add them together, and then take the square root.&lt;/p&gt;

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;, we can compute the Frobenius norm as with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;linal.norm&lt;/code&gt; method ant &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fro&lt;/code&gt; as the argument:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;fro&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;16.881943016134134
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;max-norm-1&quot;&gt;Max norm&lt;/h3&gt;

&lt;p&gt;The &lt;strong&gt;max norm&lt;/strong&gt; or &lt;strong&gt;infinity norm&lt;/strong&gt; of a matrix equals to the largest sum of the absolute value of row vectors. We denote the max norm as $\Vert \textit{A} \Vert_max$. Consider $\textit{A} \in \mathbb{R}^{m \times n}$. We define the max norm for $\textit{A}$ as:&lt;/p&gt;

\[\Vert \textit{A} \Vert_{max} := \text{max}_{i} \sum_{j=1}^n\vert a_{ij} \vert\]

&lt;p&gt;This equals to go row by row, adding the absolute value of each entry, and then selecting the largest sum.&lt;/p&gt;

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Numpy&lt;/code&gt;, we compute the max norm as:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;24.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this case, is easy to see that the third row has the largest absolute value.&lt;/p&gt;

&lt;h3 id=&quot;spectral-norm&quot;&gt;Spectral norm&lt;/h3&gt;

&lt;p&gt;To understand this norm, is necessary to first learn about eigenvectors and eigenvalues, which I cover later.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;spectral norm&lt;/strong&gt; of a matrix equals to the largest singular value $\sigma_1$. We denote the spectral norm as $\Vert \textit{A} \Vert_2$. Consider $\textit{A} \in \mathbb{R}^{m \times n}$. We define the spectral for $\textit{A}$ as:&lt;/p&gt;

\[\Vert \textit{A} \Vert_2 :=
\text{max}_{x}
\frac{\Vert \textit{A}\textbf{x} \Vert_2}{\Vert \textbf{x} \Vert_2}\]

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Numpy&lt;/code&gt;, we compute the max norm as:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;16.84810335261421
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;linear-and-affine-mappings&quot;&gt;Linear and affine mappings&lt;/h1&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Libraries for this section
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;altair&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;themes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;enable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;dark&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ThemeRegistry.enable(&apos;dark&apos;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;linear-mappings&quot;&gt;Linear mappings&lt;/h2&gt;

&lt;p&gt;Now we have covered the basics of vectors and matrices, we are ready to introduce the idea of a linear mapping. &lt;strong&gt;Linear mappings&lt;/strong&gt;, also known as &lt;em&gt;linear transformations&lt;/em&gt; and &lt;em&gt;linear functions&lt;/em&gt;, indicate the correspondence between vectors in a vector space $\textit{V}$ and the same vectors in a different vector space $\textit{W}$. This is an abstract idea. I like to think about this in the following manner: imagine there is a multiverse as in Marvel comics, but instead of humans, aliens, gods, stars, galaxies, and superheroes, we have &lt;em&gt;vectors&lt;/em&gt;. In this context, a linear mapping would indicate the &lt;em&gt;correspondence&lt;/em&gt; of entities (i.e., planets, humans, superheroes, etc) &lt;em&gt;between universes&lt;/em&gt;. Just imagine us, placidly existing in our own universe, and suddenly a &lt;em&gt;linear mapping&lt;/em&gt; happens: our entire universe would be transformed into a different one, according to whatever rules the linear mapping has enforced. Now, switch &lt;em&gt;universes&lt;/em&gt; for &lt;em&gt;vector spaces&lt;/em&gt; and &lt;em&gt;us&lt;/em&gt; by vectors, and you’ll get the full picture.&lt;/p&gt;

&lt;p&gt;So, linear mappings transform vector spaces into others. Yet, such transformations are constrained to a spefic kind: &lt;strong&gt;linear ones&lt;/strong&gt;. Consider a linear mapping $\textit{T}$ and a pair of vectors $\bf{x}$ and $\bf{y}$. To be valid, a linear mapping must satisfies these rules:&lt;/p&gt;

\[\begin{matrix}
\begin{align*}
\textit{T}(\bf{x} + \bf{y}) &amp;amp;= \textit{T}(\bf{x}) + \textit{T}(\bf{y}) \\
\text{T}(\alpha \bf{x}) &amp;amp;= \alpha\textit{T} (\bf{x}) \text{, } \forall \alpha
\end{align*}
\end{matrix}\]

&lt;p&gt;In words:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The transformation of the sum of the vectors must be equal to taking the transformation of each vector individually and then adding them up.&lt;/li&gt;
  &lt;li&gt;The transformation of a scaled version of a vector must be equal to taking the transformation of the vector first and then scaling the result.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The two properties above can be condenced into one, the &lt;strong&gt;superposition property&lt;/strong&gt;:&lt;/p&gt;

\[\textit{T}(\alpha \bf{x} + \beta \bf{y}) = \alpha \textit{T}(\bf{x}) + \beta \textit{T}(\bf{y})\]

&lt;p&gt;As a result of satisfying those properties, linear mappings &lt;strong&gt;preserve the structure of the original vector space&lt;/strong&gt;. Imagine a vector space $\in \mathbb{R}^2$, like a grid on lines in a cartesian plane. Visually, preserving the structure of the vector space after a mapping means to: (1) the origin of the coordinate space remains fixed, and (2) the lines remain lines and parallel to each other.&lt;/p&gt;

&lt;p&gt;In linear algebra, linear mappings are represented as matrices and performed by matrix multiplication. Take a vector $\bf{x}$ and a matrix $\textit{A}$. We say that when $\textit{A}$ multiplies $\bf{x}$, the matrix transform the vector into another one:&lt;/p&gt;

\[\textit{T}(\bf{x}) = \textit{A}\bf{x}\]

&lt;p&gt;The typicall notation for a linear mapping is the same we used for functions. For the vector spaces $\textit{V}$ and $\textit{W}$, we indicate the linear mapping as $\textit{T}: \textit{V} \rightarrow \textit{W}$&lt;/p&gt;

&lt;h2 id=&quot;examples-of-linear-mappings&quot;&gt;Examples of linear mappings&lt;/h2&gt;

&lt;p&gt;Let’s examine a couple of examples of proper linear mappings. In general, &lt;em&gt;dot products are linear mappings&lt;/em&gt;. This should come as no surprise since dot products are linear operations by definition. Dot products sometimes take special names, when they have a well-known effect on a linear space. I’ll examine two simple cases: &lt;strong&gt;negation&lt;/strong&gt; and &lt;strong&gt;reversal&lt;/strong&gt;. Keep in mind that although we will test this for one vector, this mapping work on the entire vector space (i.e., the span) of a given dimensionality.&lt;/p&gt;

&lt;h3 id=&quot;negation-matrix&quot;&gt;Negation matrix&lt;/h3&gt;

&lt;p&gt;A &lt;strong&gt;negation matrix&lt;/strong&gt; returns the opposite sign of each element of a vector. It can be defined as:&lt;/p&gt;

\[\textit{T} := \textit{A} := \textit{-I}\]

&lt;p&gt;This is, the negative identity matrix. Consider a pair of vectors $\bf{x} \in \mathbb{R}^3$ and $\bf{x} \in \mathbb{y}^3$, and the negation matrix $\textit{-I} \in \mathbb{R}^{3 \times 3}$. Let’s test the linear mapping properties with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We first test $\textit{T}(\bf{x} + \bf{y}) = \textit{T}(\bf{x}) + \textit{T}(\bf{y})$:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;left_side_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;right_side_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Left side of the equation:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;left_side_1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Right side of the equation:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;right_side_1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Left side of the equation:
[[ 4]
 [ 0]
 [-3]]
Right side of the equation:
[[ 4]
 [ 0]
 [-3]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Hence, we confirm we get the same results.&lt;/p&gt;

&lt;p&gt;Let’s check the second property $\text{T}(\alpha \bf{x}) = \alpha\textit{T} (\bf{x}) \text{, } \forall \alpha$&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;left_side_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;right_side_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Left side of the equation:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;left_side_2&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Right side of the equation:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;right_side_2&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Left side of the equation:
[[ 2]
 [ 0]
 [-2]]
Right side of the equation:
[[ 2]
 [ 0]
 [-2]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Again, we confirm we get the same results for both sides of the equation&lt;/p&gt;

&lt;h3 id=&quot;reversal-matrix&quot;&gt;Reversal matrix&lt;/h3&gt;

&lt;p&gt;A &lt;strong&gt;reversal matrix&lt;/strong&gt; returns reverses the order of the elements of a vector. This is, the last become the first, the second to last becomes the second, and so on. For a matrix in $\mathbb{R}^{3 \times 3}$ is defined as:&lt;/p&gt;

\[\textit{T} :=
\begin{bmatrix}
0 &amp;amp; 0 &amp;amp; 1 \\
0 &amp;amp; 1 &amp;amp; 0 \\
1 &amp;amp; 0 &amp;amp; 0
\end{bmatrix}\]

&lt;p&gt;In general, it is the &lt;em&gt;identity matrix but backwards&lt;/em&gt;, with ones from the bottom left corner to the top right corern. Consider a pair of vectors $\bf{x} \in \mathbb{R}^3$ and $\bf{x} \in \mathbb{y}^3$, and the reversal matrix $\textit{T} \in \mathbb{R}^{3 \times 3}$. Let’s test the linear mapping properties with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We first test $\textit{T}(\bf{x} + \bf{y}) = \textit{T}(\bf{x}) + \textit{T}(\bf{y})$:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x_reversal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_reversal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;left_side_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;right_side_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;x before reversal:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;x after reversal &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_reversal&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;y before reversal:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;y after reversal &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_reversal&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Left side of the equation (add reversed vectors):&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;left_side_1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Right side of the equation (add reversed vectors):&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;right_side_1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;x before reversal:
[[-1]
 [ 0]
 [ 1]]
x after reversal
[[ 1]
 [ 0]
 [-1]]
y before reversal:
[[-3]
 [ 0]
 [ 2]]
y after reversal
[[ 2]
 [ 0]
 [-3]]
Left side of the equation (add reversed vectors):
[[ 3]
 [ 0]
 [-4]]
Right side of the equation (add reversed vectors):
[[ 3]
 [ 0]
 [-4]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This works fine. Let’s check the second property $\text{T}(\alpha \bf{x}) = \alpha\textit{T} (\bf{x}) \text{, } \forall \alpha$&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;left_side_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;right_side_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Left side of the equation:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;left_side_2&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Right side of the equation:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;right_side_2&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Left side of the equation:
[[ 2]
 [ 0]
 [-2]]
Right side of the equation:
[[ 2]
 [ 0]
 [-2]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;examples-of-nonlinear-mappings&quot;&gt;Examples of nonlinear mappings&lt;/h2&gt;

&lt;p&gt;As with most subjects, examining examples of &lt;em&gt;what things are not&lt;/em&gt; can be enlightening. Let’s take a couple of non-linear mappings: &lt;strong&gt;norms&lt;/strong&gt; and &lt;strong&gt;translation&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;norms&quot;&gt;Norms&lt;/h3&gt;

&lt;p&gt;This may come as a surprise, but norms are not linear transformations. Not “some” norms, but all norms. This is because of the very definition of a norm, in particular, the &lt;strong&gt;triangle inequality&lt;/strong&gt; and &lt;strong&gt;positive definite&lt;/strong&gt; properties, colliding with the requirements of linear mappings.&lt;/p&gt;

&lt;p&gt;First, the triangle inequality defines: $\lVert \bf{x} + \bf{y} \rVert \le \lVert \bf{x} \rVert + \lVert \bf{y} \rVert$. Whereas the first requirement for linear mappings demands: $\textit{T}(\bf{x} + \bf{y}) = \textit{T}(\bf{x}) + \textit{T}(\bf{y})$. The problem here is in the $\le$ condition, which means adding two vectors and then taking the norm &lt;em&gt;can&lt;/em&gt; be less than the sum of the norms of the individual vectors. Such condition is, by defnition, not allowed for linear mappings.&lt;/p&gt;

&lt;p&gt;Second, the positive definite defines: $\lVert \bf{x} \rVert \ge 0$ and $ \lVert \bf{x} \rVert = 0 \Longleftrightarrow \bf{x}= 0$. Put simply, norms &lt;em&gt;have to&lt;/em&gt; be a postive value. For instance, the norm of $\Vert - x \Vert = \Vert x \Vert$, instead of $\Vert - x \Vert$. But, the second property for linear mappings requires $\Vert -\alpha \bf{x} \Vert = -\alpha \Vert \bf{x} \Vert$. Hence, it fails when we multiply by a negative number (i.e., it can preserve the negative sign).&lt;/p&gt;

&lt;h3 id=&quot;translation&quot;&gt;Translation&lt;/h3&gt;

&lt;p&gt;Translation is a geometric transformation that moves every vector in a vector space by the same distance in a given direction. Translation is an operation that matches our everyday life intuitions: move a cup of coffee from your left to your right, and you would have performed translation in $\mathbb{R}^3$ space.&lt;/p&gt;

&lt;p&gt;Contrary to what we have seen so far, the translation matrix is represented with &lt;strong&gt;homogeneous coordinates&lt;/strong&gt; instead of cartesian coordinates. Put simply, the homogeneous coordinate system adds a extra $1$ at the end of vectros. For instance, the vector in $\mathbb{R}^2$ cartesian coordinates:&lt;/p&gt;

\[\bf{x} =
\begin{bmatrix}
2 \\
2 \\
\end{bmatrix}\]

&lt;p&gt;Becomes the following in $\mathbb{R}^2$ homogeneous coordinates:&lt;/p&gt;

\[\bf{x} =
\begin{bmatrix}
2 \\
2 \\
1
\end{bmatrix}\]

&lt;p&gt;In fact, the translation matrix for the general case can’t be represented with cartesian coordinates. Homogeneous coordinates are the standard in fields like computer graphics since they allow us to better represent a series of transformations (or mappings) like scaling, translation, rotation, etc.&lt;/p&gt;

&lt;p&gt;A translation matrix in $\mathbb{R}^3$ can be denoted as:&lt;/p&gt;

\[\textit{T}_v =
\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; v_1 \\
0 &amp;amp; 1 &amp;amp; v_2 \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}\]

&lt;p&gt;Where $v_1$ and $v_2$ are the values added to each dimension for translation. For instance, consider $\bf{x} = \begin{bmatrix} 2 &amp;amp; 2 \end{bmatrix}^T \in \mathbb{R}^2$. If we want translate this $3$ units in the first dimension, and $1$ units in the second dimension, we first transfor the vector to homogeneous coordinates $\bf{x} = \begin{bmatrix} 2 &amp;amp; 2 &amp;amp; 1 \end{bmatrix}^T$ , and then perfom matrix-vector multiplication as usual:&lt;/p&gt;

\[\textit{T}_v =
\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 3 \\
0 &amp;amp; 1 &amp;amp; 1 \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}
\begin{bmatrix}
2 \\
2 \\
1
\end{bmatrix}=
\begin{bmatrix}
5 \\
3 \\
1
\end{bmatrix}\]

&lt;p&gt;The first two vectors in the translation matrix simple reproduce the original vector (i.e., the identity), and the third vector is the one actually “moving” the vectors.&lt;/p&gt;

&lt;p&gt;Translation is &lt;strong&gt;not a linear mapping&lt;/strong&gt; simply because $\textit{T}(\bf{x} + \bf{y}) = \textit{T}(\bf{x}) + \textit{T}(\bf{y})$ &lt;strong&gt;does not hold&lt;/strong&gt;. In the case of translation $\textit{T}(\bf{x} + \bf{y})  = \textit{T}(\bf{x} + v_1) + \textit{T}(\bf{y} + + v_1)$, which invalidates the operation as a linear mapping. This type of mapping is known as an &lt;strong&gt;affine mapping or transformation&lt;/strong&gt;, which is the topic I’ll review next.&lt;/p&gt;

&lt;h2 id=&quot;affine-mappings&quot;&gt;Affine mappings&lt;/h2&gt;

&lt;p&gt;The simplest way to describe affine mappings (or transformations) is as a &lt;em&gt;linear mapping&lt;/em&gt; + &lt;em&gt;translation&lt;/em&gt;. Hence, an affine mapping $\textit{M}$ takes the form of:&lt;/p&gt;

\[\textit{M}(\textbf{x}) = \textit{A}\textbf{x} + \textbf{b}\]

&lt;p&gt;Where $\textit{A}$ is a linear mapping or transformation and $\textbf{b}$ is the translation vector.&lt;/p&gt;

&lt;p&gt;If you are familiar with linear regression, you would notice that the above expression is its matrix form. Linear regression is usually analyzed as a linear mapping plus noise, but it can also be seen as an affine mapping. Alternative, we can say that $\textit{A}\textbf{x} + \textbf{b}$ is a linear mapping &lt;em&gt;if and only if&lt;/em&gt; $\textbf{b}=0$.&lt;/p&gt;

&lt;p&gt;From a geometrical perspective, affine mappings displace spaces (lines or hyperplanes) from the origin of the coordinate space. Consequently, affine mappings do not operate over &lt;em&gt;vector spaces&lt;/em&gt; as the zero vector condition $\bf{0} \in S$ does not hold anymore. Affine mappings act onto &lt;em&gt;affine subspaces&lt;/em&gt;, that I’ll define later in this section.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fig. 14: Affine mapping&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-10/b-affine-mapping.svg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;affine-combination-of-vectors&quot;&gt;Affine combination of vectors&lt;/h2&gt;

&lt;p&gt;We can think in affine combinations of vectors, as linear combinations with an added constraint.
Let’s recall de definitoon for a linear combination. Consider a set of vectors $x_1, …, x_k$ and scalars $\beta_1, …, \beta_k \in \mathbb{R}$, then a linear combination is:&lt;/p&gt;

\[\sum_{j=1}^k \beta_j x_j := \beta_1x_1 + ... + \beta_kx_k\]

&lt;p&gt;For affine combinations, we add the condition:&lt;/p&gt;

\[\sum_{j=1}^k \beta_j = 1\]

&lt;p&gt;In words, we constrain the sum of the weights $\beta$ to $1$. In practice, this defines a &lt;em&gt;weighted average of the vectors&lt;/em&gt;. This restriction has a palpable effect which is easier to grasp from a geometric perspective.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fig. 15: Affine combinations&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-10/b-affine-combination.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fig. 15&lt;/strong&gt; shows two affine combinations. The first combination with weights $\beta_1 = \frac{1}{2}$ and $\beta_2 = \frac{1}{2}$, which yields the midpoint between vectors $\bf{x}$ and $\bf{y}$. The second combination with weights $\beta_1 = 3$ and $\beta_2 =-1$ (add up to $1$), which yield a point over the vector $\bf{z}$. In both cases, we have that the resulting vector lies on the same line. This is a general consequence of constraining the sum of the weights to $1$: &lt;em&gt;every affine combination of the same set of vectors will map onto the same space&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;affine-span&quot;&gt;Affine span&lt;/h2&gt;

&lt;p&gt;The set of all linear combinations, define the the vector span. Similarly, the set of all affine combinations determine the &lt;strong&gt;affine span&lt;/strong&gt;. As we saw in &lt;strong&gt;Fig. 15&lt;/strong&gt;, every affine of vectors $\textbf{x}$ and $\textbf{y}$ maps onto the line $\textbf{z}$. More generally, we say that the &lt;strong&gt;affine span&lt;/strong&gt; of vectors $\textbf{x}_1, \cdots, \textbf{x}_k$ is:&lt;/p&gt;

\[\textbf{x}_1, \cdots, \textbf{x}_k := \sum_{j=1}^k \beta_j \textbf{x}_j, \vert  \sum_{j=1} \beta_j = 1 \in \mathbb{R} \forall \beta\]

&lt;p&gt;Again, in words: the affine span is the set of all linear combinations of the vector set, such that the weights add up to $1$ and all weights are real numbers. Hence, the fundamental difference between vector spaces and affine spaces, is the former will span the entire $\mathbb{R}^n$ space (assuming independent vectors), whereas the latter will span a line.&lt;/p&gt;

&lt;p&gt;Let’s consider three cases in $\mathbb{R}^3$: (1) three linearly independent vectors; (2) two linearly independent vectors and one dependent vector; (3) three linearly dependent vectors. In case (1), the affine span is the 2-dimensional plane containing those vectors. In case (2), the affine space is a line. Finally, in case (3), the span a single point. This may not be entirely obvious, so I encourage you to draw and the three cases, take the affine combinations and see what happens.&lt;/p&gt;

&lt;h2 id=&quot;affine-space-and-subspace&quot;&gt;Affine space and subspace&lt;/h2&gt;

&lt;p&gt;In simple terms, &lt;strong&gt;affine spaces&lt;/strong&gt; are &lt;em&gt;translates&lt;/em&gt; of vector spaces, this is, vector spaces that have been offset from the origin of the coordinate system. Such a notion makes sound affine spaces as a special case of vector spaces, but they are actually more general. Indeed, affine spaces provide a more general framework to do geometric manipulation, as they work independently of the choice of the coordinate system (i.e., it is not constrained to the origin). For instance, the set of solutions of the system of linear equations $\textit{A}\textbf{x}=\textbf{y}$ (i.e., linear regression), is an affine space, not a linear vector space.&lt;/p&gt;

&lt;p&gt;Consider a vector space $\textit{V}$, a vector $\textbf{x}_0 \in \textit{V}$, and a subset $\textit{U} \subseteq \textit{V}$. We define an affine subspace $\textit{L}$ as:&lt;/p&gt;

\[\textit{L} =
\textbf{x}_0 + \textit{U} := \{ \textbf{x}_0 + \textbf{u}: \textbf{u} \in \textit{U} \}\]

&lt;p&gt;Further, any point, line, plane, or hyperplane in $\mathbb{R}^n$ that does not go through the origin, is an affine subspace.&lt;/p&gt;

&lt;h2 id=&quot;affine-mappings-using-the-augmented-matrix&quot;&gt;Affine mappings using the augmented matrix&lt;/h2&gt;

&lt;p&gt;Consider the matrix $\textit{A} \in \mathbb{R}^{m \times n}$, and vectors $\textbf{x}, \textbf{b}, \textbf{y} \in  \mathbb{R}^n$&lt;/p&gt;

&lt;p&gt;We can represent the system of linear equations:&lt;/p&gt;

\[\textit{A}\textbf{x} + \textbf{b}  = \textbf{y}\]

&lt;p&gt;As a single matrix vector multiplication, by using an &lt;strong&gt;augmented matrix&lt;/strong&gt; of the form:&lt;/p&gt;

\[\left[
\begin{matrix}
&amp;amp; \textit{} &amp;amp;\\
&amp;amp; \textit{A} &amp;amp;\\
&amp;amp; \textit{} &amp;amp;\\
0 &amp;amp; \cdots &amp;amp; 1
\end{matrix}
  \left|
    \,
\begin{matrix}
x_1 \\
\vdots \\
x_n \\
1
\end{matrix}
  \right.
\right] =
\begin{bmatrix}
y_1 \\
\vdots \\
y_n \\
1
\end{bmatrix}\]

&lt;p&gt;This form is known as the &lt;strong&gt;affine transformation matrix&lt;/strong&gt;. We made use of this form when we exemplified &lt;em&gt;translation&lt;/em&gt;, which happens to be an affine mapping.&lt;/p&gt;

&lt;h2 id=&quot;special-linear-mappings&quot;&gt;Special linear mappings&lt;/h2&gt;

&lt;p&gt;There are several important linear mappings (or transformations) that can be expressed as matrix-vector multiplications of the form $\textbf{y} = \textit{A}\textbf{x}$. Such mappings are common in image processing, computer vision, and other linear applications. Further, combinations of linear and nonlinear mappings are what complex models as neural networks do to learn mappings from inputs to outputs. Here we briefly review six of the most important linear mappings.&lt;/p&gt;

&lt;h3 id=&quot;scaling&quot;&gt;Scaling&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Scaling&lt;/strong&gt; is a mapping of the form $\textbf{y} = \textit{A}\textbf{x}$, with $\textit{A} = \alpha \textit{I}$. Scaling &lt;em&gt;stretches&lt;/em&gt; $\textbf{x}$ by a factor $\vert \alpha \vert$ when $\alpha &amp;lt; 1$, &lt;em&gt;shrinks&lt;/em&gt; $\textbf{x}$ when $\alpha &amp;lt; 1$, and &lt;em&gt;reverses&lt;/em&gt; the direction of the vector when $\alpha &amp;lt; 0$. For geometrical objects in Euclidean space, scaling changes the size but not the shape of objects. An scaling matrix in $\mathbb{R}^2$ takes the form:&lt;/p&gt;

\[\begin{bmatrix}
s_1 &amp;amp; 0 \\
0   &amp;amp; s_2
\end{bmatrix}\]

&lt;p&gt;Where $s_1, s_2$ are the scaling factors.&lt;/p&gt;

&lt;p&gt;Let’s scale a vector using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;. We will define a scaling matrix $\textit{A}$, a vector $\textbf{x}$ to scale, and then plot the original and scaled vectors with Altair.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To scale $\textbf{x}$, we perform matrix-vector multiplication as usual&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;column_stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;dim-1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;dim-2&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;tran&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;tran&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;base&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;base&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]})&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }

&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;dim-1&lt;/th&gt;
      &lt;th&gt;dim-2&lt;/th&gt;
      &lt;th&gt;type&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;tran&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;4.0&lt;/td&gt;
      &lt;td&gt;8.0&lt;/td&gt;
      &lt;td&gt;tran&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;base&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;4.0&lt;/td&gt;
      &lt;td&gt;base&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;We see that the resulting scaled vector (‘tran’) is indeed two times the original vector (‘base’). Now let’s plot. The light blue line solid line represents the original vector, whereas the dashed orange line represents the scaled vector.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;chart&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mark_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;opacity&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;dim-1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;dim-2&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;strokeDash&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;chart&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div id=&quot;altair-viz-50a6b2133b244c91b1bc42a228990ef5&quot;&gt;&lt;/div&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
  (function(spec, embedOpt){
    let outputDiv = document.currentScript.previousElementSibling;
    if (outputDiv.id !== &quot;altair-viz-50a6b2133b244c91b1bc42a228990ef5&quot;) {
      outputDiv = document.getElementById(&quot;altair-viz-50a6b2133b244c91b1bc42a228990ef5&quot;);
    }
    const paths = {
      &quot;vega&quot;: &quot;https://cdn.jsdelivr.net/npm//vega@5?noext&quot;,
      &quot;vega-lib&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-lib?noext&quot;,
      &quot;vega-lite&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext&quot;,
      &quot;vega-embed&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-embed@6?noext&quot;,
    };

    function loadScript(lib) {
      return new Promise(function(resolve, reject) {
        var s = document.createElement(&apos;script&apos;);
        s.src = paths[lib];
        s.async = true;
        s.onload = () =&gt; resolve(paths[lib]);
        s.onerror = () =&gt; reject(`Error loading script: ${paths[lib]}`);
        document.getElementsByTagName(&quot;head&quot;)[0].appendChild(s);
      });
    }

    function showError(err) {
      outputDiv.innerHTML = `&lt;div class=&quot;error&quot; style=&quot;color:red;&quot;&gt;${err}&lt;/div&gt;`;
      throw err;
    }

    function displayChart(vegaEmbed) {
      vegaEmbed(outputDiv, spec, embedOpt)
        .catch(err =&gt; showError(`Javascript Error: ${err.message}&lt;br&gt;This usually means there&apos;s a typo in your chart specification. See the javascript console for the full traceback.`));
    }

    if(typeof define === &quot;function&quot; &amp;&amp; define.amd) {
      requirejs.config({paths});
      require([&quot;vega-embed&quot;], displayChart, err =&gt; showError(`Error loading script: ${err.message}`));
    } else if (typeof vegaEmbed === &quot;function&quot;) {
      displayChart(vegaEmbed);
    } else {
      loadScript(&quot;vega&quot;)
        .then(() =&gt; loadScript(&quot;vega-lite&quot;))
        .then(() =&gt; loadScript(&quot;vega-embed&quot;))
        .catch(showError)
        .then(() =&gt; displayChart(vegaEmbed));
    }

})({&quot;usermeta&quot;: {&quot;embedOptions&quot;: {&quot;theme&quot;: &quot;dark&quot;}}, &quot;config&quot;: {&quot;view&quot;: {&quot;continuousWidth&quot;: 400, &quot;continuousHeight&quot;: 300}}, &quot;data&quot;: {&quot;name&quot;: &quot;data-9f7a3cb65390d8e1f055fe5dbda17177&quot;}, &quot;mark&quot;: {&quot;type&quot;: &quot;line&quot;, &quot;opacity&quot;: 0.8}, &quot;encoding&quot;: {&quot;color&quot;: {&quot;type&quot;: &quot;nominal&quot;, &quot;field&quot;: &quot;type&quot;}, &quot;strokeDash&quot;: {&quot;type&quot;: &quot;nominal&quot;, &quot;field&quot;: &quot;type&quot;}, &quot;x&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;dim-1&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;dim-2&quot;}}, &quot;$schema&quot;: &quot;https://vega.github.io/schema/vega-lite/v4.8.1.json&quot;, &quot;datasets&quot;: {&quot;data-9f7a3cb65390d8e1f055fe5dbda17177&quot;: [{&quot;dim-1&quot;: 0.0, &quot;dim-2&quot;: 0.0, &quot;type&quot;: &quot;tran&quot;}, {&quot;dim-1&quot;: 4.0, &quot;dim-2&quot;: 8.0, &quot;type&quot;: &quot;tran&quot;}, {&quot;dim-1&quot;: 0.0, &quot;dim-2&quot;: 0.0, &quot;type&quot;: &quot;base&quot;}, {&quot;dim-1&quot;: 2.0, &quot;dim-2&quot;: 4.0, &quot;type&quot;: &quot;base&quot;}]}}, {&quot;mode&quot;: &quot;vega-lite&quot;});
&lt;/script&gt;

&lt;h3 id=&quot;reflection&quot;&gt;Reflection&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Reflection&lt;/strong&gt; is the mirror image of an object in Euclidean space. For the general case, reflection of a vector $\textbf{x}$ through a line that passes through the origin is obtained as:&lt;/p&gt;

\[\begin{bmatrix}
\cos (2 \theta) &amp;amp; \sin (2 \theta) \\
\sin (2 \theta) &amp;amp; -\cos (2 \theta)
\end{bmatrix} \textbf{x}\]

&lt;p&gt;where $\theta$ are radians of inclination with respect to the horizontal axis. I’ve been purposely avoiding trigonometric functions, so let’s examine a couple of special cases for a vector $\textbf{x}$ in $\mathbb{R}^2$ (that can be extended to an arbitrary number of dimensions).&lt;/p&gt;

&lt;p&gt;Reflection along the horizontal axis, or around the line at $0^{\circ}$ from the origin:&lt;/p&gt;

\[\begin{bmatrix}
1 &amp;amp; 0 \\
0 &amp;amp; -1
\end{bmatrix}\]

&lt;p&gt;Reflection along the vertical axis, or around the line at $90^{\circ}$ from the origin:&lt;/p&gt;

\[\begin{bmatrix}
-1 &amp;amp; 0 \\
0 &amp;amp; 1
\end{bmatrix}\]

&lt;p&gt;Reflection along the line where the horizontal axis equals the vertical axis, or around the line at $45^{\circ}$ from the origin:&lt;/p&gt;

\[\begin{bmatrix}
0 &amp;amp; 1 \\
1 &amp;amp; 0
\end{bmatrix}\]

&lt;p&gt;Reflection along the line where the horizontal axis equals the negative of the vertical axis, or around the line at $-45^{\circ}$ from the origin:&lt;/p&gt;

\[\begin{bmatrix}
0 &amp;amp; -1 \\
-1 &amp;amp; 0
\end{bmatrix}\]

&lt;p&gt;Let’s reflect a vector using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;. We will define a reflection matrix $\textit{A}$, a vector $\textbf{x}$ to reflect, and then plot the original and reflected vectors with Altair.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# rotation along the horiontal axis
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
               &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# rotation along the vertical axis
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
               &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# rotation along the line at 45 degrees from the origin
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
               &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# rotation along the line at -45 degrees from the origin
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
               &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;column_stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;dim-1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;dim-2&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                   &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;reflection&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;original&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;original&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                  &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;0-degrees&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;0-degrees&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                  &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;90-degrees&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;90-degrees&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                  &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;45-degrees&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;45-degrees&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                  &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;neg-45-degrees&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;neg-45-degrees&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]})&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }

&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;dim-1&lt;/th&gt;
      &lt;th&gt;dim-2&lt;/th&gt;
      &lt;th&gt;reflection&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;original&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;4.0&lt;/td&gt;
      &lt;td&gt;original&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0-degrees&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;-4.0&lt;/td&gt;
      &lt;td&gt;0-degrees&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;90-degrees&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;-2.0&lt;/td&gt;
      &lt;td&gt;4.0&lt;/td&gt;
      &lt;td&gt;90-degrees&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;45-degrees&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;4.0&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;45-degrees&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;neg-45-degrees&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;-4.0&lt;/td&gt;
      &lt;td&gt;-2.0&lt;/td&gt;
      &lt;td&gt;neg-45-degrees&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;base_coor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ran1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ran2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;sh&quot;&gt;&apos;&apos;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;return base chart with coordinate space&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&apos;&apos;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;df_base&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;horizontal&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ran1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ran2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;vertical&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)})&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df_base&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mark_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;white&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;horizontal&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;vertical&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df_base&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mark_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;white&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;horizontal&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;vertical&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;base&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;chart&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mark_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;dim-1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;horizontal-axis&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;dim-2&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;vertical-axis&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;reflection&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;base_coor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;5.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;5.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;chart&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div id=&quot;altair-viz-ea6b72385c0e4d57a622d722c24f7e1e&quot;&gt;&lt;/div&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
  (function(spec, embedOpt){
    let outputDiv = document.currentScript.previousElementSibling;
    if (outputDiv.id !== &quot;altair-viz-ea6b72385c0e4d57a622d722c24f7e1e&quot;) {
      outputDiv = document.getElementById(&quot;altair-viz-ea6b72385c0e4d57a622d722c24f7e1e&quot;);
    }
    const paths = {
      &quot;vega&quot;: &quot;https://cdn.jsdelivr.net/npm//vega@5?noext&quot;,
      &quot;vega-lib&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-lib?noext&quot;,
      &quot;vega-lite&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext&quot;,
      &quot;vega-embed&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-embed@6?noext&quot;,
    };

    function loadScript(lib) {
      return new Promise(function(resolve, reject) {
        var s = document.createElement(&apos;script&apos;);
        s.src = paths[lib];
        s.async = true;
        s.onload = () =&gt; resolve(paths[lib]);
        s.onerror = () =&gt; reject(`Error loading script: ${paths[lib]}`);
        document.getElementsByTagName(&quot;head&quot;)[0].appendChild(s);
      });
    }

    function showError(err) {
      outputDiv.innerHTML = `&lt;div class=&quot;error&quot; style=&quot;color:red;&quot;&gt;${err}&lt;/div&gt;`;
      throw err;
    }

    function displayChart(vegaEmbed) {
      vegaEmbed(outputDiv, spec, embedOpt)
        .catch(err =&gt; showError(`Javascript Error: ${err.message}&lt;br&gt;This usually means there&apos;s a typo in your chart specification. See the javascript console for the full traceback.`));
    }

    if(typeof define === &quot;function&quot; &amp;&amp; define.amd) {
      requirejs.config({paths});
      require([&quot;vega-embed&quot;], displayChart, err =&gt; showError(`Error loading script: ${err.message}`));
    } else if (typeof vegaEmbed === &quot;function&quot;) {
      displayChart(vegaEmbed);
    } else {
      loadScript(&quot;vega&quot;)
        .then(() =&gt; loadScript(&quot;vega-lite&quot;))
        .then(() =&gt; loadScript(&quot;vega-embed&quot;))
        .catch(showError)
        .then(() =&gt; displayChart(vegaEmbed));
    }

})({&quot;usermeta&quot;: {&quot;embedOptions&quot;: {&quot;theme&quot;: &quot;dark&quot;}}, &quot;config&quot;: {&quot;view&quot;: {&quot;continuousWidth&quot;: 400, &quot;continuousHeight&quot;: 300}}, &quot;layer&quot;: [{&quot;mark&quot;: {&quot;type&quot;: &quot;line&quot;, &quot;color&quot;: &quot;white&quot;}, &quot;encoding&quot;: {&quot;x&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;horizontal&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;vertical&quot;}}}, {&quot;mark&quot;: {&quot;type&quot;: &quot;line&quot;, &quot;color&quot;: &quot;white&quot;}, &quot;encoding&quot;: {&quot;x&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;vertical&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;horizontal&quot;}}}, {&quot;data&quot;: {&quot;name&quot;: &quot;data-d6c1c641e6407b304d469e8de4bf7492&quot;}, &quot;mark&quot;: &quot;line&quot;, &quot;encoding&quot;: {&quot;color&quot;: {&quot;type&quot;: &quot;nominal&quot;, &quot;field&quot;: &quot;reflection&quot;}, &quot;x&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;axis&quot;: {&quot;title&quot;: &quot;horizontal-axis&quot;}, &quot;field&quot;: &quot;dim-1&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;axis&quot;: {&quot;title&quot;: &quot;vertical-axis&quot;}, &quot;field&quot;: &quot;dim-2&quot;}}}], &quot;data&quot;: {&quot;name&quot;: &quot;data-dc621955550350bfc1b0624dd9983169&quot;}, &quot;$schema&quot;: &quot;https://vega.github.io/schema/vega-lite/v4.8.1.json&quot;, &quot;datasets&quot;: {&quot;data-dc621955550350bfc1b0624dd9983169&quot;: [{&quot;horizontal&quot;: -5.0, &quot;vertical&quot;: 0.0}, {&quot;horizontal&quot;: 5.0, &quot;vertical&quot;: 0.0}], &quot;data-d6c1c641e6407b304d469e8de4bf7492&quot;: [{&quot;dim-1&quot;: 0.0, &quot;dim-2&quot;: 0.0, &quot;reflection&quot;: &quot;original&quot;}, {&quot;dim-1&quot;: 2.0, &quot;dim-2&quot;: 4.0, &quot;reflection&quot;: &quot;original&quot;}, {&quot;dim-1&quot;: 0.0, &quot;dim-2&quot;: 0.0, &quot;reflection&quot;: &quot;0-degrees&quot;}, {&quot;dim-1&quot;: 2.0, &quot;dim-2&quot;: -4.0, &quot;reflection&quot;: &quot;0-degrees&quot;}, {&quot;dim-1&quot;: 0.0, &quot;dim-2&quot;: 0.0, &quot;reflection&quot;: &quot;90-degrees&quot;}, {&quot;dim-1&quot;: -2.0, &quot;dim-2&quot;: 4.0, &quot;reflection&quot;: &quot;90-degrees&quot;}, {&quot;dim-1&quot;: 0.0, &quot;dim-2&quot;: 0.0, &quot;reflection&quot;: &quot;45-degrees&quot;}, {&quot;dim-1&quot;: 4.0, &quot;dim-2&quot;: 2.0, &quot;reflection&quot;: &quot;45-degrees&quot;}, {&quot;dim-1&quot;: 0.0, &quot;dim-2&quot;: 0.0, &quot;reflection&quot;: &quot;neg-45-degrees&quot;}, {&quot;dim-1&quot;: -4.0, &quot;dim-2&quot;: -2.0, &quot;reflection&quot;: &quot;neg-45-degrees&quot;}]}}, {&quot;mode&quot;: &quot;vega-lite&quot;});
&lt;/script&gt;

&lt;h3 id=&quot;shear&quot;&gt;Shear&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Shear&lt;/strong&gt; mappings are hard to describe in words but easy to understand with images. I recommend to look at the shear mapping below and then read this description: a shear mapping displaces points of an object in a given direction (e.g., all points to the right), in a proportion equal to their perpendicular distance from an axis (e.g., the line on the $x$ axis) that remains fixed. A “proportion equal to their perpendicular distance” means that points further away from the reference axis displace more than points near to the axis.&lt;/p&gt;

&lt;p&gt;For an object in $\mathbb{R}^2$, a &lt;strong&gt;horizontal shear&lt;/strong&gt; matrix (i.e., paraller to the horizontal axis) takes the form:&lt;/p&gt;

\[\begin{bmatrix}
1 &amp;amp; m \\
0 &amp;amp; 1
\end{bmatrix}\]

&lt;p&gt;Where $m$ is the &lt;em&gt;shear factor&lt;/em&gt;, that essentially determines how pronounced is the shear.&lt;/p&gt;

&lt;p&gt;For an object in $\mathbb{R}^2$, a &lt;strong&gt;vertical shear&lt;/strong&gt; matrix (i.e., paraller to the vertical axis) takes the form:&lt;/p&gt;

\[\begin{bmatrix}
1 &amp;amp; 0 \\
m &amp;amp; 1
\end{bmatrix}\]

&lt;p&gt;Let’s shear a vector using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;. We will define a shear matrix $\textit{A}$, a pair of vectors $\textbf{x}$ and $\textbf{u}$ to shear, and then plot the original and shear vectors with Altair. The reason we define two vectors, is that shear mappings are easier to appreciate with planes or multiple sides figures than single lines.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# shear along the horiontal axis
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
               &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;u&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;column_stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;dim-1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;dim-2&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                   &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;shear&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;original&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;original&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                             &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;horizontal&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;horizontal&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                             &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;original-2&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;original-2&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                             &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;horizontal-2&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;horizontal-2&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;
                            &lt;span class=&quot;p&quot;&gt;]})&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;chart&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mark_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;dim-1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;horizontal-axis&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;dim-2&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;vertical-axis&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;shear&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;base_coor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;5.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;10.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;chart&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div id=&quot;altair-viz-74ba8ab9d1a74346854d14f7ff2c667d&quot;&gt;&lt;/div&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
  (function(spec, embedOpt){
    let outputDiv = document.currentScript.previousElementSibling;
    if (outputDiv.id !== &quot;altair-viz-74ba8ab9d1a74346854d14f7ff2c667d&quot;) {
      outputDiv = document.getElementById(&quot;altair-viz-74ba8ab9d1a74346854d14f7ff2c667d&quot;);
    }
    const paths = {
      &quot;vega&quot;: &quot;https://cdn.jsdelivr.net/npm//vega@5?noext&quot;,
      &quot;vega-lib&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-lib?noext&quot;,
      &quot;vega-lite&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext&quot;,
      &quot;vega-embed&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-embed@6?noext&quot;,
    };

    function loadScript(lib) {
      return new Promise(function(resolve, reject) {
        var s = document.createElement(&apos;script&apos;);
        s.src = paths[lib];
        s.async = true;
        s.onload = () =&gt; resolve(paths[lib]);
        s.onerror = () =&gt; reject(`Error loading script: ${paths[lib]}`);
        document.getElementsByTagName(&quot;head&quot;)[0].appendChild(s);
      });
    }

    function showError(err) {
      outputDiv.innerHTML = `&lt;div class=&quot;error&quot; style=&quot;color:red;&quot;&gt;${err}&lt;/div&gt;`;
      throw err;
    }

    function displayChart(vegaEmbed) {
      vegaEmbed(outputDiv, spec, embedOpt)
        .catch(err =&gt; showError(`Javascript Error: ${err.message}&lt;br&gt;This usually means there&apos;s a typo in your chart specification. See the javascript console for the full traceback.`));
    }

    if(typeof define === &quot;function&quot; &amp;&amp; define.amd) {
      requirejs.config({paths});
      require([&quot;vega-embed&quot;], displayChart, err =&gt; showError(`Error loading script: ${err.message}`));
    } else if (typeof vegaEmbed === &quot;function&quot;) {
      displayChart(vegaEmbed);
    } else {
      loadScript(&quot;vega&quot;)
        .then(() =&gt; loadScript(&quot;vega-lite&quot;))
        .then(() =&gt; loadScript(&quot;vega-embed&quot;))
        .catch(showError)
        .then(() =&gt; displayChart(vegaEmbed));
    }

})({&quot;usermeta&quot;: {&quot;embedOptions&quot;: {&quot;theme&quot;: &quot;dark&quot;}}, &quot;config&quot;: {&quot;view&quot;: {&quot;continuousWidth&quot;: 400, &quot;continuousHeight&quot;: 300}}, &quot;layer&quot;: [{&quot;mark&quot;: {&quot;type&quot;: &quot;line&quot;, &quot;color&quot;: &quot;white&quot;}, &quot;encoding&quot;: {&quot;x&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;horizontal&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;vertical&quot;}}}, {&quot;mark&quot;: {&quot;type&quot;: &quot;line&quot;, &quot;color&quot;: &quot;white&quot;}, &quot;encoding&quot;: {&quot;x&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;vertical&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;horizontal&quot;}}}, {&quot;data&quot;: {&quot;name&quot;: &quot;data-d2b6d700081f3dc3271961cd2c42922c&quot;}, &quot;mark&quot;: &quot;line&quot;, &quot;encoding&quot;: {&quot;color&quot;: {&quot;type&quot;: &quot;nominal&quot;, &quot;field&quot;: &quot;shear&quot;}, &quot;x&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;axis&quot;: {&quot;title&quot;: &quot;horizontal-axis&quot;}, &quot;field&quot;: &quot;dim-1&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;axis&quot;: {&quot;title&quot;: &quot;vertical-axis&quot;}, &quot;field&quot;: &quot;dim-2&quot;}}}], &quot;data&quot;: {&quot;name&quot;: &quot;data-29b144b3d9944f86f2a8e5d58a2cc2b0&quot;}, &quot;$schema&quot;: &quot;https://vega.github.io/schema/vega-lite/v4.8.1.json&quot;, &quot;datasets&quot;: {&quot;data-29b144b3d9944f86f2a8e5d58a2cc2b0&quot;: [{&quot;horizontal&quot;: -5.0, &quot;vertical&quot;: 0.0}, {&quot;horizontal&quot;: 10.0, &quot;vertical&quot;: 0.0}], &quot;data-d2b6d700081f3dc3271961cd2c42922c&quot;: [{&quot;dim-1&quot;: 0.0, &quot;dim-2&quot;: 0.0, &quot;shear&quot;: &quot;original&quot;}, {&quot;dim-1&quot;: 2.0, &quot;dim-2&quot;: 4.0, &quot;shear&quot;: &quot;original&quot;}, {&quot;dim-1&quot;: 0.0, &quot;dim-2&quot;: 0.0, &quot;shear&quot;: &quot;horizontal&quot;}, {&quot;dim-1&quot;: 8.0, &quot;dim-2&quot;: 4.0, &quot;shear&quot;: &quot;horizontal&quot;}, {&quot;dim-1&quot;: 2.0, &quot;dim-2&quot;: 0.0, &quot;shear&quot;: &quot;original-2&quot;}, {&quot;dim-1&quot;: 4.0, &quot;dim-2&quot;: 4.0, &quot;shear&quot;: &quot;original-2&quot;}, {&quot;dim-1&quot;: 2.0, &quot;dim-2&quot;: 0.0, &quot;shear&quot;: &quot;horizontal-2&quot;}, {&quot;dim-1&quot;: 10.0, &quot;dim-2&quot;: 4.0, &quot;shear&quot;: &quot;horizontal-2&quot;}]}}, {&quot;mode&quot;: &quot;vega-lite&quot;});
&lt;/script&gt;

&lt;h3 id=&quot;rotation&quot;&gt;Rotation&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Rotation&lt;/strong&gt; mappings do exactly what their name indicates: they move objects (by convection) counterclockwise in Euclidean space. For the general case in $\mathbb{R}^2$, counterclockwise of vector $\textbf{x}$ by $\theta$ radiants rotations is obtained as:&lt;/p&gt;

\[\begin{bmatrix}
\cos \theta &amp;amp; -\sin \theta \\
\sin \theta &amp;amp; \cos \theta
\end{bmatrix} \textbf{x}\]

&lt;p&gt;Again, let’s examine a couple of special cases.&lt;/p&gt;

&lt;p&gt;A $90^{\circ}$ rotation matrix in $\mathbb{R}^2$ :&lt;/p&gt;

\[\begin{bmatrix}
0 &amp;amp; -1 \\
1 &amp;amp; 0
\end{bmatrix} \textbf{x}\]

&lt;p&gt;A $180^{\circ}$ rotation matrix in $\mathbb{R}^2$:&lt;/p&gt;

\[\begin{bmatrix}
-1 &amp;amp; 0 \\
0 &amp;amp; -1
\end{bmatrix} \textbf{x}\]

&lt;p&gt;A $270^{\circ}$ rotation matrix in $\mathbb{R}^2$:&lt;/p&gt;

\[\begin{bmatrix}
0 &amp;amp; 1 \\
-1 &amp;amp; 0
\end{bmatrix} \textbf{x}\]

&lt;p&gt;Let’s rotate a vector using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;. We will define a rotation matrix $\textit{A}$, a vector $\textbf{x}$, and then plot the original and rotated vectors with Altair.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# 90-degrees roration
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
               &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 180-degrees roration
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
               &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 270-degrees roration
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
               &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;column_stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;dim-1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;dim-2&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                   &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;rotation&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;original&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;original&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                             &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;90-degrees&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;90-degrees&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                             &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;180-degrees&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;180-degrees&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                             &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;270-degrees&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;270-degrees&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;
                            &lt;span class=&quot;p&quot;&gt;]})&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }

&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;dim-1&lt;/th&gt;
      &lt;th&gt;dim-2&lt;/th&gt;
      &lt;th&gt;rotation&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;original&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;4.0&lt;/td&gt;
      &lt;td&gt;original&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;90-degrees&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;-4.0&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;90-degrees&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;180-degrees&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;-2.0&lt;/td&gt;
      &lt;td&gt;-4.0&lt;/td&gt;
      &lt;td&gt;180-degrees&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;270-degrees&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;4.0&lt;/td&gt;
      &lt;td&gt;-2.0&lt;/td&gt;
      &lt;td&gt;270-degrees&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;chart&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mark_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;dim-1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;horizontal-axis&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;dim-2&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;vertical-axis&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;rotation&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;base_coor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;5.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;5.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;chart&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div id=&quot;altair-viz-f7fb1b4711f94113ae71f3e4bf99c11e&quot;&gt;&lt;/div&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
  (function(spec, embedOpt){
    let outputDiv = document.currentScript.previousElementSibling;
    if (outputDiv.id !== &quot;altair-viz-f7fb1b4711f94113ae71f3e4bf99c11e&quot;) {
      outputDiv = document.getElementById(&quot;altair-viz-f7fb1b4711f94113ae71f3e4bf99c11e&quot;);
    }
    const paths = {
      &quot;vega&quot;: &quot;https://cdn.jsdelivr.net/npm//vega@5?noext&quot;,
      &quot;vega-lib&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-lib?noext&quot;,
      &quot;vega-lite&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext&quot;,
      &quot;vega-embed&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-embed@6?noext&quot;,
    };

    function loadScript(lib) {
      return new Promise(function(resolve, reject) {
        var s = document.createElement(&apos;script&apos;);
        s.src = paths[lib];
        s.async = true;
        s.onload = () =&gt; resolve(paths[lib]);
        s.onerror = () =&gt; reject(`Error loading script: ${paths[lib]}`);
        document.getElementsByTagName(&quot;head&quot;)[0].appendChild(s);
      });
    }

    function showError(err) {
      outputDiv.innerHTML = `&lt;div class=&quot;error&quot; style=&quot;color:red;&quot;&gt;${err}&lt;/div&gt;`;
      throw err;
    }

    function displayChart(vegaEmbed) {
      vegaEmbed(outputDiv, spec, embedOpt)
        .catch(err =&gt; showError(`Javascript Error: ${err.message}&lt;br&gt;This usually means there&apos;s a typo in your chart specification. See the javascript console for the full traceback.`));
    }

    if(typeof define === &quot;function&quot; &amp;&amp; define.amd) {
      requirejs.config({paths});
      require([&quot;vega-embed&quot;], displayChart, err =&gt; showError(`Error loading script: ${err.message}`));
    } else if (typeof vegaEmbed === &quot;function&quot;) {
      displayChart(vegaEmbed);
    } else {
      loadScript(&quot;vega&quot;)
        .then(() =&gt; loadScript(&quot;vega-lite&quot;))
        .then(() =&gt; loadScript(&quot;vega-embed&quot;))
        .catch(showError)
        .then(() =&gt; displayChart(vegaEmbed));
    }

})({&quot;usermeta&quot;: {&quot;embedOptions&quot;: {&quot;theme&quot;: &quot;dark&quot;}}, &quot;config&quot;: {&quot;view&quot;: {&quot;continuousWidth&quot;: 400, &quot;continuousHeight&quot;: 300}}, &quot;layer&quot;: [{&quot;mark&quot;: {&quot;type&quot;: &quot;line&quot;, &quot;color&quot;: &quot;white&quot;}, &quot;encoding&quot;: {&quot;x&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;horizontal&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;vertical&quot;}}}, {&quot;mark&quot;: {&quot;type&quot;: &quot;line&quot;, &quot;color&quot;: &quot;white&quot;}, &quot;encoding&quot;: {&quot;x&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;vertical&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;horizontal&quot;}}}, {&quot;data&quot;: {&quot;name&quot;: &quot;data-fdd0ac9e41bb6cb891458e6886636a2f&quot;}, &quot;mark&quot;: &quot;line&quot;, &quot;encoding&quot;: {&quot;color&quot;: {&quot;type&quot;: &quot;nominal&quot;, &quot;field&quot;: &quot;rotation&quot;}, &quot;x&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;axis&quot;: {&quot;title&quot;: &quot;horizontal-axis&quot;}, &quot;field&quot;: &quot;dim-1&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;axis&quot;: {&quot;title&quot;: &quot;vertical-axis&quot;}, &quot;field&quot;: &quot;dim-2&quot;}}}], &quot;data&quot;: {&quot;name&quot;: &quot;data-dc621955550350bfc1b0624dd9983169&quot;}, &quot;$schema&quot;: &quot;https://vega.github.io/schema/vega-lite/v4.8.1.json&quot;, &quot;datasets&quot;: {&quot;data-dc621955550350bfc1b0624dd9983169&quot;: [{&quot;horizontal&quot;: -5.0, &quot;vertical&quot;: 0.0}, {&quot;horizontal&quot;: 5.0, &quot;vertical&quot;: 0.0}], &quot;data-fdd0ac9e41bb6cb891458e6886636a2f&quot;: [{&quot;dim-1&quot;: 0.0, &quot;dim-2&quot;: 0.0, &quot;rotation&quot;: &quot;original&quot;}, {&quot;dim-1&quot;: 2.0, &quot;dim-2&quot;: 4.0, &quot;rotation&quot;: &quot;original&quot;}, {&quot;dim-1&quot;: 0.0, &quot;dim-2&quot;: 0.0, &quot;rotation&quot;: &quot;90-degrees&quot;}, {&quot;dim-1&quot;: -4.0, &quot;dim-2&quot;: 2.0, &quot;rotation&quot;: &quot;90-degrees&quot;}, {&quot;dim-1&quot;: 0.0, &quot;dim-2&quot;: 0.0, &quot;rotation&quot;: &quot;180-degrees&quot;}, {&quot;dim-1&quot;: -2.0, &quot;dim-2&quot;: -4.0, &quot;rotation&quot;: &quot;180-degrees&quot;}, {&quot;dim-1&quot;: 0.0, &quot;dim-2&quot;: 0.0, &quot;rotation&quot;: &quot;270-degrees&quot;}, {&quot;dim-1&quot;: 4.0, &quot;dim-2&quot;: -2.0, &quot;rotation&quot;: &quot;270-degrees&quot;}]}}, {&quot;mode&quot;: &quot;vega-lite&quot;});
&lt;/script&gt;

&lt;h2 id=&quot;projections&quot;&gt;Projections&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Projections&lt;/strong&gt; are a fundamental type of linear (and affine) mappings for machine learning. If you have ever heard concepts like “embeddings”, “low-dimensional representation”, or “dimensionality reduction”, they all are examples of projections. Even linear regression and principal component analysis are exemplars of projections. Thus, projections allow working with high-dimensional spaces (i.e., problems with many features or variables) more efficiently, by projecting such spaces into lower-dimensional spaces. this works because is often the case that a few dimensions contain most of the information to understand the relation between inputs and outputs. Moreover, projections can be represented as &lt;em&gt;matrices acting on vectors&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Put simply, projections are &lt;em&gt;mappings from a space onto a subpace&lt;/em&gt;, or from a set of vectors onto a subset of vectors. Additionally, projections are “idempotent”, this is, the projection has the property to be &lt;em&gt;equal to its composition with itself&lt;/em&gt;. In other words, when you wrap a projection $\phi(x) = y$ into itself as $\phi (\phi (x))$, the result does not change, i.e., $\phi (\phi (x)) = y$. Formally, for a vector space $\textit{V}$ and a vector subset $\textit{U} \subset \textit{V}$, we define a projection $\phi$ as:&lt;/p&gt;

\[\phi : \textit{V} \rightarrow \textit{U}\]

&lt;p&gt;with&lt;/p&gt;

\[\phi^2 : \phi \circ  \phi = \phi\]

&lt;p&gt;Here we are concerned with the matrix representation of projections, which receive the special name of &lt;strong&gt;projection matrices&lt;/strong&gt;, denoted as $\textit{P}_\phi$. By extension, projection matrices are also “idempotent”:&lt;/p&gt;

\[\textit{P}_\phi^2 = \textit{P}_\phi \circ  \textit{P}_\phi = \textit{P}_\phi\]

&lt;h3 id=&quot;projections-onto-lines&quot;&gt;Projections onto lines&lt;/h3&gt;

&lt;p&gt;In Freudian psychoanalysis, &lt;em&gt;projection&lt;/em&gt; is a defense mechanism of the “ego” (i.e., the sense of self), where a person denies the possession of an undesired characteristic while attributing it to someone else, i.e., “projecting” what we don’t like of us onto others.&lt;/p&gt;

&lt;p&gt;It turns out, that the concept of projection in mathematics is not that different from the Freudian one. Just make the following analogy: imagine you and foe of you are represented as vectors in a 2-dimensional cartesian plane, as $\textit{x}$ and $\textit{y}$ respectively. The way on which you would project yourself onto your foe is by tracing a perpendicular line ($\textit{z}$) from you onto him. Why perpendicular? Because this is the shortest distance between you and him, hence, the most efficient way to project yourself onto him. Now, the projection would be “how much” of yourself was “splattered” onto him, which is represented by the segment $\textit{p}$ from the origin until the point where the perpendicular line touched your foe.&lt;/p&gt;

&lt;p&gt;Now, recall that lines crossing the origin form subspaces, hence vector $\textbf{y}$ is a subspace, and that perpendicular lines form $90^{\circ}$ angles, hence the &lt;strong&gt;projection is orthogonal&lt;/strong&gt;. More formally, we can define the projection of $\textbf{x} \in \mathbb{R}^2$ onto subspace $\textit{U} \in \mathbb{R}^2$ formed by $\textbf{y}$ as:&lt;/p&gt;

\[\phi_{\textit{U}}(\textbf{x}) \in \textit{U}\]

&lt;p&gt;Where $\phi_{\textit{U}}(\textbf{x})$ must be the minimal distance between $\textbf{x}$ and $\textbf{y}$ (i.e., $\textbf{x}$ and $\textit{U}$), where distance is:&lt;/p&gt;

\[\Vert \textbf{x} - \phi_{\textit{U}}(\textbf{x}) \Vert\]

&lt;p&gt;Further, the resulting projection $\phi_{\textit{U}}(\textbf{x})$ must lie in the span of $\textit{U}$. Therefore, we can conclude that $\phi_{\textit{U}}(\textbf{x}) = \alpha \textbf{y}$, where alpha is a scalar in $\mathbb{R}$.&lt;/p&gt;

&lt;p&gt;The formula to find the orthogonal projection (I’m skipping the derivation on purpose) $\phi_{\textit{U}}(\textbf{x})$ is:&lt;/p&gt;

\[\phi_{\textit{U}}(\textbf{x}) = \alpha \textbf{y} =
\frac{\langle \textbf{x,y} \rangle}{\Vert \textbf{y} \Vert ^2} \textbf{y} =
\frac{\textbf{y}^T\cdot \textbf{x}}{\Vert \textbf{y} \Vert ^2} \textbf{y}\]

&lt;p&gt;In words: we take the dot product between $\textbf{x}$ and $\textbf{y}$, divide by the norm of $\textbf{y}$, and multiply by $\textbf{y}$. In this case, $\textbf{y}$ is also known as a basis vector, so we can say that $\textbf{x}$ is projected onto the basis $\textbf{y}$.&lt;/p&gt;

&lt;p&gt;Now, we want to express projections as matrices, i.e., as the matrix vector product $\textit{P}&lt;em&gt;\phi \textbf{x}$. For this, recall that matrix-scalar multiplication is _commutative&lt;/em&gt;, hence we can perform a little of algeabric manipulation to find:&lt;/p&gt;

\[\phi_{\textit{U}}(\textbf{x}) =
\textbf{y} \alpha = \textbf{y} \frac{\textbf{y}^T\cdot \textbf{x}}{\Vert \textbf{y} \Vert ^2} =
\frac{\textbf{y} \cdot \textbf{y}^T}{\Vert \textbf{y} \Vert ^2} \textbf{x}\]

&lt;p&gt;In this form, we can indeed express the projection as a matrix-vector multiplication, because
$\textbf{y} \cdot \textbf{y}^T$ results in a symmetrix matrix, and $\Vert \textbf{y} \Vert ^2$ is a scalar, which means that it can be expressed as a matrix:&lt;/p&gt;

\[\textit{P}_\phi =  \frac{\textbf{y} \cdot \textbf{y}^T}{\Vert \textbf{y} \Vert ^2}\]

&lt;p&gt;In sum, the matrix $\textit{P}_\phi$ will project any vector onto $\textbf{y}$.&lt;/p&gt;

&lt;p&gt;Let’s use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; to find the projection $\textit{P}_\phi$ from $\textbf{x}$ onto a basis vector $\textbf{y}$.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# base vector
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;P&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Projection matrix for y:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Projection matrix for y:
[[0.69230769 0.46153846]
 [0.46153846 0.30769231]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;P&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Projection from x onto y:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Projection from x onto y:
[[2.07692308]
 [1.38461538]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Let’s plot the vectors to make things clearer&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# origin coordinate space
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;o&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;column_stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;dim-1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;dim-2&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                   &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;x-vector&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;x-vector&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                              &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;y-base-vector&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;y-base-vector&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                              &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;z-projection&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;z-projection&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                              &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;orthogonal-vector&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;orthogonal-vector&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                  &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;size-line&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]})&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }

&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;dim-1&lt;/th&gt;
      &lt;th&gt;dim-2&lt;/th&gt;
      &lt;th&gt;vector&lt;/th&gt;
      &lt;th&gt;size-line&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;x-vector&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;3.000000&lt;/td&gt;
      &lt;td&gt;x-vector&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;y-base-vector&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;3.000000&lt;/td&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;y-base-vector&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;0.000000&lt;/td&gt;
      &lt;td&gt;z-projection&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;2.076923&lt;/td&gt;
      &lt;td&gt;1.384615&lt;/td&gt;
      &lt;td&gt;z-projection&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;3.000000&lt;/td&gt;
      &lt;td&gt;orthogonal-vector&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;2.076923&lt;/td&gt;
      &lt;td&gt;1.384615&lt;/td&gt;
      &lt;td&gt;orthogonal-vector&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;chart&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mark_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;dim-1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;horizontal-axis&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;dim-2&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;vertical-axis&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;strokeDash&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;size-line&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;base_coor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;chart&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div id=&quot;altair-viz-f8f4f43ba87149c9a10704ec132e0004&quot;&gt;&lt;/div&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
  (function(spec, embedOpt){
    let outputDiv = document.currentScript.previousElementSibling;
    if (outputDiv.id !== &quot;altair-viz-f8f4f43ba87149c9a10704ec132e0004&quot;) {
      outputDiv = document.getElementById(&quot;altair-viz-f8f4f43ba87149c9a10704ec132e0004&quot;);
    }
    const paths = {
      &quot;vega&quot;: &quot;https://cdn.jsdelivr.net/npm//vega@5?noext&quot;,
      &quot;vega-lib&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-lib?noext&quot;,
      &quot;vega-lite&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext&quot;,
      &quot;vega-embed&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-embed@6?noext&quot;,
    };

    function loadScript(lib) {
      return new Promise(function(resolve, reject) {
        var s = document.createElement(&apos;script&apos;);
        s.src = paths[lib];
        s.async = true;
        s.onload = () =&gt; resolve(paths[lib]);
        s.onerror = () =&gt; reject(`Error loading script: ${paths[lib]}`);
        document.getElementsByTagName(&quot;head&quot;)[0].appendChild(s);
      });
    }

    function showError(err) {
      outputDiv.innerHTML = `&lt;div class=&quot;error&quot; style=&quot;color:red;&quot;&gt;${err}&lt;/div&gt;`;
      throw err;
    }

    function displayChart(vegaEmbed) {
      vegaEmbed(outputDiv, spec, embedOpt)
        .catch(err =&gt; showError(`Javascript Error: ${err.message}&lt;br&gt;This usually means there&apos;s a typo in your chart specification. See the javascript console for the full traceback.`));
    }

    if(typeof define === &quot;function&quot; &amp;&amp; define.amd) {
      requirejs.config({paths});
      require([&quot;vega-embed&quot;], displayChart, err =&gt; showError(`Error loading script: ${err.message}`));
    } else if (typeof vegaEmbed === &quot;function&quot;) {
      displayChart(vegaEmbed);
    } else {
      loadScript(&quot;vega&quot;)
        .then(() =&gt; loadScript(&quot;vega-lite&quot;))
        .then(() =&gt; loadScript(&quot;vega-embed&quot;))
        .catch(showError)
        .then(() =&gt; displayChart(vegaEmbed));
    }

})({&quot;usermeta&quot;: {&quot;embedOptions&quot;: {&quot;theme&quot;: &quot;dark&quot;}}, &quot;config&quot;: {&quot;view&quot;: {&quot;continuousWidth&quot;: 400, &quot;continuousHeight&quot;: 300}}, &quot;layer&quot;: [{&quot;mark&quot;: {&quot;type&quot;: &quot;line&quot;, &quot;color&quot;: &quot;white&quot;}, &quot;encoding&quot;: {&quot;x&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;horizontal&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;vertical&quot;}}}, {&quot;mark&quot;: {&quot;type&quot;: &quot;line&quot;, &quot;color&quot;: &quot;white&quot;}, &quot;encoding&quot;: {&quot;x&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;vertical&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;horizontal&quot;}}}, {&quot;data&quot;: {&quot;name&quot;: &quot;data-fae3a4238031b3bda30da25b0bfbb236&quot;}, &quot;mark&quot;: &quot;line&quot;, &quot;encoding&quot;: {&quot;color&quot;: {&quot;type&quot;: &quot;nominal&quot;, &quot;field&quot;: &quot;vector&quot;}, &quot;size&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;size-line&quot;}, &quot;strokeDash&quot;: {&quot;type&quot;: &quot;nominal&quot;, &quot;field&quot;: &quot;vector&quot;}, &quot;x&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;axis&quot;: {&quot;title&quot;: &quot;horizontal-axis&quot;}, &quot;field&quot;: &quot;dim-1&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;axis&quot;: {&quot;title&quot;: &quot;vertical-axis&quot;}, &quot;field&quot;: &quot;dim-2&quot;}}}], &quot;data&quot;: {&quot;name&quot;: &quot;data-e28b18115621143603ed81a2a2f08951&quot;}, &quot;$schema&quot;: &quot;https://vega.github.io/schema/vega-lite/v4.8.1.json&quot;, &quot;datasets&quot;: {&quot;data-e28b18115621143603ed81a2a2f08951&quot;: [{&quot;horizontal&quot;: -1.0, &quot;vertical&quot;: 0.0}, {&quot;horizontal&quot;: 4.0, &quot;vertical&quot;: 0.0}], &quot;data-fae3a4238031b3bda30da25b0bfbb236&quot;: [{&quot;dim-1&quot;: 0.0, &quot;dim-2&quot;: 0.0, &quot;vector&quot;: &quot;x-vector&quot;, &quot;size-line&quot;: 2}, {&quot;dim-1&quot;: 1.0, &quot;dim-2&quot;: 3.0, &quot;vector&quot;: &quot;x-vector&quot;, &quot;size-line&quot;: 2}, {&quot;dim-1&quot;: 0.0, &quot;dim-2&quot;: 0.0, &quot;vector&quot;: &quot;y-base-vector&quot;, &quot;size-line&quot;: 2}, {&quot;dim-1&quot;: 3.0, &quot;dim-2&quot;: 2.0, &quot;vector&quot;: &quot;y-base-vector&quot;, &quot;size-line&quot;: 2}, {&quot;dim-1&quot;: 0.0, &quot;dim-2&quot;: 0.0, &quot;vector&quot;: &quot;z-projection&quot;, &quot;size-line&quot;: 4}, {&quot;dim-1&quot;: 2.0769230769230766, &quot;dim-2&quot;: 1.3846153846153846, &quot;vector&quot;: &quot;z-projection&quot;, &quot;size-line&quot;: 4}, {&quot;dim-1&quot;: 1.0, &quot;dim-2&quot;: 3.0, &quot;vector&quot;: &quot;orthogonal-vector&quot;, &quot;size-line&quot;: 2}, {&quot;dim-1&quot;: 2.0769230769230766, &quot;dim-2&quot;: 1.3846153846153846, &quot;vector&quot;: &quot;orthogonal-vector&quot;, &quot;size-line&quot;: 2}]}}, {&quot;mode&quot;: &quot;vega-lite&quot;});
&lt;/script&gt;

&lt;h3 id=&quot;projections-onto-general-subspaces&quot;&gt;Projections onto general subspaces&lt;/h3&gt;

&lt;p&gt;From the previous section, we learned that the projection matrix for the one-dimensional project of $\textbf{x}$ onto $\textit{U}$ (i.e. $\textbf{y}$) $\phi_{\textit{U}}(\textbf{x})$ can be expressed as:&lt;/p&gt;

\[\textit{P}_\phi = \alpha \textbf{y} = \frac{\textbf{y} \cdot \textbf{y}^T}{\Vert \textbf{y} \Vert ^2}\]

&lt;p&gt;Which implies that we the projection is entirely defined in terms of the basis subspace. Now, we are interested in projections for the general case, this is, for &lt;strong&gt;set of basis vectors $\textbf{y}_1, \cdots, \textbf{y}_m$&lt;/strong&gt;. By extension, we can define such projection as:&lt;/p&gt;

\[\phi_{\textit{U}}(\textbf{x}) = \sum_{i=1}^m \textbf{y}_i \alpha = \textit{Y} \alpha\]

&lt;p&gt;Where $\textit{Y}$ is the matrix of basis vectors.&lt;/p&gt;

&lt;p&gt;Nothing fancy going on here: we just need to take the sum for the product between each basis vector and $\alpha$. As with the one-dimensional case, we want the projection to be the minimal distance from $\textbf{x}$ onto $\textit{Y}$, which we know implies orthogonal lines (or hyperplanes) connecting $\textbf{x}$ with $\textit{Y}$. The condition for orthogonality (again, I’m skipping the derivation on purpose) here equals:&lt;/p&gt;

\[\textit{Y}^T (\textbf{x} - \textit{Y} \alpha) = 0\]

&lt;p&gt;Now, recall what we really want is to find $\alpha$ (we know $\textit{Y}$ already). Therefore, with a bit of algeabric manipulation we can clear the expression above as:&lt;/p&gt;

\[\alpha = (\textit{Y}^T \textit{Y})^{-1} \textit{Y}^T  \textbf{x}\]

&lt;p&gt;Such expression is known as the &lt;em&gt;pseudo-inverse&lt;/em&gt; or &lt;em&gt;Moore–Penrose inverse&lt;/em&gt; of $\textit{Y}$. To work, it requires $\textit{Y}$ to be full rank (i.e., independent columns, which should be the case for basis). It can be used to solve linear regression problems, although you’ll probably find the notation flipped as: $\alpha = (\textit{X}^T \textit{X})^{-1} \textit{X}^T  \textbf{y}$ (my bad choice of notation!).&lt;/p&gt;

&lt;p&gt;Going back to our Freudian projection analogy, this is like a group of people projecting themselves onto someone else, with that person representing a rough approximation of the character of the group.&lt;/p&gt;

&lt;h3 id=&quot;projections-as-approximate-solutions-to-systems-of-linear-equations&quot;&gt;Projections as approximate solutions to systems of linear equations&lt;/h3&gt;

&lt;p&gt;Machine learning prediction problems usually require to find a solution to systems of linear equations of the form:&lt;/p&gt;

\[\textit{A}\textbf{x} = \textbf{y}\]

&lt;p&gt;In other words, to represent $\textbf{y}$ as linear combinations of the columns of $\textit{A}$. Unfortunately, in most cases $\textbf{y}$ is not in the column space of $\textit{A}$, i.e., &lt;em&gt;there is no way to find a linear combination of its columns to obtain the target&lt;/em&gt; $\textbf{y}$. In such cases, we can use orthogonal projections to find &lt;strong&gt;approximate solutions&lt;/strong&gt; to the system. We usually denote approximated solutions for systems of linear equations as $\hat{\textbf{y}}$. Now, $\hat{\textbf{y}}$ will be in the span of the columns of $\textit{A}$ and will be the result of projecting $\textbf{y}$ onto the subspace of the columns of $\textit{A}$. That solution will be the best (closest) approximation of $\textbf{y}$ given the span of the columns of $\textit{A}$. In sum: &lt;strong&gt;the approximated solution $\hat{\textbf{y}}$ is the orthogonal projection of $\textbf{y}$ onto $\textit{A}$&lt;/strong&gt;.&lt;/p&gt;

&lt;h1 id=&quot;matrix-decompositions&quot;&gt;Matrix decompositions&lt;/h1&gt;

&lt;p&gt;In the Japanese manga/anime series &lt;em&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Fullmetal_Alchemist&quot;&gt;Fullmetal Alchemist&lt;/a&gt;&lt;/em&gt;, &lt;a href=&quot;https://fma.fandom.com/wiki/Alchemy&quot;&gt;Alchemy&lt;/a&gt; is understood as the metaphysical science of altering objects by manipulating its natural components, act known as &lt;em&gt;Transmutation&lt;/em&gt; (Rensei). There are three steps to Transmutation: (1) &lt;em&gt;Comprehension&lt;/em&gt;, to understand the atomic structure and properties of the object, (2) &lt;em&gt;Deconstruction&lt;/em&gt;, to break down the structure of the object into its fundamental elements (3) &lt;em&gt;Reconstruction&lt;/em&gt;, to use the natural flow of energy to reform the object into a new shape.&lt;/p&gt;

&lt;p&gt;Metaphorically speaking, we can understand linear combinations and matrix decompositions in analogy to &lt;em&gt;Transmutation&lt;/em&gt;. &lt;strong&gt;Matrix decomposition&lt;/strong&gt; is essentially about to break down a matrix into simpler “elements” or matrices (deconstruction), which allows us to better understand its fundamental structure (comprehension). Linear combinations are essentially about taking the fundamental elements of a matrix (i.e., set of vectors) to generate a new object.&lt;/p&gt;

&lt;p&gt;Matrix decomposition is also known as &lt;strong&gt;matrix factorization&lt;/strong&gt;, in reference the fact that matrices can be broken down into simpler matrices, more on less in the same way that Prime factorization breaks down large numbers into simpler primes (e.g., $112 = 2 \times 2 \times 2 \times 2 \times 7$).&lt;/p&gt;

&lt;p&gt;There are several important applications of matrix factorization in machine learning: clustering, recommender systems, dimensionality reduction, topic modeling, and others. In what follows I’ll cover a selection of several basic and common matrix decomposition techniques.&lt;/p&gt;

&lt;h2 id=&quot;lu-decomposition&quot;&gt;LU decomposition&lt;/h2&gt;

&lt;p&gt;There are multiple ways to decompose or factorize matrices. One of the simplest ways is by decomposition a matrix into a &lt;strong&gt;lower triangular matrix&lt;/strong&gt; and an &lt;strong&gt;upper triangular matrix&lt;/strong&gt;, the so-called &lt;strong&gt;LU or Lower-Upper decomposition&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;LU decomposition is of great interest to us since it’s one of the methods computers use to solve linear algebra problems. In particular, LU decomposition is a way to represent &lt;strong&gt;Gaussian Elimination in numerical linear algebra&lt;/strong&gt;. LU decomposition is flexible as it can be obtained from noninvertible or singular matrices, and from non-square matrices.&lt;/p&gt;

&lt;p&gt;The general expression for LU decomposition is:&lt;/p&gt;

\[\textit{A} = \textit{L}\textit{U}\]

&lt;p&gt;Meaning that $\textit{A}$ can be represented as the product of the lower triangular matrix $\textit{L}$ and upper triangular matrix $\textit{U}$. In the next sections, we explain the mechanics of the LU decomposition.&lt;/p&gt;

&lt;h3 id=&quot;elementary-matrices&quot;&gt;Elementary matrices&lt;/h3&gt;

&lt;p&gt;Our first step to approach LU decomposition is to introduce &lt;strong&gt;elementary matrices&lt;/strong&gt;. When considering matrices as functions or mappings, we can associate special meaning to a couple of basic or “elementary” operations performed by matrices. Our starting point is the &lt;strong&gt;identity matrix&lt;/strong&gt;, for instance:&lt;/p&gt;

\[\textit{I} =
\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 1 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}\]

&lt;p&gt;As we saw before, the identity matrix does not change the values of another matrix under multiplication:&lt;/p&gt;

\[\textit{A} \textit{I} = \textit{I} \textit{A} = \textit{A}\]

&lt;p&gt;Because it is essentially saying: &lt;em&gt;give me $1$ of each column of the matrix&lt;/em&gt;, i.e., return the original matrix. Now, consider the following matrix:&lt;/p&gt;

\[\textit{I}_2 =
\begin{bmatrix}
2 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 1 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}\]

&lt;p&gt;The only thing we did to the $\textit{I}$ to obtain $\textit{I}_2$ was to multiply the first row by $2$. This can be considered an elementary operation. Here is another example:&lt;/p&gt;

\[\textit{I}_3 =
\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 \\
2 &amp;amp; 1 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}\]

&lt;p&gt;Clearly, we can’t obtain $\textit{I}_3$ by multiplication only. From a column perspective, what we did was to add $2$ times the second column to the first column. Alternatively, from a row perspective, we can say we added $2$ times the first row to the second row.&lt;/p&gt;

&lt;p&gt;One last example:&lt;/p&gt;

\[\textit{I}_4 =
\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 1 &amp;amp; 0 \\
0 &amp;amp; -3 &amp;amp; 1
\end{bmatrix}\]

&lt;p&gt;From the column perspective, we added $-3$ times the third column to the second column. From the row perspective, we added $-3$ times the second row to the third row.&lt;/p&gt;

&lt;p&gt;You can probably see the pattern by now: by performing simple or “elementary” column or row operations, this is, &lt;em&gt;multiplication&lt;/em&gt; and &lt;em&gt;addition&lt;/em&gt;, we can obtain any lower triangular matrix. This type of matrices are what we call &lt;strong&gt;elementary matrices&lt;/strong&gt;. In a way, we can say elementary matrices “encode” fundamental column and row operations. To see this, consider the following generic matrix:&lt;/p&gt;

\[\textit{A} =
\begin{bmatrix}
a &amp;amp; b &amp;amp; c \\
d &amp;amp; e &amp;amp; f \\
g &amp;amp; h &amp;amp; i
\end{bmatrix}\]

&lt;p&gt;Let’s what happens when we multiply $\textit{A}\textit{I}_3$:&lt;/p&gt;

\[\textit{A}\textit{I}_3 =
\begin{bmatrix}
a &amp;amp; b &amp;amp; c \\
d &amp;amp; e &amp;amp; f \\
g &amp;amp; h &amp;amp; i
\end{bmatrix}
\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 \\
2 &amp;amp; 1 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix} =
\begin{bmatrix}
a+2b &amp;amp; b &amp;amp; c \\
d+2e &amp;amp; e &amp;amp; f \\
g+2h &amp;amp; h &amp;amp; i
\end{bmatrix}\]

&lt;p&gt;The result of $\textit{A}\textit{I}_3$ reflects the same elementary operations we performed on $\textit{I}$ to obtain $\textit{I}_3$ from the &lt;strong&gt;column perspective&lt;/strong&gt;: to add $2$ times the second column to the first one.&lt;/p&gt;

&lt;p&gt;Now consider what happens when we multiply from the left:&lt;/p&gt;

\[\textit{I}_3 \textit{A} =
\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 \\
2 &amp;amp; 1 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}
\begin{bmatrix}
a &amp;amp; b &amp;amp; c \\
d &amp;amp; e &amp;amp; f \\
g &amp;amp; h &amp;amp; i
\end{bmatrix}
 =
\begin{bmatrix}
a &amp;amp; b &amp;amp; c \\
d+2a &amp;amp; e+2b &amp;amp; f+2c \\
g &amp;amp; h &amp;amp; i
\end{bmatrix}\]

&lt;p&gt;Now we obtain the same elementary operations we performed on $\textit{I}$ to obtain $\textit{I}_3$ from the &lt;strong&gt;row perspective&lt;/strong&gt;: to add $2$ times the first row to the second one.&lt;/p&gt;

&lt;h3 id=&quot;the-inverse-of-elementary-matrices&quot;&gt;The inverse of elementary matrices&lt;/h3&gt;

&lt;p&gt;A nice property of elementary matrices, is that the inverse is simply the opposite operation. For instance, the inverse of $\textit{I}_2$ is:&lt;/p&gt;

\[\begin{bmatrix}
\frac{1}{2} &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 1 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}\]

&lt;p&gt;This is because instead of multiplying the first row of $\textit{I}$ by $2$, we divide it by $2$. Similarly, the inverse of $\textit{I}_3$ is:&lt;/p&gt;

\[\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 \\
-2 &amp;amp; 1 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}\]

&lt;p&gt;Again, instead of adding $2$, we add $-2$ (or substract $2$). Finally, the inverse of $\textit{I}_4$ is:&lt;/p&gt;

\[\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 1 &amp;amp; 0 \\
0 &amp;amp; 3 &amp;amp; 1
\end{bmatrix}\]

&lt;p&gt;The reason we care about elementary matrices and its inverse is that it will be fundamental to understand LU decomposition.&lt;/p&gt;

&lt;h3 id=&quot;lu-decomposition-as-gaussian-elimination&quot;&gt;LU decomposition as Gaussian Elimination&lt;/h3&gt;

&lt;p&gt;Let’s briefly recall Gaussian Elimination: it’s an robust algorithm to solve systems of linear equations, by sequentially applying three elementary transformations:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Addition and subtraction of two equations (rows)&lt;/li&gt;
  &lt;li&gt;Multiplication of an equation (rows) by a number&lt;/li&gt;
  &lt;li&gt;Switching equations (rows)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Gaussian Elimination will reduce matrices to its &lt;strong&gt;row echelon form&lt;/strong&gt;, which is an upper triangular matrix, with zero rows at the bottom, and zeros below the pivot for each column.&lt;/p&gt;

&lt;p&gt;It turns out, there is a clever way to organize the steps from Gaussian Elimination: with &lt;strong&gt;elementary matrices&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Consider the following matrix $\textit{A}$:&lt;/p&gt;

\[\begin{bmatrix}
1 &amp;amp; 3 &amp;amp; 5  \\
2 &amp;amp; 2 &amp;amp; -1 \\
1 &amp;amp; 3 &amp;amp; 2
\end{bmatrix}\]

&lt;p&gt;The first step consist of substracting two times row 1 from row 1. Before, we represented this operation as $R_2 - 2R_1$, and write down the result, which is:&lt;/p&gt;

\[\begin{bmatrix}
1 &amp;amp; 3 &amp;amp; 5  \\
0 &amp;amp; -4 &amp;amp; -11 \\
1 &amp;amp; 3 &amp;amp; 2
\end{bmatrix}\]

&lt;p&gt;Alternatively, as we learned in the previous section, &lt;em&gt;we can represent row operations as multiplication by elementary matrices&lt;/em&gt;, to obtain the same result. Since we want to substract $2$ times the first row from the second, we need to (1) multiply from the left, and (2) add a $-2$ to the first element of the second row:&lt;/p&gt;

\[\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 \\
-2 &amp;amp; 1 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}
\begin{bmatrix}
1 &amp;amp; 3 &amp;amp; 5 \\
2 &amp;amp; 2 &amp;amp; -1 \\
1 &amp;amp; 3 &amp;amp; 2
\end{bmatrix} =
\begin{bmatrix}
1 &amp;amp; 3 &amp;amp; 5  \\
0 &amp;amp; -4 &amp;amp; -11 \\
1 &amp;amp; 3 &amp;amp; 2
\end{bmatrix}\]

&lt;p&gt;You don’t have to believe me. Let’s confirm this is correct with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;l1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
               &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
               &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;l1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([[  1,   3,   5],
       [  0,  -4, -11],
       [  1,   3,   2]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As you can see, the result is exactly what we obtained before by $R_2 - 2R_1$. But, we are not done. We still need to get rid of the $1$ and $3$ in the third row. For this, we would normally do $R_3 - R_1$ to obtain:&lt;/p&gt;

\[\begin{bmatrix}
1 &amp;amp; 3 &amp;amp; 5  \\
0 &amp;amp; -4 &amp;amp; -11 \\
0 &amp;amp; 0 &amp;amp; -3
\end{bmatrix}\]

&lt;p&gt;Again, we can encode this using elementary matrices as:&lt;/p&gt;

\[\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 \\
-2 &amp;amp; 1 &amp;amp; 0 \\
-1 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}
\begin{bmatrix}
1 &amp;amp; 3 &amp;amp; 5 \\
2 &amp;amp; 2 &amp;amp; -1 \\
1 &amp;amp; 3 &amp;amp; 2
\end{bmatrix} =
\begin{bmatrix}
1 &amp;amp; 3 &amp;amp; 5  \\
0 &amp;amp; -4 &amp;amp; -11 \\
0 &amp;amp; 0 &amp;amp; -3
\end{bmatrix}\]

&lt;p&gt;Once again, let’s confirm this with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;l2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
               &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
               &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;l2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([[  1,   3,   5],
       [  0,  -4, -11],
       [  0,   0,  -3]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Indeed, the result is correct. At this point, we have reduced $\textit{A}$ to its &lt;strong&gt;row echelon form&lt;/strong&gt;. We will call $\textit{U}$ to the resulting matrix from $\textit{l} \textit{A}$, as it is an &lt;em&gt;upper triangular matrix&lt;/em&gt;. Hence, we arrived to the identity:&lt;/p&gt;

\[\textit{l} \textit{A} = \textit{U}\]

&lt;p&gt;This is not quite LU decomposition. To get there, we just need to multiply both sides of the equality by the inverse of $\textit{l}$, that we will call $\textit{L}$, which yields:&lt;/p&gt;

\[\textit{A} = \textit{L} \textit{U}\]

&lt;p&gt;There you go: we arrived to the LU decomposition expression. As a final note, recall that the inverse of $\textit{l}$ is:&lt;/p&gt;

\[\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 \\
2 &amp;amp; 1 &amp;amp; 0 \\
1 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}\]

&lt;p&gt;Let’s confirm with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; this works, by multiplying $\textit{L}$ by $\textit{U}$:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# inverse of l
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
               &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
               &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# upper triangular resulting from Gaussian Elimination
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;U&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;U&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([[ 1,  3,  5],
       [ 2,  2, -1],
       [ 1,  3,  2]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Indeed, we recover $\textit{A}$ by multiplying $\textit{L}\textit{U}$.&lt;/p&gt;

&lt;h3 id=&quot;lu-decomposition-with-pivoting&quot;&gt;LU decomposition with pivoting&lt;/h3&gt;

&lt;p&gt;If you recall the three elementary operations allowed in Gaussian Elimination, we had: (1) multiplication, (2) addition, (3) switching. At this point, we haven’t seen switching with LU decomposition. It turns out, that LU decomposition does not work when switching or permutations of rows are required to solve a system of linear equations. Further, even when pivoting is not required to solve a system, the numerical stability of Gaussian Elimination when implemented in computers is problematic, and pivoting helps to tackle that issue as well.&lt;/p&gt;

&lt;p&gt;Let’s see a simple example of pivoting. Consider the following matrix $\textit{A}$:&lt;/p&gt;

\[\begin{bmatrix}
0 &amp;amp; 1 \\
1 &amp;amp; 1
\end{bmatrix}\]

&lt;p&gt;In this case, we can’t get rid of the first $1$ in the second column by substraction. If we do that, we obtain:&lt;/p&gt;

\[\begin{bmatrix}
0 &amp;amp; 1 \\
1 &amp;amp; 0
\end{bmatrix}\]

&lt;p&gt;Which is the opposite of what we want. A simple way to fix this is by switching rows 1 and 2 as:&lt;/p&gt;

\[\begin{bmatrix}
1 &amp;amp; 1 \\
0 &amp;amp; 1
\end{bmatrix}\]

&lt;p&gt;And then substracting row 1 from row 2 to obtain:&lt;/p&gt;

\[\begin{bmatrix}
1 &amp;amp; 0 \\
0 &amp;amp; 1
\end{bmatrix}\]

&lt;p&gt;Bam! Problem fixed. Now, as with multiplication and addition, we can &lt;strong&gt;represent permutations with matrices&lt;/strong&gt; as well. In particular, by using &lt;strong&gt;permutation matrices&lt;/strong&gt;. For our previous example, we can do:&lt;/p&gt;

\[\textit{P}\textit{A}=
\begin{bmatrix}
0 &amp;amp; 1 \\
1 &amp;amp; 0
\end{bmatrix}
\begin{bmatrix}
0 &amp;amp; 1 \\
1 &amp;amp; 1
\end{bmatrix} =
\begin{bmatrix}
1 &amp;amp; 1 \\
0 &amp;amp; 1
\end{bmatrix}\]

&lt;p&gt;Let’s confirm this is correct with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;P&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;P&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([[1, 1],
       [0, 1]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It works. Now we can put all the pieces together and decompose $\textit{A}$ by using the following expression:&lt;/p&gt;

\[\textit{P}\textit{A} = \textit{L} \textit{U}\]

&lt;p&gt;This is known as LU decomposition with pivoting. An alternative expression of the same decomposition is:&lt;/p&gt;

\[\textit{A} = \textit{L} \textit{U} \textit{P}\]

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Python&lt;/code&gt;, we can use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SciPy&lt;/code&gt; to perform LUP decomposition by using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;linalg.lu&lt;/code&gt; method. Let’s decompose a larger matrix to make things more interesting.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scipy.linalg&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lu&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;U&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;lu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Pivot matrix:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Pivot matrix:
[[0. 0. 0. 1.]
 [0. 0. 1. 0.]
 [1. 0. 0. 0.]
 [0. 1. 0. 0.]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Lower triangular matrix:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Lower triangular matrix:
[[ 1.    0.    0.    0.  ]
 [ 0.75  1.    0.    0.  ]
 [ 0.5  -0.29  1.    0.  ]
 [ 0.25 -0.43  0.33  1.  ]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Upper triangular matrix:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;U&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Upper triangular matrix:
[[ 8.    7.    9.    5.  ]
 [ 0.    1.75  2.25  4.25]
 [ 0.    0.   -0.86 -0.29]
 [ 0.    0.    0.    0.67]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can confirm the decomposition is correct by multiplying the obtained matrices&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A_recover&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;P&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;U&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;PLU multiplicatin:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A_recover&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;PLU multiplicatin:
[[2 1 1 0]
 [4 3 3 1]
 [8 7 9 5]
 [6 7 9 8]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We recover $\textit{A}$ perfectly.&lt;/p&gt;

&lt;h2 id=&quot;qr-decomposition&quot;&gt;QR decomposition&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;QR decomposition&lt;/strong&gt; or &lt;strong&gt;QR factorization&lt;/strong&gt;, is another very relevant decomposition in the context of numerical linear algebra. As with LU decomposition, It can be used to solve systems of linear equations like least square problems and to find eigenvalues of a general matrix.&lt;/p&gt;

&lt;p&gt;QR decomposition works by decomposing $\textit{A}$ into an orthogonal matrix $\textit{Q}$, and a upper traingular matrix $\textit{R}$ as:&lt;/p&gt;

\[\textit{A} = \textit{Q}\textit{R}\]

&lt;p&gt;Next, we will review a few concepts to properly explain QR decomposition.&lt;/p&gt;

&lt;h3 id=&quot;orthonormal-basis&quot;&gt;Orthonormal basis&lt;/h3&gt;

&lt;p&gt;In previous sections we learned about &lt;em&gt;basis&lt;/em&gt; and &lt;em&gt;orthogonal basis&lt;/em&gt;. Specifically, we said that a set of $n$ linearly independent column vectors with $n$ elements forms a &lt;strong&gt;basis&lt;/strong&gt;. We also said that a pair of vectors $\bf{x}$ and $\bf{y}$ are &lt;strong&gt;orthogonal&lt;/strong&gt; if their inner product is zero, $\langle x,y \rangle = 0$ or $\textbf{x}^T \textbf{y} = 0$. Consequently, &lt;em&gt;a set of orthogonal vectors form an orthogonal basis for a matrix $\textit{A}$ and for the vector space spanned by such matrix&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;To go from orthogonal basis vectors to &lt;strong&gt;orthonomal basis vectors&lt;/strong&gt;, we just need to divide each vector by its lenght or norm. When we divide a basis vector by its norm we obtain a &lt;strong&gt;unit basis vector&lt;/strong&gt;. More formally, a set of vectors $\textbf{x}_1, \cdots,\textbf{x}_n$ is orthonormal if:&lt;/p&gt;

\[\textbf{x}_i^T \textbf{x}_j=
\begin{cases}
    0, &amp;amp; \text{when} &amp;amp; i\ne j &amp;amp; \text{orthogonal vectors}\\
    1, &amp;amp; \text{when} &amp;amp; i = j &amp;amp; \text{unit vectors}
\end{cases}\]

&lt;p&gt;In words: when we take the inner product of a pair of orthogonal vectors, it results in $0$; when we take the inner product of a vector with itself, it results in $1$.&lt;/p&gt;

&lt;p&gt;For instance, consider $\textbf{x}$ and $\textbf{y}$:&lt;/p&gt;

\[\textbf{x} =
\begin{bmatrix}
3 \\
4 \\
0
\end{bmatrix}
\textbf{y} =
\begin{bmatrix}
-4 \\
3 \\
2
\end{bmatrix}\]

&lt;p&gt;To obtain the normalized version of $\textbf{x}$ or $\textbf{y}$, we divide by its Euclidean norm as:&lt;/p&gt;

\[\hat{\textbf{x}} = \frac{\textbf{x}}{\Vert \textbf{x} \Vert}\]

&lt;p&gt;We add a “hat” to the normalized vector to distinguish it from the un-normalized version.&lt;/p&gt;

&lt;p&gt;Let’s try an example with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;. I’ll define vectors $\textbf{x},\textbf{y} \in \mathbb{R}^3$, compute its Euclidean norm, and then perform element-wise division $\frac{\textbf{x}}{\Vert \textbf{x} \Vert}$:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# euclidean norm of x and y
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_norm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_norm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# normalized x or unit vector
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_unit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_unit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Euclidean norm of x:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_norm&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Euclidean norm of y:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_norm&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Normalized x:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_unit&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Normalized y:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_unit&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Euclidean norm of x:
5.0

Euclidean norm of y:
5.385164807134504

Normalized x:
[[0.6]
 [0.8]
 [0. ]]

Normalized y:
[[-0.74278135]
 [ 0.55708601]
 [ 0.37139068]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can confirm that the Euclidean norm of the normalized versions of $\hat{\textbf{x}}$ and $\hat{\textbf{y}}$ equals $1$ by:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Euclidean norm of normalized x:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_unit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Euclidean norm of normalized y:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_unit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Euclidean norm of normalized x:
1.0

Euclidean norm of normalized y:
1.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Taking $\hat{\textbf{x}}$ and $\hat{\textbf{y}}$ as a set, we can confirm the conditions for the definition of orthonormal vectors are correct.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Inner product normalized vectors:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_unit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_unit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Inner product normalized x with itself:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_unit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_unit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Inner product normalized y with itself:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_unit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_unit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Inner product normalized vectors:
[[-0.]]

Inner product normalized x with itself:
[[1.]]

Inner product normalized y with itself:
[[1.]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Sets of vectors can be represented as matrices. &lt;strong&gt;We denote as $\textit{Q}$ the special case of a matrix composed of orthonormal vectors&lt;/strong&gt;. The same properties we defined for sets of vectors hold when represented in matrix form.&lt;/p&gt;

&lt;h3 id=&quot;orthonormal-basis-transpose&quot;&gt;Orthonormal basis transpose&lt;/h3&gt;

&lt;p&gt;A nice property of $\textit{Q}$ is that &lt;em&gt;the matrix product with its transpose equals the identity&lt;/em&gt;:&lt;/p&gt;

\[\textit{Q}^T \textit{Q}= \textit{I}\]

&lt;p&gt;This is true even when $\textit{Q}$ is not square. Let’s see this with the $\textit{Q} \in \mathbb{R}^{3 \times 3}$ orthonormal matrix resulting from stacking $\hat{\textbf{x}}$ and $\hat{\textbf{y}}$.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;column_stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_unit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_unit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Orthonormal matrix Q:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Orthonormal matrix Q:
[[ 0.6        -0.74278135]
 [ 0.8         0.55708601]
 [ 0.          0.37139068]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now we confirm $\textit{Q}^T \textit{Q}= \textit{I}$&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([[ 1., -0.],
       [-0.,  1.]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This property will be useful for several applications. For instance, the &lt;em&gt;coupling matrix&lt;/em&gt; or &lt;em&gt;correlation matrix&lt;/em&gt; of a matrix $\textit{A}$ equals $\textit{A}^T \textit{A}$. If we are able to transform the vectors of $\textit{A}$ into orthonormal vectors, such expressions reduces to $\textit{Q}^T \textit{Q}= \textit{I}$. Other applications are the Fourier series and Least Square problems (as we will see later).&lt;/p&gt;

&lt;h3 id=&quot;gram-schmidt-orthogonalization&quot;&gt;Gram-Schmidt Orthogonalization&lt;/h3&gt;

&lt;p&gt;In the previous section, I selected orthogonal vectors to illustrate the idea of an orthonormal basis. Unfortunately, in most cases, matrices are not full rank, i.e., not composed of a set of orthogonal vectors. Fortunately, there are ways to &lt;em&gt;transform a set of non-orthogonal vectors into orthogonal vectors&lt;/em&gt;. This is the so-called &lt;strong&gt;Gram-Schmidt orthogonalization procedure&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The Gram-Schmidt orthogonalization consist of &lt;em&gt;taking the vectors of a matrix, one by one, and making each subsequent vector orthonormal to the previous one&lt;/em&gt;. This is easier to grasp with an example. Consider the matrix $\textit{A}$:&lt;/p&gt;

\[\begin{bmatrix}
2 &amp;amp; 1 &amp;amp; -2 \\
7 &amp;amp; -3 &amp;amp; 1 \\
-3 &amp;amp; 5 &amp;amp; -1
\end{bmatrix}\]

&lt;p&gt;What we want to do, is to find the set of orthonormal vectors $\textbf{q}_1, \textbf{q}_2, \textbf{q}_3$, starting from the columns of $\textit{A}$, i.e., $\textbf{a}_1, \textbf{a}_2, \textbf{a}_3$. We can select any vector to begin with. Recall that we normalize vectors by dividing by its norm as:&lt;/p&gt;

\[\hat{\textbf{a}} = \frac{\textbf{a}}{\Vert \textbf{a} \Vert}\]

&lt;p&gt;Let’s approach this with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;A simple way to check the columns of $\textit{A}$ are not orthonormal is to compute $\textit{A}^T \textit{A}$, which should be equal to the identity in the orthonormal case.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([[ 62, -34,   6],
       [-34,  35, -10],
       [  6, -10,   6]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To build our orthogonal set, we begin by denoting $\textbf{a}_1$ as $\textbf{q}_1$.&lt;/p&gt;

&lt;p&gt;Our &lt;strong&gt;first step&lt;/strong&gt; is to generate the vector $\textbf{q}_2$ from $\textbf{a}_2$ such that is orthogonal to $\textbf{q}_1$ (i.e., $\textbf{a}_1$). To do this, we start with $\textbf{a}_2$ and subtract its projection along $\textbf{q}_1$, which yields the following expression:&lt;/p&gt;

\[\textbf{q}_2 = \textbf{a}_2 - \frac{\textbf{q}_1^T \textbf{a}_2}{\textbf{q}_1^T \textbf{q}_1} \textbf{q}_1\]

&lt;p&gt;Think in this expression carefully. What are we doing, is to subtract $\frac{\textbf{q}&lt;em&gt;1^T \textbf{a}_2}{\textbf{q}_1^T \textbf{q}_1}$ _times&lt;/em&gt; the first column from the second column. Let’s denote $\frac{\textbf{q}_1^T \textbf{a}_2}{\textbf{q}_1^T \textbf{q}_1}$ as $\alpha$, then, we can rewrite our expression as:&lt;/p&gt;

\[\textbf{q}_2 = \textbf{a}_2 - \alpha \textbf{q}_1\]

&lt;p&gt;As we will see, $\alpha$ is a scalar, so effectively we are substracting an scaled version of column one from column two. The figure below express geometrically, what I have been saying: the &lt;em&gt;non-orthogonal&lt;/em&gt; $\textbf{a}_2$ is projected onto $\textbf{q}_1$. Then, we subtract the projection $\textbf{p}$ from $\textbf{a}_2$ to obtain $\textbf{q}_2$ which is orthogonal to $\textbf{q}_1$ as you can appreciate visually (recall $\textbf{a}_1 = \textbf{q}_1$).&lt;/p&gt;

&lt;p&gt;Keep these ideas in mind as it will be important later for QR decomposition.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fig. 16: Orthogonalization&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-10/b-gram-schmidt.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s compute $\textbf{q}_2$ now:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;q1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;a2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;q2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Let’s check that $\textbf{q}_1$ and $\textbf{q}_2$ are actually orthogonal. If so, their dot product should be $0$.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;-0.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, we need to generate $\textbf{q}_3$ from $\textbf{a}_3$. This time, we want $\textbf{q}_3$ to be orthogonal to both $\textbf{q}_1$ and $\textbf{q}_2$. Therefore, we need to subtract its projection along $\textbf{q}_1$ and $\textbf{q}_2$, which yields:&lt;/p&gt;

\[\textbf{q}_3 =
\textbf{a}_3 - \frac{\textbf{q}_1^T \textbf{a}_3}{\textbf{q}_1^T \textbf{q}_1} \textbf{q}_1 -
\frac{\textbf{q}_2^T \textbf{a}_3}{\textbf{q}_2^T \textbf{q}_2} \textbf{q}_2\]

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;a3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;q3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Verify orthogonality&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Dot product q1 and q3:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Dot product q2 and q3:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Dot product q1 and q3:
-0.0

Dot product q2 and q3:
0.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can put $\textbf{q}_1, \textbf{q}_2, \textbf{q}_3$ into $\textit{Q}’$:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Q_prime&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;column_stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Orthogonal matrix Q:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Q_prime&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Orthogonal matrix Q:
[[ 2.          2.09677419 -1.33333333]
 [ 7.          0.83870968  0.66666667]
 [-3.          3.35483871  0.66666667]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The reason we call this matrix $\textit{Q}’$ is that although vectors are orthogonal, they are not normal.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Q_norms&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Q_prime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Norms of vectors in Q-prime:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Q_norms&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Norms of vectors in Q-prime:
[7.87400787 4.04411161 1.63299316]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We rename $\textit{Q}’$ to $\textit{Q}$ by normalizing its vectors.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Q_prime&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Q_norms&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([1., 1., 1.])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To confirm we did this right, let’s evaluate $\textit{Q}^T \textit{Q}$, that should return the identity:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([[ 1., -0., -0.],
       [-0.,  1.,  0.],
       [-0.,  0.,  1.]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There you go: we performed Gram-Schmidt orthogonalization of $\textit{A}$&lt;/p&gt;

&lt;h3 id=&quot;qr-decomposition-as-gram-schmidt-orthogonalization&quot;&gt;QR decomposition as Gram-Schmidt Orthogonalization&lt;/h3&gt;

&lt;p&gt;Gaussian Elimination can be represented as LU decomposition. Similarly, &lt;strong&gt;Gram-Schmidt Orthogonalization can be represented as QR decomposition&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;We learned $\textit{Q}$ is an orthonormal matrix. Now let’s examine $\textit{R}$, which is an upper triangular matrix. In LU decomposition, we used &lt;strong&gt;elementary matrices&lt;/strong&gt; to perform &lt;em&gt;row operations&lt;/em&gt;. Similarly, in the case of QR decomposition, we will use &lt;strong&gt;elementary matrices&lt;/strong&gt; to perform &lt;em&gt;column operations&lt;/em&gt;. We used a lower triangular matrix to perform row operations in LU decomposition by multiplying $\textit{A}$ from the &lt;em&gt;left side&lt;/em&gt;. This time, we will use an upper triangular matrix to perform column operations in QR decomposition by multiplying $\textit{A}$ from the &lt;em&gt;right side&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Once again, our starting point is the identity matrix. The idea is to alter the identity with the operations we want to perform over $\textit{A}$. Consider the matrix from our previous example:&lt;/p&gt;

\[\textit{A} =
\begin{bmatrix}
2 &amp;amp; 1 &amp;amp; -2 \\
7 &amp;amp; -3 &amp;amp; 1 \\
-3 &amp;amp; 5 &amp;amp; -1
\end{bmatrix}\]

&lt;p&gt;What we did in out first step, wast to subtract $\alpha = \frac{\textbf{a}_1 \cdot \textbf{a}_2} {\textbf{a}_1 \cdot \textbf{a}_1}$ of column $\textbf{a}_1$ from column $\textbf{a}_2$. Let’s compute $\alpha$ first:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;a1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;a2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;alpha factor:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;alpha factor:-0.55
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now we need to subtract $\alpha = -0.55$ times $\textbf{a}_1$ from $\textbf{a}_2$. We can represent this operation with an &lt;strong&gt;elementary matrix&lt;/strong&gt;, by doing applying the same operations the identity:&lt;/p&gt;

\[\textit{l} =
\begin{bmatrix}
1 &amp;amp; -0.55 &amp;amp; 0 \\
0 &amp;amp; 1 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}\]

&lt;p&gt;Next, we have to subtract $\beta = \frac{\textbf{a}_1 \cdot \textbf{a}_3} {\textbf{a}_1 \cdot \textbf{a}_1}$ times column $\textbf{a}_1$ and $\gamma = \frac{\textbf{a}_2 \cdot \textbf{a}_3} {\textbf{a}_2 \cdot \textbf{a}_2}$ times $\textbf{a}_2$ from $\textbf{a}_3$. Let’s compute the new $\beta$ and $\gamma$:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;a3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;beta factor:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;gamma factor:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;beta factor:0.1
gamma factor:-0.29
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can add this operations to our elementary matrix by subtracting $0.1$ times the first column from the third, and $-0.29$ times the second from the third:&lt;/p&gt;

\[\textit{l} =
\begin{bmatrix}
1 &amp;amp; -0.55 &amp;amp; 0.1 \\
0 &amp;amp; 1 &amp;amp; -0.29 \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}\]

&lt;p&gt;The last step is to normalize each vector of $\textit{l}$&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;At this point, we should be able to recover $\textit{A}$:&lt;/p&gt;

\[\begin{bmatrix}
2 &amp;amp; 1 &amp;amp; -2 \\
7 &amp;amp; -3 &amp;amp; 1 \\
-3 &amp;amp; 5 &amp;amp; -1
\end{bmatrix}\]

&lt;p&gt;As the matrix product of $\textit{Q}’$ and $\textit{l}$&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Q-prime and l product:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Q_prime&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Q-prime and l product:
[[ 2.  1. -2.]
 [ 7. -3.  1.]
 [-3.  5. -1.]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It works! Now, to recover $\textit{Q}$ will be difficult because of numerical stability and approximation issues in how we have computed things. Actually, if you remove the rounding from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;np.round(Q_prime @ l)&lt;/code&gt; you will obtain different numbers. Fortunately, there is no need to compute $\textit{Q}$ and $\textit{R}$ by hand. We follow the previous steps merely for pedagogical purposes. In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;, we can compute the QR decomposition as:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Q_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;R&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;qr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Let’s compare our $\textit{Q}$ with $\textit{Q}_1$&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Q:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Q:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Q_1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Q:
[[ 0.25400025  0.51847585 -0.81649658]
 [ 0.88900089  0.20739034  0.40824829]
 [-0.38100038  0.82956136  0.40824829]]

Q:
[[-0.25400025  0.51847585 -0.81649658]
 [-0.88900089  0.20739034  0.40824829]
 [ 0.38100038  0.82956136  0.40824829]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The numbers are the same, but some signs are flipped. This stability and approximation issues is why you probably always want to use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; functions (when available).&lt;/p&gt;

&lt;h2 id=&quot;determinant&quot;&gt;Determinant&lt;/h2&gt;

&lt;p&gt;If matrices had personality, the &lt;strong&gt;determinant&lt;/strong&gt; would be the personality trait that reveals most information about the matrix character. The determinant of a matrix is a single number that tells &lt;strong&gt;whether a matrix is invertible or singular&lt;/strong&gt;, this is, whether its columns are linearly independent or not, which is one of the most important things you can learn about a matrix. Actually, the name “determinant” refers to the property of “determining” if the matrix is singular or not. Specifically, for an square matrix $\textit{A} \in \mathbb{R}^{n \times n}$, a determinant equal to $0$, denoted as $\text{det}(\textit{A}=0)$, implies &lt;em&gt;the matrix is singular&lt;/em&gt; (i.e., noninvertible), whereas a determinant equal to $1$, denoted as $\text{det}(\textit{A})=1$, implies the &lt;em&gt;matrix is not singular&lt;/em&gt; (i.e., invertible). Although determinants can reveal if matrices are singular with a single number, it’s not used for large matrices as Gaussian Elimination is faster.&lt;/p&gt;

&lt;p&gt;Recall that matrices can be thought of as function action on vectors or other matrices. Thus, the determinant can also be considered a linear mapping of a matrix $\textit{A}$ onto a single number. But, what does that number mean? So far, we have defined determinants based on their utility of determining matrix invertibility. Before going into the calculation of determinants, let’s examine determinants from a geometrical perspective to gain insight into the meaning of determinants.&lt;/p&gt;

&lt;h3 id=&quot;determinant-as-measures-of-volume&quot;&gt;Determinant as measures of volume&lt;/h3&gt;

&lt;p&gt;From a geometric perspective, determinants indicate the $\textit{sign}$ &lt;strong&gt;area of a parallelogram&lt;/strong&gt; (e.g., a rectangular area) and the $\textit{sign}$ &lt;strong&gt;volume of the parallelepiped&lt;/strong&gt;, for a matrix whose columns consist of the basis vectors in Euclidean space.&lt;/p&gt;

&lt;p&gt;Let’s parse out the above phrase: the $\textit{sign}$ area indicates the absolute value of the area, and the $\textit{sign}$ volume equals the absolute value of the volume. You may be wondering why we need to take the absolute value since real-life objects can’t have negative area or volume. In linear algebra, we say the area of a parallelogram is &lt;strong&gt;negative&lt;/strong&gt; when the vectors forming the figure are &lt;em&gt;clockwise oriented&lt;/em&gt; (i.e., negatively oriented), and &lt;strong&gt;positive&lt;/strong&gt; when the vectors forming the figure are &lt;em&gt;counterclockwise oriented&lt;/em&gt; (i.e., positively oriented).&lt;/p&gt;

&lt;p&gt;Here is an example of a matrix $\textit{A}$ with vectors &lt;em&gt;clockwise&lt;/em&gt; or &lt;em&gt;negatively&lt;/em&gt; oriented:&lt;/p&gt;

\[\textit{A} =
\begin{bmatrix}
0 &amp;amp; 2 \\
2 &amp;amp; 0
\end{bmatrix}\]

&lt;p&gt;The elements of the first column, indicate the first vector of the matrix, while the elements of the second column, the second vector of the matrix. Therefore, when we measure the area of the parallelogram formed by the pair of vectors, we move from left to right, i.e., &lt;em&gt;clockwise&lt;/em&gt;, meaning that the vectors are &lt;strong&gt;negatively oriented&lt;/strong&gt;, and the &lt;strong&gt;area of the matrix will be negative&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Here is the same matrix $\textit{A}$ with vectors &lt;em&gt;counterclockwise&lt;/em&gt; or &lt;em&gt;positively&lt;/em&gt; oriented:&lt;/p&gt;

\[\textit{A} =
\begin{bmatrix}
2 &amp;amp; 0 \\
0 &amp;amp; 2
\end{bmatrix}\]

&lt;p&gt;Again, the elements of the &lt;em&gt;first column&lt;/em&gt;, indicate the &lt;em&gt;first vector&lt;/em&gt; of the matrix, while the elements of the &lt;em&gt;second column&lt;/em&gt;, the &lt;em&gt;second vector&lt;/em&gt; of the matrix. Therefore, when we measure the area of the parallelogram formed by the pair of vectors, we move from &lt;em&gt;right to left&lt;/em&gt;, i.e., &lt;em&gt;counterclockwise&lt;/em&gt;, meaning that the vectors are &lt;strong&gt;positively oriented&lt;/strong&gt;, and the &lt;strong&gt;area of the matrix will be positive&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The figure below exemplifies what I just said.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fig. 17: Vector orientation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-10/b-determinant-orientation.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The situation for the $\textit{sign}$ volume of the parallelepiped is no different: when the vectors are &lt;em&gt;counterclockwise&lt;/em&gt; oriented, we say the vectors are &lt;em&gt;positively oriented&lt;/em&gt; (i.e., positive volume); when the vectors are &lt;em&gt;clockwise&lt;/em&gt; oriented, we say the vectors are &lt;em&gt;negatively oriented&lt;/em&gt; (i.e., negative volume).&lt;/p&gt;

&lt;h3 id=&quot;the-2-x-2-determinant&quot;&gt;The 2 X 2 determinant&lt;/h3&gt;

&lt;p&gt;Recall that matrices are invertible or nonsingular when their columns are linearly independent. By extension, the determinant allow us to whether the columns of a matrix a linearly independent. To understand this method, let’s examine the $2 \times 2$ special case first.&lt;/p&gt;

&lt;p&gt;Consider a square matrix as:&lt;/p&gt;

\[\textit{A} =
\begin{bmatrix}
1 &amp;amp; 4 \\
2 &amp;amp; 8
\end{bmatrix}\]

&lt;p&gt;How can we decide whether the columns are linearly independent? A strategy that I often use in simple cases like this, is just to examine whether the second column equals the first column times some factor. In the case of $\textit{A}$ is easy to see that the second column equals four times the first column, so the columns are linearly &lt;em&gt;dependent&lt;/em&gt;. We can express such criteria by comparing the &lt;em&gt;elementwise division&lt;/em&gt; between each element of the second column by each element of the first column as:&lt;/p&gt;

\[\begin{bmatrix}
\frac{4}{1} =
\frac{8}{2}
\end{bmatrix}
=
\begin{bmatrix}
4 =
4
\end{bmatrix}\]

&lt;p&gt;We obtain that both entries equal $4$, meaning that the second column can be divided exactly by the first column (i.e., linearly &lt;em&gt;dependent&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;Consider this matrix now:&lt;/p&gt;

\[\textit{B} =
\begin{bmatrix}
0 &amp;amp; 4 \\
0 &amp;amp; 8
\end{bmatrix}\]

&lt;p&gt;Let’s try again our method for $\textit{B}$:&lt;/p&gt;

\[\begin{bmatrix}
\frac{4}{0} =
\frac{8}{0}
\end{bmatrix}
=
\begin{bmatrix}
\textit{undef} =
\textit{undef}
\end{bmatrix}\]

&lt;p&gt;Now we got into a problem because division by $0$ is undefined, so we can determine the relationship between columns of $\textit{B}$. Yet, by inspection, we can see the first column is simply $0$ times the second column, therefore linearly dependent. Here is when &lt;strong&gt;determinants&lt;/strong&gt; come to the rescue.&lt;/p&gt;

&lt;p&gt;Consider the generic matrix:&lt;/p&gt;

\[\begin{bmatrix}
a &amp;amp; b \\
c &amp;amp; d
\end{bmatrix}\]

&lt;p&gt;According to our previous strategy, we had:&lt;/p&gt;

\[\frac{b}{a} = \frac{d}{c}\]

&lt;p&gt;This is, we tested the elementwise division of the second column by the first column. Before, we failed because of division, so we probably want a method that does not involve it. Notice that we can rearrange our expression as:&lt;/p&gt;

\[ad = bc\]

&lt;p&gt;Let’s try again with this method for $\textit{A}$:&lt;/p&gt;

\[\begin{bmatrix}
1 \times 8 = 4 \times 2 \\
\end{bmatrix}
\begin{bmatrix}
8 = 8
\end{bmatrix}\]

&lt;p&gt;And for $\textit{B}$:&lt;/p&gt;

\[\begin{bmatrix}
0 \times 8 = 4 \times 0 \\
\end{bmatrix}
\begin{bmatrix}
0 = 0
\end{bmatrix}\]

&lt;p&gt;It works. Indeed, $ad = bc$ are equal for both matrices, $\textit{A}$ and $\textit{B}$, meaning their columns are linearly dependent. Finally, notice that we can rearange all the terms on one side of the equation as:&lt;/p&gt;

\[(ad) - (bc)=0\]

&lt;p&gt;There you go: the above expression is what is known as the &lt;strong&gt;determinant of a matrix&lt;/strong&gt;. We denote the determinant as:&lt;/p&gt;

\[\vert \textit{A} \vert =
\begin{vmatrix}
a &amp;amp; b \\
c &amp;amp; d
\end{vmatrix} =
(ad) - (bc)\]

&lt;p&gt;Or&lt;/p&gt;

\[\textit{det (A)} =
\begin{vmatrix}
a &amp;amp; b \\
c &amp;amp; d
\end{vmatrix} =
(ad) - (bc)\]

&lt;h3 id=&quot;the-n-x-n-determinant&quot;&gt;The N X N determinant&lt;/h3&gt;

&lt;p&gt;As matrices larger, computing the determinant gets more complicated. Consider the $3 \times 3$ case as:&lt;/p&gt;

\[\vert \textit{A} \vert =
\begin{vmatrix}
a &amp;amp; b &amp;amp; c\\
d &amp;amp; e &amp;amp; f\\
g &amp;amp; h &amp;amp; i
\end{vmatrix}\]

&lt;p&gt;The problem now is that linearly independent columns can be either: (1) multiples of another column, and (2) linear combinations of pairs of columns. The determinant for a $3 \times 3$ is:&lt;/p&gt;

\[\vert \textit{A} \vert = aei - afh + bfg - bdi + cdh - ceg\]

&lt;p&gt;Such expression is hard to memorize, and it will get even more complicated for larger matrices. For instance, the $4 \times 4$ entails 24 terms. As with most things in mathematics, there is a general formula to express the determinant compactly, which is known as the Leibniz’s formula:&lt;/p&gt;

\[\vert \textit{A} \vert = \sum_{\sigma} \textit{sign}(\sigma) \prod_{i=1}^n a_{\sigma(i),i}\]

&lt;p&gt;Where $\sigma$ computes the permutation for the rows and columns vectors of the matrix. Is of little importance for us to break down the meaning of this formula since we are interested in its applicability and conceptual value. What is important to notice, is that for an arbitrary square $n \times n$ matrix, we will have $n!$ terms to sum over. For instance, for a $10 \times 10$ matrix, $10! = 3,628,800$, which is a gigantic number considering the size of the matrix. In machine learning, we care about matrices with thousands or even millions of columns, so there is no use for such formula. Nonetheless, this does not mean that the determinant is useless, but the direct calculation with the above algebraic expression is not used.&lt;/p&gt;

&lt;h3 id=&quot;determinants-as-scaling-factors&quot;&gt;Determinants as scaling factors&lt;/h3&gt;

&lt;p&gt;When we think in matrices as linear mappings, this is, as functions applied to vectors (or vectors spaces), the determinant acquires an intuitive geometrical interpretation: &lt;strong&gt;as the factor by which areas are scaled under a mapping&lt;/strong&gt;. Plainly, if you do a mapping or transformation, and the area increases by a factor of $3$, then the determinant of the transformation matrix equals $3$. Consider the matrix $\textit{A}$ and the basis vector $\textbf{x}$:&lt;/p&gt;

\[\textit{A} =
\begin{bmatrix}
4 &amp;amp; 0 \\
0 &amp;amp; 3
\end{bmatrix}\]

\[\textbf{x} =
\begin{bmatrix}
1 &amp;amp; 1
\end{bmatrix}\]

&lt;p&gt;Is easy to see that the parallelogram formed by the basis vectors of $\textbf{x}$ is $1 \times 1 = 1$. When we apply $\textit{A}\textbf{x}$, we get:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([4, 3])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Meaning that the vertical axis was scaled by $4$ and the horizontal axis by $3$, hence, the new parallelogram has area $4 \times 3 = 12$. Since the new area has increased by a factor of $12$, the determinant $\vert \textit{A} \vert =  12$. Although we exemplified this with the basis vectors in $\textit{x}$, the determinant of $\textit{A}$ for mappings of the entire vector space. The figure below visually illustrates this idea.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fig. 18: Determinants&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-10/b-determinant-scaling.svg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-importance-of-determinants&quot;&gt;The importance of determinants&lt;/h3&gt;

&lt;p&gt;Considering that calculating the determinant is not computationally feasible for large matrices and that we can determine linear independence via Gaussian Elimination, you may be wondering what’s the point of learning about determinants in the first place. I also asked myself more than once. It turns out that determinants play a crucial conceptual role in other topics in matrix decomposition, particularly eigenvalues and eigenvectors. Some books I reviewed devote a ton of space to determinants, whereas others (like Strang’s Intro to Linear Algebra) do not. In any case, we study determinants mostly because of its conceptual value to better understand linear algebra and matrix decomposition.&lt;/p&gt;

&lt;h2 id=&quot;eigenthings&quot;&gt;Eigenthings&lt;/h2&gt;

&lt;p&gt;Eigenvectors, eigenvalues, and their associated mathematical objects and properties (which I call “Eigen-things”) have important applications in machine learning like Principal Component Analysis (PCA), Spectral Clustering (K-means), Google’s PageRank algorithm, Markov processes, and others. Next, we will review several of these “eigen-things”.&lt;/p&gt;

&lt;h3 id=&quot;change-of-basis&quot;&gt;Change of basis&lt;/h3&gt;

&lt;p&gt;Previously, we said that a set of $n$ linearly independent vectors with $n$ elements forms a &lt;strong&gt;basis&lt;/strong&gt; for a vector space. For instance, we say that the &lt;em&gt;orthogonal&lt;/em&gt; pair of vectors $\textbf{x}$ and $\textbf{y}$ (or horizontal and vertical axes), describe the Cartesian plane or $\mathbb{R}^2$ space. Further, if we think in the $\textbf{x}$ and $\textbf{y}$ pair as unit vectors, then we can describe any vector in $\mathbb{R}^2$ as a linear combination of $\textbf{x}$ and $\textbf{y}$. For example, the vector:&lt;/p&gt;

\[\textbf{c}=
\begin{bmatrix}
- 3 \\
- 1
\end{bmatrix}\]

&lt;p&gt;Can be described as scaling the unit vector $\textbf{x}=\begin{bmatrix} 1 \ 0 \end{bmatrix}$ by $-3$, and scaling the unit vector $\textbf{y}=\begin{bmatrix} 0 \ 1 \end{bmatrix}$ by $-1$.&lt;/p&gt;

&lt;p&gt;If you are like me, you have probably gotten use to the idea of describing any 2-dimensional space as $\textbf{x}$ and $\textbf{y}$ coordinates, with $\textbf{x}$ lying perfectly horizontal and $\textbf{y}$ perpendicular to it, as if this were the only natural way of thinking on coordinates in space. It turns out, there is nothing “natural” about it. You could literally draw a pair of orthogonal vectors on any orientation in space, define the first one as $\textbf{x’}=\begin{bmatrix} 1 \ 0 \end{bmatrix}$, and the second one as $\textbf{y’}=\begin{bmatrix} 0 \ 1 \end{bmatrix}$, and that would be perfectly fine. It may look different, but every single mathematical property we have studied so far about vectors would hold. For instance, in Fig 19. the &lt;strong&gt;alternative coordinates&lt;/strong&gt; $\textbf{x’}$ and $\textbf{y’}$ are equivalent to the vectors $\textbf{a}=\begin{bmatrix} -2 \ 2 \end{bmatrix}$ and $\textbf{b}=\begin{bmatrix} 2 \ 2 \end{bmatrix}$, in the standard $\textbf{x}$ and $\textbf{y}$ coordinates.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fig. 19: Change of basis&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-10/b-change-basis.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The question now is how to “move” from one set of basis vectors to the other. The answer is with &lt;strong&gt;linear mappings&lt;/strong&gt;. We know already that $\textbf{x’, y’}$ equals to $\textbf{a}=\begin{bmatrix} -2 \ 2 \end{bmatrix}$ and $\textbf{b}=\begin{bmatrix} 2 \ 2 \end{bmatrix}$ in $\textbf{x, y}$ coordinates. To find the values of $\textbf{x, y}$ in $\textbf{x’, y’}$, we need to take the &lt;strong&gt;inverse of $\textit{T}$&lt;/strong&gt;. Think about it in this way: we represented $\textbf{x’=a, y’=a}$ in $\textbf{x, y}$ by scaling its unit vectors by the transformation matrix $\textit{T}$ as:&lt;/p&gt;

\[\textit{T}\textit{A} =
\begin{bmatrix}
-2 &amp;amp; 2 \\
2 &amp;amp; 2
\end{bmatrix}
\begin{bmatrix}
1 &amp;amp; 0 \\
0 &amp;amp; 1
\end{bmatrix} =
\begin{bmatrix}
-2 &amp;amp; 2 \\
2 &amp;amp; 2
\end{bmatrix}\]

&lt;p&gt;Now, to do the &lt;em&gt;opposite&lt;/em&gt;, i.e., to “translate” the values of the coordinates $\textbf{x, y}$ to values in $\textbf{x’, y’}$, we scale $\textbf{x, y}$ by the inverse of $\textit{T}$ as:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Transformation or mapping matrix
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Inverse of T
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T_i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# x, y vectors
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;x, y vectors in x&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;, y&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; coordinates:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T_i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;x, y vectors in x&apos;, y&apos; coordinates:
[[ 0.25  0.25]
 [-0.25  0.25]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;That is our answer: $\textbf{x, y}$ equals to $\begin{bmatrix}0.25 \ -0.25 \end{bmatrix}$ and $\begin{bmatrix}0.25 \ 0.25 \end{bmatrix}$ in $\textbf{x’, y’}$ coordinate space. &lt;strong&gt;Fig. 19&lt;/strong&gt; illustrate this as well.&lt;/p&gt;

&lt;p&gt;This may become clearer by mapping $\textbf{c}=\begin{bmatrix}-3 \ -1 \end{bmatrix}$ in $\textbf{x, y}$, onto $\textbf{c’}$ in $\textbf{x’, y’}$ alternative coordinates. To do the mapping, again, we need to multiply $\textbf{c}$ by $\textit{T}^{-1}$. Let’s try this out with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# vector to map
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Vector a=[1,3] in x&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; and y&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; basis:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T_i&lt;/span&gt;&lt;span class=&quot;nd&quot;&gt;@a&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Vector a=[1,3] in x&apos; and y&apos; basis:
[[-1. ]
 [ 0.5]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In &lt;strong&gt;Fig. 19&lt;/strong&gt;, we can confirm the mapping by simply visual inspection.&lt;/p&gt;

&lt;h3 id=&quot;eigenvectors-eigenvalues-and-eigenspaces&quot;&gt;Eigenvectors, Eigenvalues and Eigenspaces&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Eigen&lt;/em&gt; is a German word meaning “own” or “characteristic”. Thus, roughly speaking, the eigenvector and eigenvalue of a matrix refer to their “characteristic vector” and “characteristic value” for that vector, respectively. As a cognitive scientist, I like to think in eigenvectors as the “pivotal” personality trait of someone, i.e., the personality “axis” around which everything else revolves, and the eigenvalue as the “intensity” of that trait.&lt;/p&gt;

&lt;p&gt;Put simply, the &lt;strong&gt;eigenvector of a matrix&lt;/strong&gt; is a non-zero vector that &lt;em&gt;only gets scaled&lt;/em&gt; when multiplied by a transformation matrix $\textit{A}$. In other words, the vector does not rotate or change direction in any manner. It just gets larger or shorter. The &lt;strong&gt;eigenvalue of a matrix&lt;/strong&gt; is the factor by which the eigenvector gets scaled. This is a bit of a stretch, but in terms of the personality analogy, we can think in the eigenvector as the personality trait that does not change even when an individual change of context: Lisa Simpson “pivotal” personality trait is &lt;em&gt;conscientiousness&lt;/em&gt;, and no matter where she is, home, school, etc., their personality revolves around that. Following the analogy, the eigenvalue would represent the magnitude or intensity of such traits in Lisa. Fig 20. illustrate the geometrical representation of an eigenvector with a cube rotation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fig. 20: Eigenvector in a 3-dimensional rotation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-10/b-eigenvector.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;More formally, we define eigenvectors and eigenvalues as:&lt;/p&gt;

\[\textit{A}\textbf{x} := \lambda \textbf{x}\]

&lt;p&gt;Where $\textit{A}$ is a square matrix in $\mathbb{R}^{n \times n}$, $\textbf{x}$ the eigenvector, and $\lambda$ an scalar in $\mathbb{R}$. This identity may look weird to you: &lt;strong&gt;How do we go from matrix-vector multiplication to scalar-vector multiplication?&lt;/strong&gt; We are basically saying that somehow multiplying $\textbf{x}$ by a matrix $\textit{A}$ or a scalar $\lambda$ yields the same result. To make sense of this, recall our discussion about the effects of a matrix on a vector. Mappings or transformation like reflection and shear boils down to &lt;em&gt;a combination of scaling and rotation&lt;/em&gt;. If a mapping $\textit{A}$ does not rotate $\textbf{x}$, it makes sense that such mapping can be reduced to a simpler scalar-vector multiplication $\lambda \textbf{x}$.&lt;/p&gt;

&lt;p&gt;If you recall our discussion about elementary matrices, you may see a simple way to make the $\textit{A}\textbf{x} = \lambda \textbf{x}$ more intuitive. Elementary matrices allow us to encode row and column operations on a matrix. Scalar multiplication, can be represented as by multiplying either the rows or columns of the identity matrix by the desired factor. For instance, for $\lambda = 2$:&lt;/p&gt;

\[2
\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 1 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix} =
\begin{bmatrix}
2 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; 2 &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 2
\end{bmatrix}\]

&lt;p&gt;This allow us to rewrite as $\lambda \textit{I}\textbf{x}$, and to maintain the matrix-vector multiplication form as:&lt;/p&gt;

\[\textit{A}\textbf{x} = \lambda \textit{I}\textbf{x}\]

&lt;p&gt;We can go further, and rearange our expression to:&lt;/p&gt;

\[\textit{A}\textbf{x} -\lambda \textit{I}\textbf{x} = 0\]

&lt;p&gt;And to factor our $\textbf{x}$ to get:&lt;/p&gt;

\[(\textit{A} -\lambda \textit{I})\textbf{x} = 0\]

&lt;p&gt;The first part of our new expression, $(\textit{A} -\lambda \textit{I})$, will yield a matrix, meaning that now we have matrix-vector multiplication. In particular, we want a non-zero vector $\textbf{x}$ that when multiplied by $(\textit{A} -\lambda \textit{I})$ yields $0$. The only way to achieve this is when the scaling factor associated with $(\textit{A} -\lambda \textit{I})$ is $0$ as well. Here is when &lt;strong&gt;determinants&lt;/strong&gt; come into play. Recall that the determinant of a matrix represents the scaling factor of such mapping, which in this specific case, happens to be the &lt;em&gt;eigenvalue&lt;/em&gt; of the matrix. Consequently, we want:&lt;/p&gt;

\[\textit{det}(\textit{A} -\lambda \textit{I}) = 0\]

&lt;p&gt;Since $\textit{A}$ and $\textit{I}$ are fixed, in practice, we want to find a value of $\lambda$ that will yield a $0$ determinant of the matrix. Any matrix with a determinant of $0$ will be &lt;em&gt;singular&lt;/em&gt;. This time, we want the matrix to be singular, as we are trying to solve a problem with three unknowns and two equations, therefore, it is the only way to solve it.&lt;/p&gt;

&lt;p&gt;By finding a value for $\lambda$ that makes the determinat $0$, we are effectively making the equality $(\textit{A} -\lambda \textit{I})\textbf{x} = 0$ true.&lt;/p&gt;

&lt;p&gt;Let’s do an example to make these ideas more concrete. Consider the following matrix:&lt;/p&gt;

\[\begin{bmatrix}
4 &amp;amp; 2 \\
1 &amp;amp; 3
\end{bmatrix}\]

&lt;p&gt;Let’s first multiply $\textit{A} -\lambda \textit{I}$ to get a single matrix:&lt;/p&gt;

\[\begin{bmatrix}
4 &amp;amp; 2 \\
1 &amp;amp; 3
\end{bmatrix} -
\lambda
\begin{bmatrix}
1 &amp;amp; 0 \\
0 &amp;amp; 1
\end{bmatrix}
=
\begin{bmatrix}
4 &amp;amp; 2 \\
1 &amp;amp; 3
\end{bmatrix} -
\begin{bmatrix}
\lambda  &amp;amp; 0 \\
0 &amp;amp; \lambda
\end{bmatrix}
=
\begin{bmatrix}
4 - \lambda &amp;amp; 2 \\
1 &amp;amp; 3 - \lambda
\end{bmatrix}\]

&lt;p&gt;We begin by computing the determinant as:&lt;/p&gt;

\[\textit{det (A)} =
\begin{vmatrix}
a &amp;amp; b \\
c &amp;amp; d
\end{vmatrix} =
(ad) - (bc)\]

&lt;p&gt;Which yield the following polynomial:&lt;/p&gt;

\[\begin{vmatrix}
4 - \lambda &amp;amp; 2 \\
1 &amp;amp; 3 - \lambda
\end{vmatrix} =
(4 - \lambda) (3 - \lambda) - 2 \times 1\]

&lt;p&gt;That we solve as any other quadratic polynomial, which receives the special name of &lt;strong&gt;characteristc polynomial&lt;/strong&gt;. When we equate the characteristic polynomial to $0$, we call such expression the &lt;strong&gt;characteristic equation&lt;/strong&gt;. The roots of the characteristic equation, are the eigenvalues of the matrix:&lt;/p&gt;

\[(4 - \lambda) (3 - \lambda) - 2 \times 1 =
12 - 4\lambda - 3\lambda + \lambda^2 -3 =
10 - 7\lambda + \lambda^2\]

&lt;p&gt;Wich can be factorized as:&lt;/p&gt;

\[(2- \lambda) (5 - \lambda)\]

&lt;p&gt;There you go: we obtain &lt;strong&gt;eigenvalues $\lambda_1 = 2$, and $\lambda_2 = 5$.&lt;/strong&gt; this simply means that $\textit{A}\textbf{x} = \lambda \textbf{x}$ can be solved for eigenvalues equal to $2$ and $5$, assuming non-zero eigenvectors.&lt;/p&gt;

&lt;p&gt;Once we find the eigenvalues, we can compute the eigenvector for each of them. Let’s start with $\lambda_1 = 2$:&lt;/p&gt;

\[\begin{bmatrix}
4 - \lambda &amp;amp; 2 \\
1  &amp;amp; 3 - \lambda
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix} =
\begin{bmatrix}
4 - 2 &amp;amp; 2 \\
1  &amp;amp; 3 - 2
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix} =
\begin{bmatrix}
2 &amp;amp; 2 \\
1 &amp;amp; 1
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix} = 0\]

&lt;p&gt;Since the first and second column are identical, we obtain that the solution for the system is pair of such that $\textbf{x}_1 = - \textbf{x}_2$, for instance:&lt;/p&gt;

\[\textit{E}_{\lambda=2}=
\begin{bmatrix}
-1 \\
1
\end{bmatrix}\]

&lt;p&gt;Such vector correspond to the &lt;strong&gt;eigenspace&lt;/strong&gt; for the eigenvalue $\lambda = 2$. An eigenspace denotes all the vectors that correspond to a given eigenvalue, which in this case is the span of $\textit{E}_{\lambda=2}$.&lt;/p&gt;

&lt;p&gt;Now let’s evaluate for $\lambda = 5$:&lt;/p&gt;

\[\begin{bmatrix}
4 - \lambda &amp;amp; 2 \\
1  &amp;amp; 3 - \lambda
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix} =
\begin{bmatrix}
4 - 5 &amp;amp; 2 \\
1  &amp;amp; 3 - 5
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix} =
\begin{bmatrix}
-1 &amp;amp; 2 \\
1 &amp;amp; -2
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix}
= 0\]

&lt;p&gt;Since the first column is just $-2$ times the second, the solution for the system will be any pair such that $2\textbf{x}_1 = \textbf{x}_2$, i.e.:&lt;/p&gt;

\[\textit{E}_{\lambda=5}
\begin{bmatrix}
2 \\
1
\end{bmatrix}\]

&lt;p&gt;With the span of $\textit{E}_{\lambda=5}$ as the eigenspace for the eigenvalue $\lambda = 5$.&lt;/p&gt;

&lt;p&gt;As usual, we can find the eigenvectors and eigenvalues of a matrix with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;. Let’s check our computation:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vectors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;eig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Eigenvalues of A:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Eigenvectors of A:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vectors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Eigenvalues of A:
[5. 2.]

Eigenvectors of A:
[[ 0.894 -0.707]
 [ 0.447  0.707]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The eigenvalues are effectively $5$ and $2$. The eigenvectors (aside rounding error), match exactly what we found. For $\lambda=5$, $2\textbf{x}_1 = \textbf{x}_1$, and for $\lambda=2$ that $\textbf{x}_1 = - \textbf{x}_2$.&lt;/p&gt;

&lt;p&gt;Not all matrices will have eigenvalues and eigenvectors in $\mathbb{R}$. Recall that we said that eigenvalues essentially indicate scaling, whereas eigenvectors indicate the vectors that remain unchanged under a linear mapping. It follows that if a linear transformation does not stretch vectors and rotates all of them, then no eigenvectors and eigenvalues should be found. An example of this is a rotation matrix:&lt;/p&gt;

\[\begin{bmatrix}
0 &amp;amp; -1 \\
1 &amp;amp; 0
\end{bmatrix}\]

&lt;p&gt;Let’s compute its eigenvectors and eigenvalues in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vectors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;eig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;B Eigenvalues:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;B Eigenvectors:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vectors&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;B Eigenvalues:
[0.+1.j 0.-1.j]

B Eigenvectors:
[[0.70710678+0.j         0.70710678-0.j        ]
 [0.        -0.70710678j 0.        +0.70710678j]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;strong&gt;&lt;em&gt;+0.j&lt;/em&gt;&lt;/strong&gt; indicates the solution yield imaginary numbers, meaning that there are not eigenvectors or eigenvalues for the matrix $\textit{B} \in \mathbb{R}$&lt;/p&gt;

&lt;h3 id=&quot;trace-and-determinant-with-eigenvalues&quot;&gt;Trace and determinant with eigenvalues&lt;/h3&gt;

&lt;p&gt;The &lt;strong&gt;trace&lt;/strong&gt; of a matrix is the &lt;em&gt;sum of its diagonal elements&lt;/em&gt;. Formally, we define the trace for a square matrix $\textit{A} \in \mathbb{R}^{n \times n}$ as:&lt;/p&gt;

\[tr(\textit{A}) := \sum_{i=1}^n = a_{ii}\]

&lt;p&gt;There is something very special about eigenvalues: &lt;em&gt;its sum equals the trace of the matrix&lt;/em&gt;. Recall the matrix $\textit{A}$ from the previous section:&lt;/p&gt;

\[\begin{bmatrix}
4 &amp;amp; 2 \\
1 &amp;amp; 3
\end{bmatrix}\]

&lt;p&gt;Which has a trace equal to $4 + 3 = 7$. We found that their eigenvalues were $\lambda_1 = 2$ and $\lambda_2 = 5$, which also add up to $7$.&lt;/p&gt;

&lt;p&gt;Here is another curious fact about eigenvalues: &lt;em&gt;its product equals to the determinant of the matrix&lt;/em&gt;. The determinant of $\textit{A}$ equals to $(4 \times 3) - (2 \times 1) = 10$. The product of the eigenvalues is also $10$.&lt;/p&gt;

&lt;p&gt;These two properties hold only when we have a full set of eigenvalues, this is when we have as many eigenvalues as dimensions in the matrix.&lt;/p&gt;

&lt;h3 id=&quot;eigendecomposition&quot;&gt;Eigendecomposition&lt;/h3&gt;

&lt;p&gt;In previous sections, we associated LU decomposition with Gaussian Elimination and QR decomposition with Gram-Schmidt Orthogonalization. Similarly, we can associate the Eigenvalue algorithm to find the eigenvalues and eigenvectors of a matrix, wit the &lt;strong&gt;Eigendecomposition&lt;/strong&gt; or &lt;strong&gt;Eigenvalue Decomposition&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;We learned that we can find the eigenvalues and eigenvectors of a square matrix (assuming they exist) with:&lt;/p&gt;

\[(\textit{A} - \lambda \textit{I})\textbf{x} = 0\]

&lt;p&gt;Process that entail to first solve the characteristic equation for the polynomial, and then evaluate each eigenvalue to find the corresponding eigenvector. The question now is how to express such process as a single matrix-matrix operation. Let’s consider the following transformation matrix:&lt;/p&gt;

\[\textit{T}=
\begin{bmatrix}
5 &amp;amp; 3 &amp;amp; 0 \\
2 &amp;amp; 6 &amp;amp; 0 \\
4 &amp;amp; -2 &amp;amp; 2
\end{bmatrix}\]

&lt;p&gt;Let’s begin by computing the eigenvalues and eigenvectors with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;eigenvalues&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eigenvectors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;eig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;B Eigenvalues:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eigenvalues&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;B Eigenvectors:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eigenvectors&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;B Eigenvalues:
[2. 8. 3.]

B Eigenvectors:
[[ 0.          0.6882472   0.18291323]
 [ 0.          0.6882472  -0.12194215]
 [ 1.          0.22941573  0.97553722]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We obtained a vector of eigenvalues and a
Now, we know that the following identity must be true for scalar-matrix multiplication:&lt;/p&gt;

\[\textit{A}\textbf{x} = \lambda \textit{I} \textbf{x}\]

&lt;p&gt;Since we want to multiply a matrix of eigenvalues by the matrix of eigenvectors, we have to be careful about selecting the order of the multiplication. Recall that matrix-matrix multiplication &lt;em&gt;is not commutative&lt;/em&gt;, meaning that the multiplication order matters. Before this wasn’t a problem, because scalar-matrix multiplication is commutative. What we want, is in operation such that eigenvalues scale eigenvectors. For this, we will put the eigenvectors in a matrix $\textit{X}$, the result of $\lambda \textit{I}$ in a matrix $\Lambda$, and multiply $\textit{X}$ by $\Lambda$ from the right side as:&lt;/p&gt;

\[\textit{A}\textbf{X} = \textbf{X} \Lambda\]

&lt;p&gt;Let’s do this with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eigenvectors&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;I&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;identity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;I&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eigenvalues&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Left-side of the equation AX:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Right-side of the equation XL:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Left-side of the equation AX:
[[ 0.          5.50597761  0.54873968]
 [ 0.          5.50597761 -0.36582646]
 [ 2.          1.83532587  2.92661165]]

Right-side of the equation XL:
[[ 0.          5.50597761  0.54873968]
 [ 0.          5.50597761 -0.36582646]
 [ 2.          1.83532587  2.92661165]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Verify equality&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Entry-wise comparison: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;allclose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Entry-wise comparison: True
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;A a side note, it is not a good idea to compare &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; arrays with the equality operator, as rounding error and the finite internal bit representation may yield &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;False&lt;/code&gt; when values are technically equal. For instance:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;array([[ True,  True, False],
       [ True,  True, False],
       [ True,  True,  True]])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We still have one issue to address to complete the Eigendecomposition of $\textit{A}$: to get rid of $\textit{X}$ on the left side of the equation. A first thought is simply to multiply by the $\textit{X}^{-1}$ to cancel $\textit{X}$ on both sides. This won’t work because on the left side of the equation, $\textit{X}$ is multiplying from the right of $\textit{A}$, whereas on the right side of the equation, $\textit{X}$ is multiplying from the left of $\Lambda$. Yet, we still can get take the inverse to eliminate only from the left side of the equation and obtain:&lt;/p&gt;

\[\textit{A} = \textit{X} \Lambda \textit{X}^{-1}\]

&lt;p&gt;Lo and behold, &lt;strong&gt;we have found the expression for the Eigendecomposition.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Let’s confirm this works:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;X_inv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Original matrix A:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Reconstruction of A with Eigen Decomposition of A:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_inv&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Original matrix A:
[[ 5  3  0]
 [ 2  6  0]
 [ 4 -2  2]]

Reconstruction of A with Eigen Decomposition of A:
[[ 5.  3.  0.]
 [ 2.  6.  0.]
 [ 4. -2.  2.]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;eigenbasis-are-a-good-basis&quot;&gt;Eigenbasis are a good basis&lt;/h3&gt;

&lt;p&gt;There are cases when a transformation or mapping $\textit{T}$ has associated a full set of eigenvectors, i.e., as many eigenvectors as dimensions in $\textit{T}$. We call this set of eigenvectors an &lt;strong&gt;eigenbasis&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;When approaching linear algebra problems, selecting a “good” basis for the matrix or vector space can significantly simplify computation, and also reveals several facts about the matrix that would be otherwise hard to see. Eigenbasis, are an example of a basis that would make our life easier in several situations.&lt;/p&gt;

&lt;p&gt;From the previous section, we learned that the Eigenvalue Decomposition is defined as:&lt;/p&gt;

\[\textit{A} := \textit{X} \Lambda \textit{X}^{-1}\]

&lt;p&gt;Conceptually, a first lesson is that transformations, like $\textit{A}$, have two main components: a matrix $\Lambda$ that stretch, shrink, or flip, the vectors, and $\textit{X}$, which represent the “axes” around which the transformation occurs.&lt;/p&gt;

&lt;p&gt;Eigenbasis also make computing the power of a matrix easy. Consider the case of $\textit{A}^2$:&lt;/p&gt;

\[\textit{A}^2 = \textit{X} \Lambda \textit{X}^{-1} \textit{X} \Lambda \textit{X}^{-1}\]

&lt;p&gt;Since $\textit{X}^{-1} \textit{X}$ equals the identity, we obtain:&lt;/p&gt;

\[\textit{A}^2 = \textit{X} \Lambda^2 \textit{X}^{-1}\]

&lt;p&gt;The pattern:&lt;/p&gt;

\[\textit{A}^n = \textit{X} \Lambda^n \textit{X}^{-1}\]

&lt;p&gt;Generalizes to any power. For powers of $n=2$ or $n=3$ such approach may not be the best, as computing the power directly on $\textit{A}$ may be easier. But, when dealing with large matrices with powers of thousands or millions, this approach is far superior. Further, it even works for the inverse:&lt;/p&gt;

\[\textit{A}^{-1} = \textit{X} \Lambda^{-1} \textit{X}^{-1}\]

&lt;p&gt;We can see this is true by testing that $\textit{A} \textit{A}^{-1}$ equals the identity:&lt;/p&gt;

\[\textit{A} \textit{A}^{-1} = \textit{X} \Lambda \textit{X}^{-1} \textit{X} \Lambda^{-1} \textit{X}^{-1}\]

&lt;p&gt;Pay attention to what happens now: $\textit{X}^{-1} \textit{X}= \textit{I}$, which yields:&lt;/p&gt;

\[\textit{A} \textit{A}^{-1} = \textit{X} \Lambda \textit{I} \Lambda^{-1} \textit{X}^{-1} = \textit{X} \Lambda \Lambda^{-1} \textit{X}^{-1}\]

&lt;p&gt;Now, $\Lambda \Lambda^{-1} $, also yields the identity:&lt;/p&gt;

\[\textit{A} \textit{A}^{-1} = \textit{X} \textit{I} \textit{X}^{-1} = \textit{X} \textit{X}^{-1}\]

&lt;p&gt;Finally, $\textit{X} \textit{X}^{-1}$, also yields the identity:&lt;/p&gt;

\[\textit{A} \textit{A}^{-1} = \textit{I}\]

&lt;h3 id=&quot;geometric-interpretation-of-eigendecomposition&quot;&gt;Geometric interpretation of Eigendecomposition&lt;/h3&gt;

&lt;p&gt;We said that Eigenbasis is a good basis as it allows us to perform computations more easily and to better understand the nature of linear mappings or transformations. The geometric interpretation of Eigendecomposition further reinforces that point. In concrete, the Eigendecomposition elements can be interpreted as follow:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$\textit{X}^{-1}$ change basis (rotation) from the standard basis into the eigenbasis&lt;/li&gt;
  &lt;li&gt;$\Lambda$ scale (stretch, shrinks, or flip) the corresponding eigenvectors&lt;/li&gt;
  &lt;li&gt;$\textit{X}$ change of basis (rotation) from the eigenbasis basis onto the original standard basis orientation&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Fig 21. illustrate the action of $\textit{X}^{-1}$, $\Lambda$, and $\textit{X}$ in a pair of vectors in the standard basis.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fig. 21: Eigendecomposition&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-10/b-eigendecomposition.svg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-problem-with-eigendecomposition&quot;&gt;The problem with Eigendecomposition&lt;/h3&gt;

&lt;p&gt;The problem is simple: &lt;strong&gt;Eigendecomposition can only be performed on square matrices, and sometimes the decomposition does not even exist&lt;/strong&gt;. This is very limiting from an applied perspective, as most practical problems involve non-square matrices.&lt;/p&gt;

&lt;p&gt;Ideally, we would like to have a more general decomposition, that allows for non-square matrices and that exist for all matrices. In the next section we introduce the &lt;strong&gt;Singular Value Decomposition&lt;/strong&gt;, which takes care of these issues.&lt;/p&gt;

&lt;h2 id=&quot;singular-value-decomposition&quot;&gt;Singular Value Decomposition&lt;/h2&gt;

&lt;p&gt;Singular Value Decomposition (SVD) is one the most relevant decomposition in applied settings, as it goes beyond the limitations of Eigendecomposition. Specifically, SVD can be performed for &lt;strong&gt;non-squared matrices and singular matrices (i.e., matrices without a full set of eigenvectors)&lt;/strong&gt;. SVD can be used for the same applications that Eigendecomposition (e.g., low-rank approximations) plus the cases for which Eigendecomposition does not work.&lt;/p&gt;

&lt;h3 id=&quot;singular-value-decomposition-theorem&quot;&gt;Singular Value Decomposition Theorem&lt;/h3&gt;

&lt;p&gt;Since we reviewed Eigendecomposition already, understanding SVD becomes easier. The SVD theorem states that any rectangular matrix $\textit{A} \in \mathbb{R}^{m \times n}$ can be decomposed as the product of an orthogonal matrix $\textit{U} \in \mathbb{R}^{m \times m}$, a diagonal matrix $\Sigma \in \mathbb{R}^{m \times m}$, and another orthogonal matrix $\textit{X}^{-1} \in \mathbb{R}^{n \times n}$:&lt;/p&gt;

\[\textit{A} := \textit{U} \Sigma \textit{X}^{-1}\]

&lt;p&gt;Another common notation is: $\textit{A} := \textit{U} \Sigma \textit{V}^{T}$. Here I’m using $\textit{X}^{-1}$ just to denote that the right orthogonal matrix is the same as in the Eigenvalue decomposition. Also notice that the inverse of an square orthogonal matrix is $\textit{X}^{-1} = \textit{X}^{T}$.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;Singular Values&lt;/em&gt; are the non-negative values along the diagonal of $\Sigma$, which play the same role as eigenvalues in Eigendecomposition. You may even find some authors call them eigenvalues as well. Since $\Sigma$ is a rectangular matrix of the shape as $\textit{A}$, the diagonal of the matrix which contains the singular values will necessarily define a square submatrix within $\Sigma$. There are two situations to pay attention to: (1) when $m &amp;gt; n$, i.e., more rows than columns, and (2) when $m &amp;lt; n$, i.e., more columns than rows.&lt;/p&gt;

&lt;p&gt;For the first case, $m &amp;gt; n$, we will have zero-padding at the bottom of $\Sigma$ as:&lt;/p&gt;

\[\Sigma =
\begin{bmatrix}
\sigma_1 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; \ddots &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; \sigma_n \\
0 &amp;amp; \cdots &amp;amp; 0 \\
\vdots &amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; \cdots &amp;amp; 0 \\
\end{bmatrix}\]

&lt;p&gt;For the second case, $m &amp;lt; n$, we will have zero-padding at the right of $\Sigma$ as:&lt;/p&gt;

\[\Sigma =
\begin{bmatrix}
\sigma_1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\
0 &amp;amp; \ddots &amp;amp; 0 &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \sigma_n &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\
\end{bmatrix}\]

&lt;p&gt;Take the case of $\textit{A}^{3 \times 2}$, the SVD is defined as:&lt;/p&gt;

\[\textit{A} = \textit{U}\Sigma\textit{V}^T=
\begin{bmatrix}
a_{11} &amp;amp; a_{12} \\
a_{21} &amp;amp; a_{22} \\
a_{31} &amp;amp; a_{32}
\end{bmatrix} =
\begin{bmatrix}
u_{11} &amp;amp; u_{12} &amp;amp; u_{13} \\
u_{21} &amp;amp; u_{22} &amp;amp; u_{23} \\
u_{31} &amp;amp; u_{32} &amp;amp; u_{33}
\end{bmatrix}
\begin{bmatrix}
\sigma_{11} &amp;amp; 0 \\
0 &amp;amp; \sigma_{22} \\
0 &amp;amp; 0
\end{bmatrix}
\begin{bmatrix}
v_{11} &amp;amp; v_{12}  \\
v_{21} &amp;amp; v_{22}  \\
\end{bmatrix}\]

&lt;p&gt;Now let’s evaluate the opposite case, $\textit{A}^{2 \times 3}$, the SVD is defined as:&lt;/p&gt;

\[\textit{A} = \textit{U}\Sigma\textit{V}^T=
\begin{bmatrix}
a_{11} &amp;amp; a_{12} &amp;amp; a_{13}  \\
a_{21} &amp;amp; a_{22} &amp;amp; a_{23}
\end{bmatrix} =
\begin{bmatrix}
u_{11} &amp;amp; u_{12}  \\
u_{21} &amp;amp; u_{22}  \\
\end{bmatrix}
\begin{bmatrix}
\sigma_{11} &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; \sigma_{22} &amp;amp; 0
\end{bmatrix}
\begin{bmatrix}
v_{11} &amp;amp; v_{12} &amp;amp; v_{13} \\
v_{21} &amp;amp; v_{22} &amp;amp; v_{23} \\
v_{31} &amp;amp; v_{32} &amp;amp; v_{33}
\end{bmatrix}\]

&lt;h3 id=&quot;singular-value-decomposition-computation&quot;&gt;Singular Value Decomposition computation&lt;/h3&gt;

&lt;p&gt;SVD computation leads to messy calculations in most cases, so this time I’ll just use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;. We will compute three cases: a wide matrix $\textit{A}^{2 \times 3}$, a tall matrix $\textit{A}^{3 \times 2}$, and a square matrix $\textit{A}^{3 \times 3}$ with a pair of linearly dependent vectors (i.e., a “defective” matrix, or singular, or not full rank, etc.).&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# 2 x 3 matrix
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A_wide&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                   &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 3 x 2 matrix
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A_tall&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                   &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                   &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 3 x 3 matrix: col 3 equals 2 x col 1
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A_square&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                     &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                     &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;U1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;S1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;V_T1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;svd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A_wide&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;U2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;S2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;V_T2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;svd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A_tall&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;U3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;S3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;V_T3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;svd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A_square&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Left orthogonal matrix wide A:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;U1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Singular values diagonal matrix wide A:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Right orthogonal matrix wide A:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;V_T1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Left orthogonal matrix wide A:
[[-0.55  0.83]
 [ 0.83  0.55]]

Singular values diagonal matrix wide A:
[3.74 1.  ]

Right orthogonal matrix wide A:
[[-0.96 -0.15  0.22]
 [-0.    0.83  0.55]
 [ 0.27 -0.53  0.8 ]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As expected, we obtain a $n \times n$ orthogonal matrix on the left, and a $m \times m$ orthogonal matrix on the right. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; only returns the singular values along the diagonal instead of the $2 \times 3$ matrix, yet it makes no difference regarding the values of the SVD.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Left orthogonal matrix for tall A:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;U2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Singular values diagonal matrix for tall A:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Right orthogonal matrix for tall A:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;V_T2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Left orthogonal matrix for tall A:
[[-0.59 -0.24 -0.77]
 [ 0.8  -0.32 -0.51]
 [-0.13 -0.91  0.38]]

Singular values diagonal matrix for tall A:
[3.67 2.13]

Right orthogonal matrix for tall A:
[[-0.97 -0.23]
 [ 0.23 -0.97]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As expected, we obtain a $m \times m$ orthogonal matrix on the left and a $n \times n$ orthogonal matrix on the right. Notice that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; returns the singular values in descending order of magnitude. This is a convention you’ll find in the literature frequently.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Left orthogonal matrix for square A:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;U3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Singular values diagonal matrix for square A:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Right orthogonal matrix for square A:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;V_T3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Left orthogonal matrix for square A:
[[-0.54 -0.2  -0.82]
 [ 0.79 -0.46 -0.41]
 [-0.29 -0.86  0.41]]

Singular values diagonal matrix for square A:
[8.44 1.95 0.  ]

Right orthogonal matrix for square A:
[[-0.44 -0.13 -0.89]
 [ 0.06 -0.99  0.12]
 [ 0.89  0.   -0.45]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Although column three is just two times column one (i.e., linearly dependent), we obtain the SVD for $\textit{A}$. Notice that the third singular value equals $0$, which is a reflection of the fact that the third column just contains redundant information.&lt;/p&gt;

&lt;h3 id=&quot;geometric-interpretation-of-the-singular-value-decomposition&quot;&gt;Geometric interpretation of the Singular Value Decomposition&lt;/h3&gt;

&lt;p&gt;As with Eigendecomposition, SVD has a nice geometric interpretation as a sequence of linear mappings or transformations. Concretely:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$\textit{V}^T$ change basis (rotation) from the standard basis into a set of orthogonal basis&lt;/li&gt;
  &lt;li&gt;$\Sigma$ scale (stretch, shrinks, or flip) the corresponding orthogonal basis&lt;/li&gt;
  &lt;li&gt;$\textit{U}$ change of basis (rotation) from the new orthogonal basis onto some other orientation, i.e., not necessarily where we started.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The key difference with Eigendecomposition is in $\textit{U}$: instead of going back to the standard basis, $\textit{U}$ performs a change of basis onto another direction.&lt;/p&gt;

&lt;p&gt;Fig 22. illustrate the effect of $\textit{A}^{3 \times 2}$, i.e., $\textit{V}^T$, $\Sigma$, and $\textit{U}$, in a pair of vectors in the standard basis. The fact that the right orthogonal matrix has $3$ column vectors generates the third dimension which is orthogonal to the ellipse surface.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fig. 22: Singular Value Decomposition&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-10/b-svd.svg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;singular-value-decomposition-vs-eigendecomposition&quot;&gt;Singular Value Decomposition vs Eigendecomposition&lt;/h3&gt;

&lt;p&gt;The SVD and Eigendecomposition are very similar, so it’s easy to get confused about how they differ. Here is a list of the most important ways on which both are different:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The SVD decomposition exist for any rectangular matrix $\in \mathbb{R}^{m \times n}$ , while the Eigendecomposition exist only for square matrices $\in \mathbb{R}^{n \times n}$.&lt;/li&gt;
  &lt;li&gt;The SVD decomposition exists even if the matrix $\textit{A}$ is defective, singular, or not full rank, whereas the Eigendecomposition does not have a solution in $\mathbb{R}$ in such a case.&lt;/li&gt;
  &lt;li&gt;Eigenvectors $\textit{X}$ are orthogonal only for &lt;em&gt;symmetric matrices&lt;/em&gt;, whereas the vectors in the $\textit{U}$ and $\textit{V}$ are orthonormal. Hence, $\textit{X}$ represents a rotation only for symmetric matrices, whereas $\textit{U}$ and $\textit{V}$ are always rotations.&lt;/li&gt;
  &lt;li&gt;In the Eigendecomposition, $\textit{X}$ and $\textit{X}^T$ are the inverse fo each other, whereas $\textit{U}$ and $\textit{V}$ in the SVD are not.&lt;/li&gt;
  &lt;li&gt;The singular values in $\Sigma$ are always real and positive, which is not necessarily the case for $\Lambda$ in the Eigendecomposition.&lt;/li&gt;
  &lt;li&gt;The SVD change basis in both the domain and codomain. The Eigendecomposition change basis in the same vector space.&lt;/li&gt;
  &lt;li&gt;For symmetric matrices, $\textit{A} \in \mathbb{R}^{n \times n}$, the SVD and Eigendecomposition yield the same results.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;matrix-approximation&quot;&gt;Matrix Approximation&lt;/h2&gt;

&lt;p&gt;In machine learning applications, it is common to find matrices with thousands, hundreds of thousands, and even millions of rows and columns. Although the Eigendecomposition and Singular Value Decomposition make matrix factorization efficient to compute, such large matrices can consume an enormous amount of time and computational resources. One common way to “get around” these issues is to utilize &lt;strong&gt;low-rank approximations&lt;/strong&gt; of the original matrices. By low-rank we mean utilizing a subset of orthogonal vectors instead of the full set of orthogonal vectors, such that we can obtain a “reasonably” good approximation of the original matrix.&lt;/p&gt;

&lt;p&gt;There are many well-known and widely use low-approximation procedures in machine learning, like Principal Component Analysis, Factor Analysis, and Latent Semantic analysis, and dimensionality reduction techniques more generally. Low-rank approximations are possible because in most instances, a small subset of vectors contains most of the information in the matrix, which is a way to say the most data points can be computed as linear combinations of a subset of orthogonal vectors.&lt;/p&gt;

&lt;h3 id=&quot;best-rank-k-approximation-with-svd&quot;&gt;Best rank-k approximation with SVD&lt;/h3&gt;

&lt;p&gt;So far we have represented the SVD as the product of three matrices, $\textit{U}$, $\Sigma$, and $\textit{V}^T$. We can represent this same computation as a the sum of the matching columns of each of these components as:&lt;/p&gt;

\[\textit{A} := \sum_{i=1}^{r} \sigma_i \textbf{u}_i \textbf{u}_i^T\]

&lt;p&gt;Notice that each iteration of $\sum_{i=1}^{r} \textbf{u}_i \textbf{u}_i^T $ will generate a matrix $\sigma_i \textit{A}_i$, which then can be multiplied by $\sigma_i$. In other words, the above expression also equals:&lt;/p&gt;

\[\sum_{i=1}^r \sigma_i \textit{A}_i\]

&lt;p&gt;In matrix notation, we can express the same idea as:&lt;/p&gt;

\[\textit{A}_k = \textit{U}_k \Sigma_k \textit{V}_k^T\]

&lt;p&gt;Now, we can approximate $\textit{A}$ by taking the sum over $k$ values instead of $r$ values. For instance, for a square matrix with $r=100$ orthogonal vectors, we can compute an approximation with the $k=5$ orthogonal vectors as:&lt;/p&gt;

\[\hat{\textit{A}} := \sum_{i=1}^{k=5} \sigma_i \textbf{u}_i \textbf{u}_i^T = \sum_{i=1}^k \sigma_i \textit{A}_i\]

&lt;p&gt;In practice, this means that we take $k=5$ orthogonal vectors from $\textit{U}$ and $\textit{V}^T$, times $5$ singular values, which requires considerably less computation and memory than the $100 \times 100$ matrix. We call this the &lt;strong&gt;best low-rank approximation&lt;/strong&gt; simply because it takes the $5$ largest singular values, which account for most of the information. Nonetheless, we still a precise way to estimate how good is our estimation, for which we need to compute the norm for $\hat{\textit{A}}$ and $\textit{A}$, and how they differ.&lt;/p&gt;

&lt;h3 id=&quot;best-low-rank-approximation-as-a-minimization-problem&quot;&gt;Best low-rank approximation as a minimization problem&lt;/h3&gt;

&lt;p&gt;In the previous section, we mentioned we need to compute some norm for $\hat{\textit{A}}$ and $\textit{A}$, and then compare. This can be conceptualized as a error minimization problem, where we search for the smallest distance between $\textit{A}$ and the low-rank approximation $\hat{\textit{A}}$. For instance, we can use the Frobenius and compute the distance between $\hat{\textit{A}}$ and $\textit{A}$ as:&lt;/p&gt;

\[\Vert \textit{A} - \hat{\textit{A}} \Vert_F\]

&lt;p&gt;Alternatively, we can compute the explained variance for the decomposition, where the highest the variance the better the approximation, ranging from $0$ to $1$. We can perform the SVD approximation with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sklearn&lt;/code&gt; as:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sklearn.decomposition&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TruncatedSVD&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;SVD1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;TruncatedSVD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_components&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;SVD5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;TruncatedSVD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_components&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;SVD10&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;TruncatedSVD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_components&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;SVD1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;SVD5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;SVD10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;TruncatedSVD(algorithm=&apos;randomized&apos;, n_components=10, n_iter=7, random_state=1,
             tol=0.0)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Explained variance by component:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;SVD approximation with 1 component:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SVD1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;explained_variance_ratio_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;SVD approximation with 5 components:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SVD5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;explained_variance_ratio_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;SVD approximation with 10 component:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SVD10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;explained_variance_ratio_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Explained variance by component:

SVD approximation with 1 component:
[0.01]

SVD approximation with 5 components:
[0.01 0.04 0.03 0.03 0.03]

SVD approximation with 10 component:
[0.01 0.04 0.03 0.03 0.03 0.03 0.03 0.03 0.03 0.03]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Singular values for each approximation:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;SVD approximation with 1 component:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SVD1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;singular_values_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;SVD approximation with 5 components:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SVD5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;singular_values_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;SVD approximation with 10 component:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SVD10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;singular_values_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Singular values for each approximation:

SVD approximation with 1 component:
[50.42]

SVD approximation with 5 components:
[50.42  5.47  5.26  5.16  5.08]

SVD approximation with 10 component:
[50.42  5.47  5.26  5.16  5.08  5.06  4.99  4.88  4.72  4.63]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Total explained variance by each approximation:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Singular values for each approximation:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;SVD approximation with 1 component:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SVD1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;explained_variance_ratio_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;SVD approximation with 5 components:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SVD5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;explained_variance_ratio_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;SVD approximation with 10 component:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SVD10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;explained_variance_ratio_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Total explained variance by each approximation:

Singular values for each approximation:

SVD approximation with 1 component:
0.01

SVD approximation with 5 components:
0.15

SVD approximation with 10 component:
0.29
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As expected, the more components (i.e., the highest the rank of the approximation), the highest the explained variance.&lt;/p&gt;

&lt;p&gt;We can compute and compare the norms by first capturing each matrix of the SVD as recovering $\hat{\textit{A}}$, then compute the Frobenius norm of the difference between $\textit{A}$ and $\hat{\textit{A}}$.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sklearn.utils.extmath&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;randomized_svd&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;U&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;S&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;V_T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;randomized_svd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_components&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;A5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;U&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;S&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;V_T&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Norm of the difference between A and rank 5 approximation:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;fro&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Norm of the difference between A and rank 5 approximation:
26.32
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This number is not very informative in itself, so we usually utilize the explained variance as an indication of how good is the low-rank approximation.&lt;/p&gt;

&lt;h1 id=&quot;epilogue&quot;&gt;Epilogue&lt;/h1&gt;

&lt;p&gt;Linear algebra is an enormous and fascinating subject. These notes are just an introduction to the subject with machine learning in mind. I am no mathematician, and I have no formal mathematical training, yet, I greatly enjoyed writing this document. I have learned quite a lot by doing it and I hope it may help others that, like me, embark on the journey of acquiring a new skill by themselves, even when such effort may seem crazy to others.&lt;/p&gt;
</description>
        <pubDate>Tue, 26 May 2020 00:00:00 +0800</pubDate>
        <link>//intro-linear-algebra</link>
        <link href="/intro-linear-algebra"/>
        <guid isPermaLink="true">/intro-linear-algebra</guid>
      </item>
    
      <item>
        <title>Introduction to Linear Regression - mathematics and application with Python</title>
        <description>&lt;iframe src=&quot;https://github.com/sponsors/pabloinsente/card&quot; title=&quot;Sponsor pabloinsente&quot; height=&quot;225&quot; width=&quot;600&quot; style=&quot;border: 0;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Linear regression is among the most widely used tools in machine learning. Linear models are &lt;em&gt;linear&lt;/em&gt; simply because the outputs are modeled as &lt;em&gt;linear combinations&lt;/em&gt; of input vectors. Hence, we want to learn a function $f$ that describes with as little error as possible, the linear relationship between inputs and outputs.&lt;/p&gt;

&lt;h2 id=&quot;model-definition&quot;&gt;Model definition&lt;/h2&gt;

&lt;p&gt;Consider a matrix of inputs $\textit{X} \in \mathbb{R}^{m\times n}$, a vector of weights $\bf{w} \in \mathbb{R}^n$, and output vector $\bf{y} \in \mathbb{R}$. We predict $\bf{y}$ given $\textit{X}$ as:&lt;/p&gt;

\[\hat{y} = \hat{w}_0 + \sum_{j=1}^nx_j\hat{w}_j = \hat{w}_0 + \hat{w}_1x_1 + \cdots + \hat{w}_nx_n\]

&lt;p&gt;Where $\hat{w_0}$ is the &lt;em&gt;bias&lt;/em&gt; or &lt;em&gt;intercept&lt;/em&gt;. Note we add a “hat” to the unknown estimated parameters to distinguish them from known given values. To express a linear regression in matrix notation, we can incorporate a constant vector $x_i=1$ to $\textit{X}$, and the bias $\hat{w_0}$ into the vector of weights $\hat{w}$, to obtain:&lt;/p&gt;

\[\hat{y} = X^T\bf{\hat{w}}\]

&lt;p&gt;&lt;img src=&quot;/assets/post-11/b-lin-regression.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note that in the absence of the bias term, our solution will be forced to pass through the origin of the coordinate space, forming a subspace. Adding the bias term allows our solution to be detached of such constrain, forming an affine set.&lt;/p&gt;

&lt;h2 id=&quot;cost-function&quot;&gt;Cost function&lt;/h2&gt;

&lt;p&gt;The goal of our model is to find a set of weights that minimizes some measure of error or cost function. The most popular measure of error is the &lt;em&gt;Sum of Squared Errors&lt;/em&gt; (SSE), sometimes referred to as &lt;em&gt;Residual Sum of Squares&lt;/em&gt; (RSS). The expression describing the SSE is:&lt;/p&gt;

\[SSE(w) = \sum_{i=1}^n(y_i - x_i^Tw)^2\]

&lt;p&gt;In words: we take the squared difference between the target and predicted value for the $i$ row of $\textit{X}$ and sum up the result. Again, we can express this in matrix notation as:&lt;/p&gt;

\[SSE(w)= (y-\textit{X}w)^T(y-\textit{X}b)\]

&lt;p&gt;In machine learning is common to take the mean of the SSE to obtain the Mean Squared Error (MSE) as:&lt;/p&gt;

\[MSE(w) = \frac{1}{n} \sum_{i=1}^n(y_i - x_i^Tw)^2\]

&lt;h2 id=&quot;model-training&quot;&gt;Model training&lt;/h2&gt;

&lt;p&gt;Given that the SSE is a quadratic function of the weights, the error surface is convex, hence always has a minimum. There are multiple ways to adjust the weights to minimize our measure of error. One way, know as &lt;em&gt;closed-form solution&lt;/em&gt; or &lt;em&gt;normal equations&lt;/em&gt;, is to solve for where the derivatives with respect to the weights are $0$:&lt;/p&gt;

\[\Delta_wSSE=0\]

&lt;p&gt;Using the matrix notation, we solve for $w$ as:&lt;/p&gt;

\[\begin{align}
(y-\textit{X}w)^T(y-\textit{X}b) &amp;amp;= 0 \\
(y^T-\textit{X}^Tw^T)(y-\textit{X}b) &amp;amp;= 0 \\
(w^T \textit{X}^T \textit{X}w - 2w^T\textit{X}y + y^Ty) &amp;amp;= 0 \\
2\textit{X}^T \textit{X} w- 2w^T\textit{X}y &amp;amp;= 0 \\
(\textit{X}^T \textit{X})^{-1} \textit{X}^T y &amp;amp;= w \\
\end{align}\]

&lt;p&gt;Note that the solution $(\textit{X}^T \textit{X})^{-1} \textit{X}^T y = w$ works only if $\textit{X}$ is &lt;em&gt;nonsingular&lt;/em&gt; or &lt;em&gt;invertible&lt;/em&gt; (see &lt;a href=&quot;https://en.wikipedia.org/wiki/Invertible_matrix&quot;&gt;here&lt;/a&gt;). Geometrically, this means that each vector in $\textit{X}$ is independent of each other. Otherwise, we can compute the &lt;em&gt;minimum norm solution&lt;/em&gt; for the singular case (see &lt;a href=&quot;https://see.stanford.edu/materials/lsoeldsee263/08-min-norm.pdf&quot;&gt;here&lt;/a&gt;). If you are familiar with iterative methods, you can think of the closed-form solution as a one-step solution.&lt;/p&gt;

&lt;p&gt;A different approach to solve a linear model with iterative methods like &lt;em&gt;gradient descent&lt;/em&gt;. This method is preferred when the matrix is large, as inverting large matrices is computationally expensive. I won’t describe gradient descent in this section (which you can review &lt;a href=&quot;https://www.youtube.com/watch?v=IHZwWFHWa-w&quot;&gt;here&lt;/a&gt;), to maintain our focus in the linear regression problem.&lt;/p&gt;

&lt;h2 id=&quot;simple-linear-regression-example&quot;&gt;Simple linear regression example&lt;/h2&gt;

&lt;p&gt;Let’s try out a simple linear regression example with Python and sklearn. By simple, I mean one feature and one output. In the next section will do a multivariable or multi-feature regression.&lt;/p&gt;

&lt;p&gt;We will load the &lt;em&gt;Boston house prices dataset&lt;/em&gt; from sklearn. This dataset contains 506 rows (houses), 13 columns (houses features). The targets (house prices) range from 5 to 50. Our goal is just to show how to run a linear regression with sklearn, so we won’t do an exploratory data analysis this time. A detailed description of the dataset attributes can be found &lt;a href=&quot;https://scikit-learn.org/stable/datasets/index.html#boston-dataset&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Libraries for this section
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sklearn.datasets&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_boston&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sklearn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linear_model&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sklearn.metrics&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean_squared_error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r2_score&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sklearn.model_selection&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;altair&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;themes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;enable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;dark&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ThemeRegistry.enable(&apos;dark&apos;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We first load the dataset using sklearn API&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;load_boston&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;return_X_y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Dataset shape: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Dataset shape: (506, 13)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We split our data into training and testing sets, using a 80/20 split.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train_test_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Training data shape: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Testing data shape: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Training labels shape: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Testing labels shape: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Training data shape: (404, 13)
Testing data shape: (102, 13)
Training labels shape: (404,)
Testing labels shape: (102,)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Since we want to run a regression with a single feature predictor, let’s compute the correlation coefficients for each feature and the target (house prices).&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;corr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;corrcoef&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;column_stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;13&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;max_abs_corr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;absolute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;corr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;13&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;correlation coefficient features and house prices: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;corr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;feature with max absolute correlation with house prices: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_abs_corr&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;correlation coefficient features and house prices: [-0.39  0.36 -0.48  0.18 -0.43  0.7  -0.38  0.25 -0.38 -0.47 -0.51  0.33
 -0.74  1.  ]
feature with max absolute correlation with house prices: 12
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Feature number 12 has the maximum absolute correlation with house prices, $-0.74$. According to the documentation, this is the  &lt;strong&gt;% lower status of the population&lt;/strong&gt;: the more low-status people around, the lower the house price.&lt;/p&gt;

&lt;p&gt;Now we fit the model using the training set.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linear_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;LinearRegression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# pick all the rows for the 12 variable
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The model has learned the coefficients or weights $w$ that best fit the data, which we can use to make predictions on the testing set.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# pick all the rows for the 12 variable
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We evaluate the overall performance by computing the SSE, MSE, and the $R^2$ coefficient of determination.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;SSE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;MSE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;mean_squared_error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;R2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;r2_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Sum of Squared Errors: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SSE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Mean of Squared Errors: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MSE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;R2 coefficient of determination: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;R2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Sum of Squared Errors: 4726.3
Mean of Squared Errors: 46.34
R2 coefficient of determination: 0.43
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Based on a single feature, we obtain a $SSE\approx4726.3$, a $MSE\approx46.34$, and a $R^2\approx0.43$.&lt;/p&gt;

&lt;p&gt;Finally, let’s visualize the regression line for this pair of variables.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;low-status-pop&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;house-prices&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;chart&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mark_point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;fuchsia&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;low-status-pop&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;house-prices&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;chart&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;transform_regression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;low-status-pop&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;house-prices&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mark_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;yellow&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div id=&quot;altair-viz-9e02787248644f9eae4df3e50966b314&quot;&gt;&lt;/div&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
  (function(spec, embedOpt){
    let outputDiv = document.currentScript.previousElementSibling;
    if (outputDiv.id !== &quot;altair-viz-9e02787248644f9eae4df3e50966b314&quot;) {
      outputDiv = document.getElementById(&quot;altair-viz-9e02787248644f9eae4df3e50966b314&quot;);
    }
    const paths = {
      &quot;vega&quot;: &quot;https://cdn.jsdelivr.net/npm//vega@5?noext&quot;,
      &quot;vega-lib&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-lib?noext&quot;,
      &quot;vega-lite&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext&quot;,
      &quot;vega-embed&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-embed@6?noext&quot;,
    };

    function loadScript(lib) {
      return new Promise(function(resolve, reject) {
        var s = document.createElement(&apos;script&apos;);
        s.src = paths[lib];
        s.async = true;
        s.onload = () =&gt; resolve(paths[lib]);
        s.onerror = () =&gt; reject(`Error loading script: ${paths[lib]}`);
        document.getElementsByTagName(&quot;head&quot;)[0].appendChild(s);
      });
    }

    function showError(err) {
      outputDiv.innerHTML = `&lt;div class=&quot;error&quot; style=&quot;color:red;&quot;&gt;${err}&lt;/div&gt;`;
      throw err;
    }

    function displayChart(vegaEmbed) {
      vegaEmbed(outputDiv, spec, embedOpt)
        .catch(err =&gt; showError(`Javascript Error: ${err.message}&lt;br&gt;This usually means there&apos;s a typo in your chart specification. See the javascript console for the full traceback.`));
    }

    if(typeof define === &quot;function&quot; &amp;&amp; define.amd) {
      requirejs.config({paths});
      require([&quot;vega-embed&quot;], displayChart, err =&gt; showError(`Error loading script: ${err.message}`));
    } else if (typeof vegaEmbed === &quot;function&quot;) {
      displayChart(vegaEmbed);
    } else {
      loadScript(&quot;vega&quot;)
        .then(() =&gt; loadScript(&quot;vega-lite&quot;))
        .then(() =&gt; loadScript(&quot;vega-embed&quot;))
        .catch(showError)
        .then(() =&gt; displayChart(vegaEmbed));
    }
  })({&quot;usermeta&quot;: {&quot;embedOptions&quot;: {&quot;theme&quot;: &quot;dark&quot;}}, &quot;config&quot;: {&quot;view&quot;: {&quot;continuousWidth&quot;: 400, &quot;continuousHeight&quot;: 300}}, &quot;layer&quot;: [{&quot;mark&quot;: {&quot;type&quot;: &quot;point&quot;, &quot;color&quot;: &quot;fuchsia&quot;}, &quot;encoding&quot;: {&quot;x&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;low-status-pop&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;house-prices&quot;}}}, {&quot;mark&quot;: {&quot;type&quot;: &quot;line&quot;, &quot;color&quot;: &quot;yellow&quot;}, &quot;encoding&quot;: {&quot;x&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;low-status-pop&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;house-prices&quot;}}, &quot;transform&quot;: [{&quot;on&quot;: &quot;low-status-pop&quot;, &quot;regression&quot;: &quot;house-prices&quot;}]}], &quot;data&quot;: {&quot;name&quot;: &quot;data-3c42feedf3ed9a0d516ec43eb701840b&quot;}, &quot;$schema&quot;: &quot;https://vega.github.io/schema/vega-lite/v4.8.1.json&quot;, &quot;datasets&quot;: {&quot;data-3c42feedf3ed9a0d516ec43eb701840b&quot;: [{&quot;low-status-pop&quot;: 4.98, &quot;house-prices&quot;: 24.0}, {&quot;low-status-pop&quot;: 9.14, &quot;house-prices&quot;: 21.6}, {&quot;low-status-pop&quot;: 4.03, &quot;house-prices&quot;: 34.7}, {&quot;low-status-pop&quot;: 2.94, &quot;house-prices&quot;: 33.4}, {&quot;low-status-pop&quot;: 5.33, &quot;house-prices&quot;: 36.2}, {&quot;low-status-pop&quot;: 5.21, &quot;house-prices&quot;: 28.7}, {&quot;low-status-pop&quot;: 12.43, &quot;house-prices&quot;: 22.9}, {&quot;low-status-pop&quot;: 19.15, &quot;house-prices&quot;: 27.1}, {&quot;low-status-pop&quot;: 29.93, &quot;house-prices&quot;: 16.5}, {&quot;low-status-pop&quot;: 17.1, &quot;house-prices&quot;: 18.9}, {&quot;low-status-pop&quot;: 20.45, &quot;house-prices&quot;: 15.0}, {&quot;low-status-pop&quot;: 13.27, &quot;house-prices&quot;: 18.9}, {&quot;low-status-pop&quot;: 15.71, &quot;house-prices&quot;: 21.7}, {&quot;low-status-pop&quot;: 8.26, &quot;house-prices&quot;: 20.4}, {&quot;low-status-pop&quot;: 10.26, &quot;house-prices&quot;: 18.2}, {&quot;low-status-pop&quot;: 8.47, &quot;house-prices&quot;: 19.9}, {&quot;low-status-pop&quot;: 6.58, &quot;house-prices&quot;: 23.1}, {&quot;low-status-pop&quot;: 14.67, &quot;house-prices&quot;: 17.5}, {&quot;low-status-pop&quot;: 11.69, &quot;house-prices&quot;: 20.2}, {&quot;low-status-pop&quot;: 11.28, &quot;house-prices&quot;: 18.2}, {&quot;low-status-pop&quot;: 21.02, &quot;house-prices&quot;: 13.6}, {&quot;low-status-pop&quot;: 13.83, &quot;house-prices&quot;: 19.6}, {&quot;low-status-pop&quot;: 18.72, &quot;house-prices&quot;: 15.2}, {&quot;low-status-pop&quot;: 19.88, &quot;house-prices&quot;: 14.5}, {&quot;low-status-pop&quot;: 16.3, &quot;house-prices&quot;: 15.6}, {&quot;low-status-pop&quot;: 16.51, &quot;house-prices&quot;: 13.9}, {&quot;low-status-pop&quot;: 14.81, &quot;house-prices&quot;: 16.6}, {&quot;low-status-pop&quot;: 17.28, &quot;house-prices&quot;: 14.8}, {&quot;low-status-pop&quot;: 12.8, &quot;house-prices&quot;: 18.4}, {&quot;low-status-pop&quot;: 11.98, &quot;house-prices&quot;: 21.0}, {&quot;low-status-pop&quot;: 22.6, &quot;house-prices&quot;: 12.7}, {&quot;low-status-pop&quot;: 13.04, &quot;house-prices&quot;: 14.5}, {&quot;low-status-pop&quot;: 27.71, &quot;house-prices&quot;: 13.2}, {&quot;low-status-pop&quot;: 18.35, &quot;house-prices&quot;: 13.1}, {&quot;low-status-pop&quot;: 20.34, &quot;house-prices&quot;: 13.5}, {&quot;low-status-pop&quot;: 9.68, &quot;house-prices&quot;: 18.9}, {&quot;low-status-pop&quot;: 11.41, &quot;house-prices&quot;: 20.0}, {&quot;low-status-pop&quot;: 8.77, &quot;house-prices&quot;: 21.0}, {&quot;low-status-pop&quot;: 10.13, &quot;house-prices&quot;: 24.7}, {&quot;low-status-pop&quot;: 4.32, &quot;house-prices&quot;: 30.8}, {&quot;low-status-pop&quot;: 1.98, &quot;house-prices&quot;: 34.9}, {&quot;low-status-pop&quot;: 4.84, &quot;house-prices&quot;: 26.6}, {&quot;low-status-pop&quot;: 5.81, &quot;house-prices&quot;: 25.3}, {&quot;low-status-pop&quot;: 7.44, &quot;house-prices&quot;: 24.7}, {&quot;low-status-pop&quot;: 9.55, &quot;house-prices&quot;: 21.2}, {&quot;low-status-pop&quot;: 10.21, &quot;house-prices&quot;: 19.3}, {&quot;low-status-pop&quot;: 14.15, &quot;house-prices&quot;: 20.0}, {&quot;low-status-pop&quot;: 18.8, &quot;house-prices&quot;: 16.6}, {&quot;low-status-pop&quot;: 30.81, &quot;house-prices&quot;: 14.4}, {&quot;low-status-pop&quot;: 16.2, &quot;house-prices&quot;: 19.4}, {&quot;low-status-pop&quot;: 13.45, &quot;house-prices&quot;: 19.7}, {&quot;low-status-pop&quot;: 9.43, &quot;house-prices&quot;: 20.5}, {&quot;low-status-pop&quot;: 5.28, &quot;house-prices&quot;: 25.0}, {&quot;low-status-pop&quot;: 8.43, &quot;house-prices&quot;: 23.4}, {&quot;low-status-pop&quot;: 14.8, &quot;house-prices&quot;: 18.9}, {&quot;low-status-pop&quot;: 4.81, &quot;house-prices&quot;: 35.4}, {&quot;low-status-pop&quot;: 5.77, &quot;house-prices&quot;: 24.7}, {&quot;low-status-pop&quot;: 3.95, &quot;house-prices&quot;: 31.6}, {&quot;low-status-pop&quot;: 6.86, &quot;house-prices&quot;: 23.3}, {&quot;low-status-pop&quot;: 9.22, &quot;house-prices&quot;: 19.6}, {&quot;low-status-pop&quot;: 13.15, &quot;house-prices&quot;: 18.7}, {&quot;low-status-pop&quot;: 14.44, &quot;house-prices&quot;: 16.0}, {&quot;low-status-pop&quot;: 6.73, &quot;house-prices&quot;: 22.2}, {&quot;low-status-pop&quot;: 9.5, &quot;house-prices&quot;: 25.0}, {&quot;low-status-pop&quot;: 8.05, &quot;house-prices&quot;: 33.0}, {&quot;low-status-pop&quot;: 4.67, &quot;house-prices&quot;: 23.5}, {&quot;low-status-pop&quot;: 10.24, &quot;house-prices&quot;: 19.4}, {&quot;low-status-pop&quot;: 8.1, &quot;house-prices&quot;: 22.0}, {&quot;low-status-pop&quot;: 13.09, &quot;house-prices&quot;: 17.4}, {&quot;low-status-pop&quot;: 8.79, &quot;house-prices&quot;: 20.9}, {&quot;low-status-pop&quot;: 6.72, &quot;house-prices&quot;: 24.2}, {&quot;low-status-pop&quot;: 9.88, &quot;house-prices&quot;: 21.7}, {&quot;low-status-pop&quot;: 5.52, &quot;house-prices&quot;: 22.8}, {&quot;low-status-pop&quot;: 7.54, &quot;house-prices&quot;: 23.4}, {&quot;low-status-pop&quot;: 6.78, &quot;house-prices&quot;: 24.1}, {&quot;low-status-pop&quot;: 8.94, &quot;house-prices&quot;: 21.4}, {&quot;low-status-pop&quot;: 11.97, &quot;house-prices&quot;: 20.0}, {&quot;low-status-pop&quot;: 10.27, &quot;house-prices&quot;: 20.8}, {&quot;low-status-pop&quot;: 12.34, &quot;house-prices&quot;: 21.2}, {&quot;low-status-pop&quot;: 9.1, &quot;house-prices&quot;: 20.3}, {&quot;low-status-pop&quot;: 5.29, &quot;house-prices&quot;: 28.0}, {&quot;low-status-pop&quot;: 7.22, &quot;house-prices&quot;: 23.9}, {&quot;low-status-pop&quot;: 6.72, &quot;house-prices&quot;: 24.8}, {&quot;low-status-pop&quot;: 7.51, &quot;house-prices&quot;: 22.9}, {&quot;low-status-pop&quot;: 9.62, &quot;house-prices&quot;: 23.9}, {&quot;low-status-pop&quot;: 6.53, &quot;house-prices&quot;: 26.6}, {&quot;low-status-pop&quot;: 12.86, &quot;house-prices&quot;: 22.5}, {&quot;low-status-pop&quot;: 8.44, &quot;house-prices&quot;: 22.2}, {&quot;low-status-pop&quot;: 5.5, &quot;house-prices&quot;: 23.6}, {&quot;low-status-pop&quot;: 5.7, &quot;house-prices&quot;: 28.7}, {&quot;low-status-pop&quot;: 8.81, &quot;house-prices&quot;: 22.6}, {&quot;low-status-pop&quot;: 8.2, &quot;house-prices&quot;: 22.0}, {&quot;low-status-pop&quot;: 8.16, &quot;house-prices&quot;: 22.9}, {&quot;low-status-pop&quot;: 6.21, &quot;house-prices&quot;: 25.0}, {&quot;low-status-pop&quot;: 10.59, &quot;house-prices&quot;: 20.6}, {&quot;low-status-pop&quot;: 6.65, &quot;house-prices&quot;: 28.4}, {&quot;low-status-pop&quot;: 11.34, &quot;house-prices&quot;: 21.4}, {&quot;low-status-pop&quot;: 4.21, &quot;house-prices&quot;: 38.7}, {&quot;low-status-pop&quot;: 3.57, &quot;house-prices&quot;: 43.8}, {&quot;low-status-pop&quot;: 6.19, &quot;house-prices&quot;: 33.2}, {&quot;low-status-pop&quot;: 9.42, &quot;house-prices&quot;: 27.5}, {&quot;low-status-pop&quot;: 7.67, &quot;house-prices&quot;: 26.5}, {&quot;low-status-pop&quot;: 10.63, &quot;house-prices&quot;: 18.6}, {&quot;low-status-pop&quot;: 13.44, &quot;house-prices&quot;: 19.3}, {&quot;low-status-pop&quot;: 12.33, &quot;house-prices&quot;: 20.1}, {&quot;low-status-pop&quot;: 16.47, &quot;house-prices&quot;: 19.5}, {&quot;low-status-pop&quot;: 18.66, &quot;house-prices&quot;: 19.5}, {&quot;low-status-pop&quot;: 14.09, &quot;house-prices&quot;: 20.4}, {&quot;low-status-pop&quot;: 12.27, &quot;house-prices&quot;: 19.8}, {&quot;low-status-pop&quot;: 15.55, &quot;house-prices&quot;: 19.4}, {&quot;low-status-pop&quot;: 13.0, &quot;house-prices&quot;: 21.7}, {&quot;low-status-pop&quot;: 10.16, &quot;house-prices&quot;: 22.8}, {&quot;low-status-pop&quot;: 16.21, &quot;house-prices&quot;: 18.8}, {&quot;low-status-pop&quot;: 17.09, &quot;house-prices&quot;: 18.7}, {&quot;low-status-pop&quot;: 10.45, &quot;house-prices&quot;: 18.5}, {&quot;low-status-pop&quot;: 15.76, &quot;house-prices&quot;: 18.3}, {&quot;low-status-pop&quot;: 12.04, &quot;house-prices&quot;: 21.2}, {&quot;low-status-pop&quot;: 10.3, &quot;house-prices&quot;: 19.2}, {&quot;low-status-pop&quot;: 15.37, &quot;house-prices&quot;: 20.4}, {&quot;low-status-pop&quot;: 13.61, &quot;house-prices&quot;: 19.3}, {&quot;low-status-pop&quot;: 14.37, &quot;house-prices&quot;: 22.0}, {&quot;low-status-pop&quot;: 14.27, &quot;house-prices&quot;: 20.3}, {&quot;low-status-pop&quot;: 17.93, &quot;house-prices&quot;: 20.5}, {&quot;low-status-pop&quot;: 25.41, &quot;house-prices&quot;: 17.3}, {&quot;low-status-pop&quot;: 17.58, &quot;house-prices&quot;: 18.8}, {&quot;low-status-pop&quot;: 14.81, &quot;house-prices&quot;: 21.4}, {&quot;low-status-pop&quot;: 27.26, &quot;house-prices&quot;: 15.7}, {&quot;low-status-pop&quot;: 17.19, &quot;house-prices&quot;: 16.2}, {&quot;low-status-pop&quot;: 15.39, &quot;house-prices&quot;: 18.0}, {&quot;low-status-pop&quot;: 18.34, &quot;house-prices&quot;: 14.3}, {&quot;low-status-pop&quot;: 12.6, &quot;house-prices&quot;: 19.2}, {&quot;low-status-pop&quot;: 12.26, &quot;house-prices&quot;: 19.6}, {&quot;low-status-pop&quot;: 11.12, &quot;house-prices&quot;: 23.0}, {&quot;low-status-pop&quot;: 15.03, &quot;house-prices&quot;: 18.4}, {&quot;low-status-pop&quot;: 17.31, &quot;house-prices&quot;: 15.6}, {&quot;low-status-pop&quot;: 16.96, &quot;house-prices&quot;: 18.1}, {&quot;low-status-pop&quot;: 16.9, &quot;house-prices&quot;: 17.4}, {&quot;low-status-pop&quot;: 14.59, &quot;house-prices&quot;: 17.1}, {&quot;low-status-pop&quot;: 21.32, &quot;house-prices&quot;: 13.3}, {&quot;low-status-pop&quot;: 18.46, &quot;house-prices&quot;: 17.8}, {&quot;low-status-pop&quot;: 24.16, &quot;house-prices&quot;: 14.0}, {&quot;low-status-pop&quot;: 34.41, &quot;house-prices&quot;: 14.4}, {&quot;low-status-pop&quot;: 26.82, &quot;house-prices&quot;: 13.4}, {&quot;low-status-pop&quot;: 26.42, &quot;house-prices&quot;: 15.6}, {&quot;low-status-pop&quot;: 29.29, &quot;house-prices&quot;: 11.8}, {&quot;low-status-pop&quot;: 27.8, &quot;house-prices&quot;: 13.8}, {&quot;low-status-pop&quot;: 16.65, &quot;house-prices&quot;: 15.6}, {&quot;low-status-pop&quot;: 29.53, &quot;house-prices&quot;: 14.6}, {&quot;low-status-pop&quot;: 28.32, &quot;house-prices&quot;: 17.8}, {&quot;low-status-pop&quot;: 21.45, &quot;house-prices&quot;: 15.4}, {&quot;low-status-pop&quot;: 14.1, &quot;house-prices&quot;: 21.5}, {&quot;low-status-pop&quot;: 13.28, &quot;house-prices&quot;: 19.6}, {&quot;low-status-pop&quot;: 12.12, &quot;house-prices&quot;: 15.3}, {&quot;low-status-pop&quot;: 15.79, &quot;house-prices&quot;: 19.4}, {&quot;low-status-pop&quot;: 15.12, &quot;house-prices&quot;: 17.0}, {&quot;low-status-pop&quot;: 15.02, &quot;house-prices&quot;: 15.6}, {&quot;low-status-pop&quot;: 16.14, &quot;house-prices&quot;: 13.1}, {&quot;low-status-pop&quot;: 4.59, &quot;house-prices&quot;: 41.3}, {&quot;low-status-pop&quot;: 6.43, &quot;house-prices&quot;: 24.3}, {&quot;low-status-pop&quot;: 7.39, &quot;house-prices&quot;: 23.3}, {&quot;low-status-pop&quot;: 5.5, &quot;house-prices&quot;: 27.0}, {&quot;low-status-pop&quot;: 1.73, &quot;house-prices&quot;: 50.0}, {&quot;low-status-pop&quot;: 1.92, &quot;house-prices&quot;: 50.0}, {&quot;low-status-pop&quot;: 3.32, &quot;house-prices&quot;: 50.0}, {&quot;low-status-pop&quot;: 11.64, &quot;house-prices&quot;: 22.7}, {&quot;low-status-pop&quot;: 9.81, &quot;house-prices&quot;: 25.0}, {&quot;low-status-pop&quot;: 3.7, &quot;house-prices&quot;: 50.0}, {&quot;low-status-pop&quot;: 12.14, &quot;house-prices&quot;: 23.8}, {&quot;low-status-pop&quot;: 11.1, &quot;house-prices&quot;: 23.8}, {&quot;low-status-pop&quot;: 11.32, &quot;house-prices&quot;: 22.3}, {&quot;low-status-pop&quot;: 14.43, &quot;house-prices&quot;: 17.4}, {&quot;low-status-pop&quot;: 12.03, &quot;house-prices&quot;: 19.1}, {&quot;low-status-pop&quot;: 14.69, &quot;house-prices&quot;: 23.1}, {&quot;low-status-pop&quot;: 9.04, &quot;house-prices&quot;: 23.6}, {&quot;low-status-pop&quot;: 9.64, &quot;house-prices&quot;: 22.6}, {&quot;low-status-pop&quot;: 5.33, &quot;house-prices&quot;: 29.4}, {&quot;low-status-pop&quot;: 10.11, &quot;house-prices&quot;: 23.2}, {&quot;low-status-pop&quot;: 6.29, &quot;house-prices&quot;: 24.6}, {&quot;low-status-pop&quot;: 6.92, &quot;house-prices&quot;: 29.9}, {&quot;low-status-pop&quot;: 5.04, &quot;house-prices&quot;: 37.2}, {&quot;low-status-pop&quot;: 7.56, &quot;house-prices&quot;: 39.8}, {&quot;low-status-pop&quot;: 9.45, &quot;house-prices&quot;: 36.2}, {&quot;low-status-pop&quot;: 4.82, &quot;house-prices&quot;: 37.9}, {&quot;low-status-pop&quot;: 5.68, &quot;house-prices&quot;: 32.5}, {&quot;low-status-pop&quot;: 13.98, &quot;house-prices&quot;: 26.4}, {&quot;low-status-pop&quot;: 13.15, &quot;house-prices&quot;: 29.6}, {&quot;low-status-pop&quot;: 4.45, &quot;house-prices&quot;: 50.0}, {&quot;low-status-pop&quot;: 6.68, &quot;house-prices&quot;: 32.0}, {&quot;low-status-pop&quot;: 4.56, &quot;house-prices&quot;: 29.8}, {&quot;low-status-pop&quot;: 5.39, &quot;house-prices&quot;: 34.9}, {&quot;low-status-pop&quot;: 5.1, &quot;house-prices&quot;: 37.0}, {&quot;low-status-pop&quot;: 4.69, &quot;house-prices&quot;: 30.5}, {&quot;low-status-pop&quot;: 2.87, &quot;house-prices&quot;: 36.4}, {&quot;low-status-pop&quot;: 5.03, &quot;house-prices&quot;: 31.1}, {&quot;low-status-pop&quot;: 4.38, &quot;house-prices&quot;: 29.1}, {&quot;low-status-pop&quot;: 2.97, &quot;house-prices&quot;: 50.0}, {&quot;low-status-pop&quot;: 4.08, &quot;house-prices&quot;: 33.3}, {&quot;low-status-pop&quot;: 8.61, &quot;house-prices&quot;: 30.3}, {&quot;low-status-pop&quot;: 6.62, &quot;house-prices&quot;: 34.6}, {&quot;low-status-pop&quot;: 4.56, &quot;house-prices&quot;: 34.9}, {&quot;low-status-pop&quot;: 4.45, &quot;house-prices&quot;: 32.9}, {&quot;low-status-pop&quot;: 7.43, &quot;house-prices&quot;: 24.1}, {&quot;low-status-pop&quot;: 3.11, &quot;house-prices&quot;: 42.3}, {&quot;low-status-pop&quot;: 3.81, &quot;house-prices&quot;: 48.5}, {&quot;low-status-pop&quot;: 2.88, &quot;house-prices&quot;: 50.0}, {&quot;low-status-pop&quot;: 10.87, &quot;house-prices&quot;: 22.6}, {&quot;low-status-pop&quot;: 10.97, &quot;house-prices&quot;: 24.4}, {&quot;low-status-pop&quot;: 18.06, &quot;house-prices&quot;: 22.5}, {&quot;low-status-pop&quot;: 14.66, &quot;house-prices&quot;: 24.4}, {&quot;low-status-pop&quot;: 23.09, &quot;house-prices&quot;: 20.0}, {&quot;low-status-pop&quot;: 17.27, &quot;house-prices&quot;: 21.7}, {&quot;low-status-pop&quot;: 23.98, &quot;house-prices&quot;: 19.3}, {&quot;low-status-pop&quot;: 16.03, &quot;house-prices&quot;: 22.4}, {&quot;low-status-pop&quot;: 9.38, &quot;house-prices&quot;: 28.1}, {&quot;low-status-pop&quot;: 29.55, &quot;house-prices&quot;: 23.7}, {&quot;low-status-pop&quot;: 9.47, &quot;house-prices&quot;: 25.0}, {&quot;low-status-pop&quot;: 13.51, &quot;house-prices&quot;: 23.3}, {&quot;low-status-pop&quot;: 9.69, &quot;house-prices&quot;: 28.7}, {&quot;low-status-pop&quot;: 17.92, &quot;house-prices&quot;: 21.5}, {&quot;low-status-pop&quot;: 10.5, &quot;house-prices&quot;: 23.0}, {&quot;low-status-pop&quot;: 9.71, &quot;house-prices&quot;: 26.7}, {&quot;low-status-pop&quot;: 21.46, &quot;house-prices&quot;: 21.7}, {&quot;low-status-pop&quot;: 9.93, &quot;house-prices&quot;: 27.5}, {&quot;low-status-pop&quot;: 7.6, &quot;house-prices&quot;: 30.1}, {&quot;low-status-pop&quot;: 4.14, &quot;house-prices&quot;: 44.8}, {&quot;low-status-pop&quot;: 4.63, &quot;house-prices&quot;: 50.0}, {&quot;low-status-pop&quot;: 3.13, &quot;house-prices&quot;: 37.6}, {&quot;low-status-pop&quot;: 6.36, &quot;house-prices&quot;: 31.6}, {&quot;low-status-pop&quot;: 3.92, &quot;house-prices&quot;: 46.7}, {&quot;low-status-pop&quot;: 3.76, &quot;house-prices&quot;: 31.5}, {&quot;low-status-pop&quot;: 11.65, &quot;house-prices&quot;: 24.3}, {&quot;low-status-pop&quot;: 5.25, &quot;house-prices&quot;: 31.7}, {&quot;low-status-pop&quot;: 2.47, &quot;house-prices&quot;: 41.7}, {&quot;low-status-pop&quot;: 3.95, &quot;house-prices&quot;: 48.3}, {&quot;low-status-pop&quot;: 8.05, &quot;house-prices&quot;: 29.0}, {&quot;low-status-pop&quot;: 10.88, &quot;house-prices&quot;: 24.0}, {&quot;low-status-pop&quot;: 9.54, &quot;house-prices&quot;: 25.1}, {&quot;low-status-pop&quot;: 4.73, &quot;house-prices&quot;: 31.5}, {&quot;low-status-pop&quot;: 6.36, &quot;house-prices&quot;: 23.7}, {&quot;low-status-pop&quot;: 7.37, &quot;house-prices&quot;: 23.3}, {&quot;low-status-pop&quot;: 11.38, &quot;house-prices&quot;: 22.0}, {&quot;low-status-pop&quot;: 12.4, &quot;house-prices&quot;: 20.1}, {&quot;low-status-pop&quot;: 11.22, &quot;house-prices&quot;: 22.2}, {&quot;low-status-pop&quot;: 5.19, &quot;house-prices&quot;: 23.7}, {&quot;low-status-pop&quot;: 12.5, &quot;house-prices&quot;: 17.6}, {&quot;low-status-pop&quot;: 18.46, &quot;house-prices&quot;: 18.5}, {&quot;low-status-pop&quot;: 9.16, &quot;house-prices&quot;: 24.3}, {&quot;low-status-pop&quot;: 10.15, &quot;house-prices&quot;: 20.5}, {&quot;low-status-pop&quot;: 9.52, &quot;house-prices&quot;: 24.5}, {&quot;low-status-pop&quot;: 6.56, &quot;house-prices&quot;: 26.2}, {&quot;low-status-pop&quot;: 5.9, &quot;house-prices&quot;: 24.4}, {&quot;low-status-pop&quot;: 3.59, &quot;house-prices&quot;: 24.8}, {&quot;low-status-pop&quot;: 3.53, &quot;house-prices&quot;: 29.6}, {&quot;low-status-pop&quot;: 3.54, &quot;house-prices&quot;: 42.8}, {&quot;low-status-pop&quot;: 6.57, &quot;house-prices&quot;: 21.9}, {&quot;low-status-pop&quot;: 9.25, &quot;house-prices&quot;: 20.9}, {&quot;low-status-pop&quot;: 3.11, &quot;house-prices&quot;: 44.0}, {&quot;low-status-pop&quot;: 5.12, &quot;house-prices&quot;: 50.0}, {&quot;low-status-pop&quot;: 7.79, &quot;house-prices&quot;: 36.0}, {&quot;low-status-pop&quot;: 6.9, &quot;house-prices&quot;: 30.1}, {&quot;low-status-pop&quot;: 9.59, &quot;house-prices&quot;: 33.8}, {&quot;low-status-pop&quot;: 7.26, &quot;house-prices&quot;: 43.1}, {&quot;low-status-pop&quot;: 5.91, &quot;house-prices&quot;: 48.8}, {&quot;low-status-pop&quot;: 11.25, &quot;house-prices&quot;: 31.0}, {&quot;low-status-pop&quot;: 8.1, &quot;house-prices&quot;: 36.5}, {&quot;low-status-pop&quot;: 10.45, &quot;house-prices&quot;: 22.8}, {&quot;low-status-pop&quot;: 14.79, &quot;house-prices&quot;: 30.7}, {&quot;low-status-pop&quot;: 7.44, &quot;house-prices&quot;: 50.0}, {&quot;low-status-pop&quot;: 3.16, &quot;house-prices&quot;: 43.5}, {&quot;low-status-pop&quot;: 13.65, &quot;house-prices&quot;: 20.7}, {&quot;low-status-pop&quot;: 13.0, &quot;house-prices&quot;: 21.1}, {&quot;low-status-pop&quot;: 6.59, &quot;house-prices&quot;: 25.2}, {&quot;low-status-pop&quot;: 7.73, &quot;house-prices&quot;: 24.4}, {&quot;low-status-pop&quot;: 6.58, &quot;house-prices&quot;: 35.2}, {&quot;low-status-pop&quot;: 3.53, &quot;house-prices&quot;: 32.4}, {&quot;low-status-pop&quot;: 2.98, &quot;house-prices&quot;: 32.0}, {&quot;low-status-pop&quot;: 6.05, &quot;house-prices&quot;: 33.2}, {&quot;low-status-pop&quot;: 4.16, &quot;house-prices&quot;: 33.1}, {&quot;low-status-pop&quot;: 7.19, &quot;house-prices&quot;: 29.1}, {&quot;low-status-pop&quot;: 4.85, &quot;house-prices&quot;: 35.1}, {&quot;low-status-pop&quot;: 3.76, &quot;house-prices&quot;: 45.4}, {&quot;low-status-pop&quot;: 4.59, &quot;house-prices&quot;: 35.4}, {&quot;low-status-pop&quot;: 3.01, &quot;house-prices&quot;: 46.0}, {&quot;low-status-pop&quot;: 3.16, &quot;house-prices&quot;: 50.0}, {&quot;low-status-pop&quot;: 7.85, &quot;house-prices&quot;: 32.2}, {&quot;low-status-pop&quot;: 8.23, &quot;house-prices&quot;: 22.0}, {&quot;low-status-pop&quot;: 12.93, &quot;house-prices&quot;: 20.1}, {&quot;low-status-pop&quot;: 7.14, &quot;house-prices&quot;: 23.2}, {&quot;low-status-pop&quot;: 7.6, &quot;house-prices&quot;: 22.3}, {&quot;low-status-pop&quot;: 9.51, &quot;house-prices&quot;: 24.8}, {&quot;low-status-pop&quot;: 3.33, &quot;house-prices&quot;: 28.5}, {&quot;low-status-pop&quot;: 3.56, &quot;house-prices&quot;: 37.3}, {&quot;low-status-pop&quot;: 4.7, &quot;house-prices&quot;: 27.9}, {&quot;low-status-pop&quot;: 8.58, &quot;house-prices&quot;: 23.9}, {&quot;low-status-pop&quot;: 10.4, &quot;house-prices&quot;: 21.7}, {&quot;low-status-pop&quot;: 6.27, &quot;house-prices&quot;: 28.6}, {&quot;low-status-pop&quot;: 7.39, &quot;house-prices&quot;: 27.1}, {&quot;low-status-pop&quot;: 15.84, &quot;house-prices&quot;: 20.3}, {&quot;low-status-pop&quot;: 4.97, &quot;house-prices&quot;: 22.5}, {&quot;low-status-pop&quot;: 4.74, &quot;house-prices&quot;: 29.0}, {&quot;low-status-pop&quot;: 6.07, &quot;house-prices&quot;: 24.8}, {&quot;low-status-pop&quot;: 9.5, &quot;house-prices&quot;: 22.0}, {&quot;low-status-pop&quot;: 8.67, &quot;house-prices&quot;: 26.4}, {&quot;low-status-pop&quot;: 4.86, &quot;house-prices&quot;: 33.1}, {&quot;low-status-pop&quot;: 6.93, &quot;house-prices&quot;: 36.1}, {&quot;low-status-pop&quot;: 8.93, &quot;house-prices&quot;: 28.4}, {&quot;low-status-pop&quot;: 6.47, &quot;house-prices&quot;: 33.4}, {&quot;low-status-pop&quot;: 7.53, &quot;house-prices&quot;: 28.2}, {&quot;low-status-pop&quot;: 4.54, &quot;house-prices&quot;: 22.8}, {&quot;low-status-pop&quot;: 9.97, &quot;house-prices&quot;: 20.3}, {&quot;low-status-pop&quot;: 12.64, &quot;house-prices&quot;: 16.1}, {&quot;low-status-pop&quot;: 5.98, &quot;house-prices&quot;: 22.1}, {&quot;low-status-pop&quot;: 11.72, &quot;house-prices&quot;: 19.4}, {&quot;low-status-pop&quot;: 7.9, &quot;house-prices&quot;: 21.6}, {&quot;low-status-pop&quot;: 9.28, &quot;house-prices&quot;: 23.8}, {&quot;low-status-pop&quot;: 11.5, &quot;house-prices&quot;: 16.2}, {&quot;low-status-pop&quot;: 18.33, &quot;house-prices&quot;: 17.8}, {&quot;low-status-pop&quot;: 15.94, &quot;house-prices&quot;: 19.8}, {&quot;low-status-pop&quot;: 10.36, &quot;house-prices&quot;: 23.1}, {&quot;low-status-pop&quot;: 12.73, &quot;house-prices&quot;: 21.0}, {&quot;low-status-pop&quot;: 7.2, &quot;house-prices&quot;: 23.8}, {&quot;low-status-pop&quot;: 6.87, &quot;house-prices&quot;: 23.1}, {&quot;low-status-pop&quot;: 7.7, &quot;house-prices&quot;: 20.4}, {&quot;low-status-pop&quot;: 11.74, &quot;house-prices&quot;: 18.5}, {&quot;low-status-pop&quot;: 6.12, &quot;house-prices&quot;: 25.0}, {&quot;low-status-pop&quot;: 5.08, &quot;house-prices&quot;: 24.6}, {&quot;low-status-pop&quot;: 6.15, &quot;house-prices&quot;: 23.0}, {&quot;low-status-pop&quot;: 12.79, &quot;house-prices&quot;: 22.2}, {&quot;low-status-pop&quot;: 9.97, &quot;house-prices&quot;: 19.3}, {&quot;low-status-pop&quot;: 7.34, &quot;house-prices&quot;: 22.6}, {&quot;low-status-pop&quot;: 9.09, &quot;house-prices&quot;: 19.8}, {&quot;low-status-pop&quot;: 12.43, &quot;house-prices&quot;: 17.1}, {&quot;low-status-pop&quot;: 7.83, &quot;house-prices&quot;: 19.4}, {&quot;low-status-pop&quot;: 5.68, &quot;house-prices&quot;: 22.2}, {&quot;low-status-pop&quot;: 6.75, &quot;house-prices&quot;: 20.7}, {&quot;low-status-pop&quot;: 8.01, &quot;house-prices&quot;: 21.1}, {&quot;low-status-pop&quot;: 9.8, &quot;house-prices&quot;: 19.5}, {&quot;low-status-pop&quot;: 10.56, &quot;house-prices&quot;: 18.5}, {&quot;low-status-pop&quot;: 8.51, &quot;house-prices&quot;: 20.6}, {&quot;low-status-pop&quot;: 9.74, &quot;house-prices&quot;: 19.0}, {&quot;low-status-pop&quot;: 9.29, &quot;house-prices&quot;: 18.7}, {&quot;low-status-pop&quot;: 5.49, &quot;house-prices&quot;: 32.7}, {&quot;low-status-pop&quot;: 8.65, &quot;house-prices&quot;: 16.5}, {&quot;low-status-pop&quot;: 7.18, &quot;house-prices&quot;: 23.9}, {&quot;low-status-pop&quot;: 4.61, &quot;house-prices&quot;: 31.2}, {&quot;low-status-pop&quot;: 10.53, &quot;house-prices&quot;: 17.5}, {&quot;low-status-pop&quot;: 12.67, &quot;house-prices&quot;: 17.2}, {&quot;low-status-pop&quot;: 6.36, &quot;house-prices&quot;: 23.1}, {&quot;low-status-pop&quot;: 5.99, &quot;house-prices&quot;: 24.5}, {&quot;low-status-pop&quot;: 5.89, &quot;house-prices&quot;: 26.6}, {&quot;low-status-pop&quot;: 5.98, &quot;house-prices&quot;: 22.9}, {&quot;low-status-pop&quot;: 5.49, &quot;house-prices&quot;: 24.1}, {&quot;low-status-pop&quot;: 7.79, &quot;house-prices&quot;: 18.6}, {&quot;low-status-pop&quot;: 4.5, &quot;house-prices&quot;: 30.1}, {&quot;low-status-pop&quot;: 8.05, &quot;house-prices&quot;: 18.2}, {&quot;low-status-pop&quot;: 5.57, &quot;house-prices&quot;: 20.6}, {&quot;low-status-pop&quot;: 17.6, &quot;house-prices&quot;: 17.8}, {&quot;low-status-pop&quot;: 13.27, &quot;house-prices&quot;: 21.7}, {&quot;low-status-pop&quot;: 11.48, &quot;house-prices&quot;: 22.7}, {&quot;low-status-pop&quot;: 12.67, &quot;house-prices&quot;: 22.6}, {&quot;low-status-pop&quot;: 7.79, &quot;house-prices&quot;: 25.0}, {&quot;low-status-pop&quot;: 14.19, &quot;house-prices&quot;: 19.9}, {&quot;low-status-pop&quot;: 10.19, &quot;house-prices&quot;: 20.8}, {&quot;low-status-pop&quot;: 14.64, &quot;house-prices&quot;: 16.8}, {&quot;low-status-pop&quot;: 5.29, &quot;house-prices&quot;: 21.9}, {&quot;low-status-pop&quot;: 7.12, &quot;house-prices&quot;: 27.5}, {&quot;low-status-pop&quot;: 14.0, &quot;house-prices&quot;: 21.9}, {&quot;low-status-pop&quot;: 13.33, &quot;house-prices&quot;: 23.1}, {&quot;low-status-pop&quot;: 3.26, &quot;house-prices&quot;: 50.0}, {&quot;low-status-pop&quot;: 3.73, &quot;house-prices&quot;: 50.0}, {&quot;low-status-pop&quot;: 2.96, &quot;house-prices&quot;: 50.0}, {&quot;low-status-pop&quot;: 9.53, &quot;house-prices&quot;: 50.0}, {&quot;low-status-pop&quot;: 8.88, &quot;house-prices&quot;: 50.0}, {&quot;low-status-pop&quot;: 34.77, &quot;house-prices&quot;: 13.8}, {&quot;low-status-pop&quot;: 37.97, &quot;house-prices&quot;: 13.8}, {&quot;low-status-pop&quot;: 13.44, &quot;house-prices&quot;: 15.0}, {&quot;low-status-pop&quot;: 23.24, &quot;house-prices&quot;: 13.9}, {&quot;low-status-pop&quot;: 21.24, &quot;house-prices&quot;: 13.3}, {&quot;low-status-pop&quot;: 23.69, &quot;house-prices&quot;: 13.1}, {&quot;low-status-pop&quot;: 21.78, &quot;house-prices&quot;: 10.2}, {&quot;low-status-pop&quot;: 17.21, &quot;house-prices&quot;: 10.4}, {&quot;low-status-pop&quot;: 21.08, &quot;house-prices&quot;: 10.9}, {&quot;low-status-pop&quot;: 23.6, &quot;house-prices&quot;: 11.3}, {&quot;low-status-pop&quot;: 24.56, &quot;house-prices&quot;: 12.3}, {&quot;low-status-pop&quot;: 30.63, &quot;house-prices&quot;: 8.8}, {&quot;low-status-pop&quot;: 30.81, &quot;house-prices&quot;: 7.2}, {&quot;low-status-pop&quot;: 28.28, &quot;house-prices&quot;: 10.5}, {&quot;low-status-pop&quot;: 31.99, &quot;house-prices&quot;: 7.4}, {&quot;low-status-pop&quot;: 30.62, &quot;house-prices&quot;: 10.2}, {&quot;low-status-pop&quot;: 20.85, &quot;house-prices&quot;: 11.5}, {&quot;low-status-pop&quot;: 17.11, &quot;house-prices&quot;: 15.1}, {&quot;low-status-pop&quot;: 18.76, &quot;house-prices&quot;: 23.2}, {&quot;low-status-pop&quot;: 25.68, &quot;house-prices&quot;: 9.7}, {&quot;low-status-pop&quot;: 15.17, &quot;house-prices&quot;: 13.8}, {&quot;low-status-pop&quot;: 16.35, &quot;house-prices&quot;: 12.7}, {&quot;low-status-pop&quot;: 17.12, &quot;house-prices&quot;: 13.1}, {&quot;low-status-pop&quot;: 19.37, &quot;house-prices&quot;: 12.5}, {&quot;low-status-pop&quot;: 19.92, &quot;house-prices&quot;: 8.5}, {&quot;low-status-pop&quot;: 30.59, &quot;house-prices&quot;: 5.0}, {&quot;low-status-pop&quot;: 29.97, &quot;house-prices&quot;: 6.3}, {&quot;low-status-pop&quot;: 26.77, &quot;house-prices&quot;: 5.6}, {&quot;low-status-pop&quot;: 20.32, &quot;house-prices&quot;: 7.2}, {&quot;low-status-pop&quot;: 20.31, &quot;house-prices&quot;: 12.1}, {&quot;low-status-pop&quot;: 19.77, &quot;house-prices&quot;: 8.3}, {&quot;low-status-pop&quot;: 27.38, &quot;house-prices&quot;: 8.5}, {&quot;low-status-pop&quot;: 22.98, &quot;house-prices&quot;: 5.0}, {&quot;low-status-pop&quot;: 23.34, &quot;house-prices&quot;: 11.9}, {&quot;low-status-pop&quot;: 12.13, &quot;house-prices&quot;: 27.9}, {&quot;low-status-pop&quot;: 26.4, &quot;house-prices&quot;: 17.2}, {&quot;low-status-pop&quot;: 19.78, &quot;house-prices&quot;: 27.5}, {&quot;low-status-pop&quot;: 10.11, &quot;house-prices&quot;: 15.0}, {&quot;low-status-pop&quot;: 21.22, &quot;house-prices&quot;: 17.2}, {&quot;low-status-pop&quot;: 34.37, &quot;house-prices&quot;: 17.9}, {&quot;low-status-pop&quot;: 20.08, &quot;house-prices&quot;: 16.3}, {&quot;low-status-pop&quot;: 36.98, &quot;house-prices&quot;: 7.0}, {&quot;low-status-pop&quot;: 29.05, &quot;house-prices&quot;: 7.2}, {&quot;low-status-pop&quot;: 25.79, &quot;house-prices&quot;: 7.5}, {&quot;low-status-pop&quot;: 26.64, &quot;house-prices&quot;: 10.4}, {&quot;low-status-pop&quot;: 20.62, &quot;house-prices&quot;: 8.8}, {&quot;low-status-pop&quot;: 22.74, &quot;house-prices&quot;: 8.4}, {&quot;low-status-pop&quot;: 15.02, &quot;house-prices&quot;: 16.7}, {&quot;low-status-pop&quot;: 15.7, &quot;house-prices&quot;: 14.2}, {&quot;low-status-pop&quot;: 14.1, &quot;house-prices&quot;: 20.8}, {&quot;low-status-pop&quot;: 23.29, &quot;house-prices&quot;: 13.4}, {&quot;low-status-pop&quot;: 17.16, &quot;house-prices&quot;: 11.7}, {&quot;low-status-pop&quot;: 24.39, &quot;house-prices&quot;: 8.3}, {&quot;low-status-pop&quot;: 15.69, &quot;house-prices&quot;: 10.2}, {&quot;low-status-pop&quot;: 14.52, &quot;house-prices&quot;: 10.9}, {&quot;low-status-pop&quot;: 21.52, &quot;house-prices&quot;: 11.0}, {&quot;low-status-pop&quot;: 24.08, &quot;house-prices&quot;: 9.5}, {&quot;low-status-pop&quot;: 17.64, &quot;house-prices&quot;: 14.5}, {&quot;low-status-pop&quot;: 19.69, &quot;house-prices&quot;: 14.1}, {&quot;low-status-pop&quot;: 12.03, &quot;house-prices&quot;: 16.1}, {&quot;low-status-pop&quot;: 16.22, &quot;house-prices&quot;: 14.3}, {&quot;low-status-pop&quot;: 15.17, &quot;house-prices&quot;: 11.7}, {&quot;low-status-pop&quot;: 23.27, &quot;house-prices&quot;: 13.4}, {&quot;low-status-pop&quot;: 18.05, &quot;house-prices&quot;: 9.6}, {&quot;low-status-pop&quot;: 26.45, &quot;house-prices&quot;: 8.7}, {&quot;low-status-pop&quot;: 34.02, &quot;house-prices&quot;: 8.4}, {&quot;low-status-pop&quot;: 22.88, &quot;house-prices&quot;: 12.8}, {&quot;low-status-pop&quot;: 22.11, &quot;house-prices&quot;: 10.5}, {&quot;low-status-pop&quot;: 19.52, &quot;house-prices&quot;: 17.1}, {&quot;low-status-pop&quot;: 16.59, &quot;house-prices&quot;: 18.4}, {&quot;low-status-pop&quot;: 18.85, &quot;house-prices&quot;: 15.4}, {&quot;low-status-pop&quot;: 23.79, &quot;house-prices&quot;: 10.8}, {&quot;low-status-pop&quot;: 23.98, &quot;house-prices&quot;: 11.8}, {&quot;low-status-pop&quot;: 17.79, &quot;house-prices&quot;: 14.9}, {&quot;low-status-pop&quot;: 16.44, &quot;house-prices&quot;: 12.6}, {&quot;low-status-pop&quot;: 18.13, &quot;house-prices&quot;: 14.1}, {&quot;low-status-pop&quot;: 19.31, &quot;house-prices&quot;: 13.0}, {&quot;low-status-pop&quot;: 17.44, &quot;house-prices&quot;: 13.4}, {&quot;low-status-pop&quot;: 17.73, &quot;house-prices&quot;: 15.2}, {&quot;low-status-pop&quot;: 17.27, &quot;house-prices&quot;: 16.1}, {&quot;low-status-pop&quot;: 16.74, &quot;house-prices&quot;: 17.8}, {&quot;low-status-pop&quot;: 18.71, &quot;house-prices&quot;: 14.9}, {&quot;low-status-pop&quot;: 18.13, &quot;house-prices&quot;: 14.1}, {&quot;low-status-pop&quot;: 19.01, &quot;house-prices&quot;: 12.7}, {&quot;low-status-pop&quot;: 16.94, &quot;house-prices&quot;: 13.5}, {&quot;low-status-pop&quot;: 16.23, &quot;house-prices&quot;: 14.9}, {&quot;low-status-pop&quot;: 14.7, &quot;house-prices&quot;: 20.0}, {&quot;low-status-pop&quot;: 16.42, &quot;house-prices&quot;: 16.4}, {&quot;low-status-pop&quot;: 14.65, &quot;house-prices&quot;: 17.7}, {&quot;low-status-pop&quot;: 13.99, &quot;house-prices&quot;: 19.5}, {&quot;low-status-pop&quot;: 10.29, &quot;house-prices&quot;: 20.2}, {&quot;low-status-pop&quot;: 13.22, &quot;house-prices&quot;: 21.4}, {&quot;low-status-pop&quot;: 14.13, &quot;house-prices&quot;: 19.9}, {&quot;low-status-pop&quot;: 17.15, &quot;house-prices&quot;: 19.0}, {&quot;low-status-pop&quot;: 21.32, &quot;house-prices&quot;: 19.1}, {&quot;low-status-pop&quot;: 18.13, &quot;house-prices&quot;: 19.1}, {&quot;low-status-pop&quot;: 14.76, &quot;house-prices&quot;: 20.1}, {&quot;low-status-pop&quot;: 16.29, &quot;house-prices&quot;: 19.9}, {&quot;low-status-pop&quot;: 12.87, &quot;house-prices&quot;: 19.6}, {&quot;low-status-pop&quot;: 14.36, &quot;house-prices&quot;: 23.2}, {&quot;low-status-pop&quot;: 11.66, &quot;house-prices&quot;: 29.8}, {&quot;low-status-pop&quot;: 18.14, &quot;house-prices&quot;: 13.8}, {&quot;low-status-pop&quot;: 24.1, &quot;house-prices&quot;: 13.3}, {&quot;low-status-pop&quot;: 18.68, &quot;house-prices&quot;: 16.7}, {&quot;low-status-pop&quot;: 24.91, &quot;house-prices&quot;: 12.0}, {&quot;low-status-pop&quot;: 18.03, &quot;house-prices&quot;: 14.6}, {&quot;low-status-pop&quot;: 13.11, &quot;house-prices&quot;: 21.4}, {&quot;low-status-pop&quot;: 10.74, &quot;house-prices&quot;: 23.0}, {&quot;low-status-pop&quot;: 7.74, &quot;house-prices&quot;: 23.7}, {&quot;low-status-pop&quot;: 7.01, &quot;house-prices&quot;: 25.0}, {&quot;low-status-pop&quot;: 10.42, &quot;house-prices&quot;: 21.8}, {&quot;low-status-pop&quot;: 13.34, &quot;house-prices&quot;: 20.6}, {&quot;low-status-pop&quot;: 10.58, &quot;house-prices&quot;: 21.2}, {&quot;low-status-pop&quot;: 14.98, &quot;house-prices&quot;: 19.1}, {&quot;low-status-pop&quot;: 11.45, &quot;house-prices&quot;: 20.6}, {&quot;low-status-pop&quot;: 18.06, &quot;house-prices&quot;: 15.2}, {&quot;low-status-pop&quot;: 23.97, &quot;house-prices&quot;: 7.0}, {&quot;low-status-pop&quot;: 29.68, &quot;house-prices&quot;: 8.1}, {&quot;low-status-pop&quot;: 18.07, &quot;house-prices&quot;: 13.6}, {&quot;low-status-pop&quot;: 13.35, &quot;house-prices&quot;: 20.1}, {&quot;low-status-pop&quot;: 12.01, &quot;house-prices&quot;: 21.8}, {&quot;low-status-pop&quot;: 13.59, &quot;house-prices&quot;: 24.5}, {&quot;low-status-pop&quot;: 17.6, &quot;house-prices&quot;: 23.1}, {&quot;low-status-pop&quot;: 21.14, &quot;house-prices&quot;: 19.7}, {&quot;low-status-pop&quot;: 14.1, &quot;house-prices&quot;: 18.3}, {&quot;low-status-pop&quot;: 12.92, &quot;house-prices&quot;: 21.2}, {&quot;low-status-pop&quot;: 15.1, &quot;house-prices&quot;: 17.5}, {&quot;low-status-pop&quot;: 14.33, &quot;house-prices&quot;: 16.8}, {&quot;low-status-pop&quot;: 9.67, &quot;house-prices&quot;: 22.4}, {&quot;low-status-pop&quot;: 9.08, &quot;house-prices&quot;: 20.6}, {&quot;low-status-pop&quot;: 5.64, &quot;house-prices&quot;: 23.9}, {&quot;low-status-pop&quot;: 6.48, &quot;house-prices&quot;: 22.0}, {&quot;low-status-pop&quot;: 7.88, &quot;house-prices&quot;: 11.9}]}}, {&quot;mode&quot;: &quot;vega-lite&quot;});
&lt;/script&gt;

&lt;h2 id=&quot;multivariable-linear-regression-example&quot;&gt;Multivariable linear regression example&lt;/h2&gt;

&lt;p&gt;Now let’s fit a model with all 13 features as predictors. For this we just need to remove &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[:, 12].reshape(-1, 1)&lt;/code&gt; from the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fit&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;predict&lt;/code&gt; methods.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;multi_reg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linear_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;LinearRegression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;multi_reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;y_pred_multi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;multi_reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Again, let’s evaluate the overall performance by computing the SSE, MSE, and the $R^2$ coefficient of determination.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;SSE_multi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_pred_multi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;MSE_multi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;mean_squared_error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_pred_multi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;R2_multi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;r2_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_pred_multi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Sum of Squared Errors multivariable regression: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SSE_multi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Mean of Squared Errors multivariable regression: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MSE_multi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;R2 coefficient of determination multivariable regression: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;R2_multi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Sum of Squared Errors multivariable regression: 3411.8
Mean of Squared Errors multivariable regression: 33.45
R2 coefficient of determination multivariable regression: 0.59
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Based on the 13 features, we obtain a $SSE\approx3411.8$, a $MSE\approx33.45$, and a $R^2\approx0.59$. As expected, the error measures went down and the association measure went up, as more features provide more information for prediction.&lt;/p&gt;

&lt;p&gt;Visualization is not possible for a 13 features regression, but you can make your best effort by imaging a 3D space and thinking “13!” with all your might.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Bishop, C. M. (2006). 3. Linear models for regression. Pattern recognition and machine learning. springer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Deisenroth, M. P., Faisal, A. A., &amp;amp; Ong, C. S. (2020) 9. Linear Regression. In Mathematics for machine learning. Cambridge University Press.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Friedman, J., Hastie, T., &amp;amp; Tibshirani, R. (2009). 3. Linear Methods for Regression. In The elements of statistical learning (Vol. 1, No. 10). New York: Springer series in statistics.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sat, 25 Apr 2020 00:00:00 +0800</pubDate>
        <link>//intro-linear-regression</link>
        <link href="/intro-linear-regression"/>
        <guid isPermaLink="true">/intro-linear-regression</guid>
      </item>
    
      <item>
        <title>The Recurrent Neural Network - Theory and Implementation of the Elman Network and LSTM</title>
        <description>&lt;iframe src=&quot;https://github.com/sponsors/pabloinsente/card&quot; title=&quot;Sponsor pabloinsente&quot; height=&quot;225&quot; width=&quot;600&quot; style=&quot;border: 0;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;learning-objectives&quot;&gt;Learning objectives&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Understand the principles behind the creation of the recurrent neural network&lt;/li&gt;
  &lt;li&gt;Obtain intuition about difficulties training RNNs, namely: vanishing/exploding gradients and long-term dependencies&lt;/li&gt;
  &lt;li&gt;Obtain intuition about mechanics of backpropagation through time BPTT&lt;/li&gt;
  &lt;li&gt;Develop a Long Short-Term memory implementation in Keras&lt;/li&gt;
  &lt;li&gt;Learn about the uses and limitations of RNNs from a cognitive science perspective&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;historical-and-theoretical-background&quot;&gt;Historical and theoretical background&lt;/h2&gt;

&lt;p&gt;The poet Delmore Schwartz once wrote: &lt;strong&gt;“…time is the fire in which we burn”&lt;/strong&gt;. We can’t escape time. Time is embedded in every human thought and action. Yet, so far, we have been oblivious to the role of time in neural network modeling. Indeed, in all models we have examined so far we have implicitly assumed that &lt;strong&gt;data is “perceived” all at once&lt;/strong&gt;, although there are countless examples where time is a critical consideration: movement, speech production, planning, decision-making, etc. We also have implicitly assumed that &lt;strong&gt;past-states have no influence in future-states&lt;/strong&gt;. This is, the input pattern at time-step $t-1$ does not influence the output of time-step $t-0$, or $t+1$, or any subsequent outcome for that matter. In probabilistic jargon, this equals to assume that each sample is drawn independently from each other. We know in many scenarios this is simply not true: when giving a talk, my next utterance will depend upon my past utterances; when running, my last stride will condition my next stride, and so on. You can imagine endless examples.&lt;/p&gt;

&lt;p&gt;Multilayer Perceptrons and Convolutional Networks, in principle, can be used to approach problems where time and sequences are a consideration (for instance &lt;a href=&quot;https://arxiv.org/pdf/1603.06995.pdf&quot;&gt;Cui et al, 2016&lt;/a&gt;). Nevertheless, introducing time considerations in such architectures is cumbersome, and better architectures have been envisioned. In particular, &lt;strong&gt;Recurrent Neural Networks (RNNs)&lt;/strong&gt; are the modern standard to deal with &lt;strong&gt;time-dependent&lt;/strong&gt; and/or &lt;strong&gt;sequence-dependent&lt;/strong&gt; problems. This type of network is “recurrent” in the sense that they can &lt;strong&gt;revisit or reuse past states as inputs to predict the next or future states&lt;/strong&gt;. To put it plainly, they have &lt;strong&gt;memory&lt;/strong&gt;. Indeed, memory is what allows us to incorporate our past thoughts and behaviors into our future thoughts and behaviors.&lt;/p&gt;

&lt;h3 id=&quot;hopfield-network&quot;&gt;Hopfield Network&lt;/h3&gt;

&lt;p&gt;One of the earliest examples of networks incorporating “recurrences” was the so-called &lt;strong&gt;Hopfield Network&lt;/strong&gt;, introduced in 1982 by &lt;a href=&quot;https://en.wikipedia.org/wiki/John_Hopfield&quot;&gt;John Hopfield&lt;/a&gt;, at the time, a physicist at Caltech. Hopfield networks were important as they helped to reignite the interest in neural networks in the early ’80s. In his 1982 paper, Hopfield wanted to address the fundamental question of &lt;strong&gt;emergence&lt;/strong&gt; in cognitive systems: Can relatively stable cognitive phenomena, like memories, emerge from the collective action of large numbers of simple neurons? After all, such behavior was observed in other physical systems like vortex patterns in fluid flow. Brains seemed like another promising candidate.&lt;/p&gt;

&lt;p&gt;Hopfield networks are known as a type of &lt;strong&gt;energy-based&lt;/strong&gt; (instead of error-based) network because their properties derive from a global energy-function (Raj, 2020). In resemblance to the McCulloch-Pitts neuron, Hopfield neurons are binary threshold units but with recurrent instead of feed-forward connections, where each unit is &lt;strong&gt;bi-directionally connected&lt;/strong&gt; to each other, as shown in &lt;strong&gt;Figure 1&lt;/strong&gt;. This means that each unit &lt;em&gt;receives&lt;/em&gt; inputs and &lt;em&gt;sends&lt;/em&gt; inputs to every other connected unit. A consequence of this architecture is that &lt;strong&gt;weights values are symmetric&lt;/strong&gt;, such that weights &lt;em&gt;coming into&lt;/em&gt; a unit are the same as the ones &lt;em&gt;coming out&lt;/em&gt; of a unit. The value of each unit is determined by a linear function wrapped into a threshold function $T$, as $y_i = T(\sum w_{ji}y_j + b_i)$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 1: Hopfield Network&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-9/hopfield-net.png&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Hopfield network’s idea is that each configuration of binary-values $C$ in the network is associated with a &lt;strong&gt;global energy value $-E$&lt;/strong&gt;. Here is a simplified picture of the training process: imagine you have a network with five neurons with a configuration of $C_1=(0, 1, 0, 1, 0)$. Now, imagine $C_1$ yields a global energy-value $E_1= 2$ (following the energy function formula). Your goal is to &lt;em&gt;minimize&lt;/em&gt; $E$ by changing one element of the network $c_i$ at a time. By using the weight updating rule $\Delta w$, you can subsequently get a new configuration like $C_2=(1, 1, 0, 1, 0)$, as new weights will cause a change in the activation values $(0,1)$. If $C_2$ yields a &lt;em&gt;lower value of $E$&lt;/em&gt;, let’s say, $1.5$, you are moving in the right direction. If you keep iterating with new configurations the network will eventually “settle” into a &lt;strong&gt;global energy minimum&lt;/strong&gt; (conditioned to the initial state of the network).&lt;/p&gt;

&lt;p&gt;A fascinating aspect of Hopfield networks, besides the introduction of recurrence, is that is closely based in neuroscience research about learning and memory, particularly Hebbian learning (Hebb, 1949). In fact, Hopfield (1982) proposed this model as a way to capture &lt;strong&gt;memory formation and retrieval&lt;/strong&gt;. The idea is that the energy-minima of the network could represent the &lt;strong&gt;formation of a memory&lt;/strong&gt;, which further gives rise to a property known as &lt;strong&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Content-addressable_memory&quot;&gt;content-addressable memory (CAM)&lt;/a&gt;&lt;/strong&gt;. Here is the idea with a computer analogy: when you access information stored in the random access memory of your computer (RAM), you give the “address” where the “memory” is located to retrieve it. CAM works the other way around: you give information about the &lt;strong&gt;content&lt;/strong&gt; you are searching for, and the computer should retrieve the “memory”. This is great because this works even when you have &lt;strong&gt;partial or corrupted&lt;/strong&gt; information about the content, which is a much more &lt;strong&gt;realistic depiction of how human memory works&lt;/strong&gt;. It is similar to doing a google search. Just think in how many times you have searched for lyrics with partial information, like “song with the beeeee bop ba bodda bope!”.&lt;/p&gt;

&lt;p&gt;It is important to highlight that the sequential adjustment of Hopfield networks is &lt;strong&gt;not driven by error correction&lt;/strong&gt;: there isn’t a “target” as in supervised-based neural networks. Hopfield networks are systems that “evolve” until they find a stable low-energy state. If you “perturb” such a system, the system will “re-evolve” towards its previous stable-state, similar to how those inflatable “Bop Bags” toys get back to their initial position no matter how hard you punch them. It is almost like the system “remembers” its previous stable-state (isn’t?). This ability to “return” to a previous stable-state after the perturbation is why they serve as models of memory.&lt;/p&gt;

&lt;h3 id=&quot;elman-network&quot;&gt;Elman Network&lt;/h3&gt;

&lt;p&gt;Although Hopfield networks where innovative and fascinating models, the first successful example of a recurrent network trained with backpropagation was introduced by &lt;a href=&quot;https://en.wikipedia.org/wiki/Jeffrey_Elman&quot;&gt;Jeffrey Elman&lt;/a&gt;, the so-called &lt;strong&gt;Elman Network&lt;/strong&gt; (Elman, 1990). Elman was a cognitive scientist at UC San Diego at the time, part of the group of researchers that published the famous PDP book.&lt;/p&gt;

&lt;p&gt;In 1990, Elman published “Finding Structure in Time”, a highly influential work for in cognitive science. Elman was concerned with the problem of representing “time” or “sequences” in neural networks. In his view, you could take either an “explicit” approach or an “implicit” approach. The &lt;strong&gt;explicit&lt;/strong&gt; approach represents time &lt;strong&gt;spacially&lt;/strong&gt;. Consider a vector $x = [x_1,x_2 \cdots, x_n]$, where element $x_1$ represents the first value of a sequence, $x_2$ the second element, and $x_n$ the last element. Hence, the spacial location in $\bf{x}$ is indicating the temporal location of each element. You can think about elements of $\bf{x}$ as sequences of words or actions, one after the other, for instance: $x^1=[Sound, of, the, funky, drummer]$ is a sequence of length five. Elman saw &lt;strong&gt;several drawbacks&lt;/strong&gt; to this approach. First, although $\bf{x}$ is a sequence, the network still needs to represent the sequence all at once as an input, this is, a network would need five input neurons to process $x^1$. Second, it imposes a rigid limit on the duration of pattern, in other words, the network needs a fixed number of elements for every input vector $\bf{x}$: a network with five input units, can’t accommodate a sequence of length six. True, you could start with a six input network, but then shorter sequences would be misrepresented since mismatched units would receive zero input. This is a problem for most domains where sequences have a variable duration. Finally, it can’t easily distinguish &lt;strong&gt;relative&lt;/strong&gt; temporal position from &lt;strong&gt;absolute&lt;/strong&gt; temporal position. Consider the sequence $s = [1, 1]$ and a vector input length of four bits. Such a sequence can be presented in at least three variations:&lt;/p&gt;

\[x_1 = [0, 1, 1, 0]\\
x_2 = [0, 0, 1, 1]\\
x_3 = [1, 1, 0, 0]\]

&lt;p&gt;Here, $\bf{x_1}$, $\bf{x_2}$, and $\bf{x_3}$ are instances of $\bf{s}$ but spacially displaced in the input vector. Geometrically, those three vectors are very different from each other (you can compute similarity measures to put a number on that), although representing the same instance. Even though you can train a neural net to learn those three patterns are associated with the same target, their inherent dissimilarity probably will hinder the network’s ability to generalize the learned association.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;implicit&lt;/strong&gt; approach represents time by &lt;strong&gt;its effect in intermediate computations&lt;/strong&gt;. To do this, Elman added a &lt;strong&gt;context unit&lt;/strong&gt; to save past computations and incorporate those in future computations. In short, &lt;strong&gt;memory&lt;/strong&gt;. Elman based his approach in the work of &lt;a href=&quot;https://people.eecs.berkeley.edu/~jordan/&quot;&gt;Michael I. Jordan&lt;/a&gt; on serial processing (1986). Jordan’s network implements recurrent connections from the network output $\hat{y}$ to its hidden units $h$, via a “memory unit” $\mu$ (equivalent to Elman’s “context unit”) as depicted in &lt;strong&gt;Figure 2&lt;/strong&gt;. In short, the memory unit keeps a running average of &lt;strong&gt;all past outputs&lt;/strong&gt;: this is how the past history is implicitly accounted for on each new computation. There is no learning in the memory unit, which means the weights are fixed to $1$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 2: Jordan Network&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-9/jordan-net.png&quot; width=&quot;70%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Jordan’s network diagrams exemplifies the two ways in which recurrent nets are usually represented. On the left, the &lt;strong&gt;compact format&lt;/strong&gt; depicts the network structure as a circuit. On the right, the &lt;strong&gt;unfolded representation&lt;/strong&gt; incorporates the notion of time-steps calculations. The unfolded representation also illustrates how a recurrent network can be constructed in a pure feed-forward fashion, with as many layers as time-steps in your sequence. One key consideration is that the weights will be identical on each time-step (or layer). Keep this unfolded representation in mind as will become important later.&lt;/p&gt;

&lt;p&gt;Elman’s innovation was twofold: &lt;strong&gt;recurrent connections between hidden units and memory&lt;/strong&gt; (context) units, and &lt;strong&gt;trainable parameters from the memory units to the hidden units&lt;/strong&gt;. Memory units now have to “remember” the past state of hidden units, which means that instead of keeping a running average, they “clone” the value at the previous time-step $t-1$. Memory units also have to learn useful representations (weights) for encoding temporal properties of the sequential input.  &lt;strong&gt;Figure 3&lt;/strong&gt; summarizes Elman’s network in compact and unfolded fashion.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 3: Elman Network&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-9/elman-net.png&quot; width=&quot;70%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: there is something curious about Elman’s architecture. What it is the point of “cloning” $h$ into $c$ at each time-step? You could bypass $c$ altogether by sending the value of $h_t$ straight into $h_{t+1}$, wich yield mathematically identical results. The most likely explanation for this was that Elman’s starting point was Jordan’s network, which had a separated memory unit. Regardless, keep in mind we don’t need $c$ units to design a functionally identical network.&lt;/p&gt;

&lt;p&gt;Elman performed multiple experiments with this architecture demonstrating it was capable to solve multiple problems with a sequential structure: a temporal version of the XOR problem; learning the structure (i.e., vowels and consonants sequential order) in sequences of letters; discovering the notion of “word”, and even learning complex lexical classes like word order in short sentences. Let’s briefly explore the temporal XOR solution as an exemplar. &lt;strong&gt;Table 1&lt;/strong&gt; shows the XOR problem:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Table 1&lt;/strong&gt;: Truth Table For XOR Function&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;$x_1$&lt;/th&gt;
      &lt;th&gt;$x_2$&lt;/th&gt;
      &lt;th&gt;$y$&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Here is a way to transform the XOR problem into a sequence. Consider the following vector:&lt;/p&gt;

\[s= [1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,...]\]

&lt;p&gt;In $\bf{s}$, the first and second elements, $s_1$ and $s_2$, represent $x_1$ and $x_2$ inputs of &lt;strong&gt;Table 1&lt;/strong&gt;, whereas the third element, $s_3$, represents the corresponding output $y$. This pattern repeats until the end of the sequence $s$ as shown in &lt;strong&gt;Figure 4&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 4: Temporal XOR&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-9/temporal-xor.png&quot; width=&quot;70%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Elman trained his network with a 3,000 elements sequence for 600 iterations over the entire dataset, on the task of predicting the next item $s_{t+1}$ of the sequence $s$, meaning that he fed inputs to the network &lt;strong&gt;one by one&lt;/strong&gt;. He showed that &lt;strong&gt;error pattern&lt;/strong&gt; followed a predictable trend: the mean squared error was &lt;strong&gt;lower every 3 outputs&lt;/strong&gt;, and higher in between, meaning the network learned to predict the third element in the sequence, as shown in &lt;strong&gt;Chart 1&lt;/strong&gt; (the numbers are made up, but the pattern is the same found by Elman (1990)).&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;altair&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;MSE&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.35&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.27&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.14&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.35&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.36&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.31&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                  &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;cycle&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;13&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)})&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mark_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;cycle&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;MSE&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;properties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Chart 1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div id=&quot;altair-viz-ef2294cd0f0d4c57b58a84b45ed20df7&quot;&gt;&lt;/div&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
  (function(spec, embedOpt){
    const outputDiv = document.getElementById(&quot;altair-viz-ef2294cd0f0d4c57b58a84b45ed20df7&quot;);
    const paths = {
      &quot;vega&quot;: &quot;https://cdn.jsdelivr.net/npm//vega@5?noext&quot;,
      &quot;vega-lib&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-lib?noext&quot;,
      &quot;vega-lite&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-lite@4.0.2?noext&quot;,
      &quot;vega-embed&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-embed@6?noext&quot;,
    };

    function loadScript(lib) {
      return new Promise(function(resolve, reject) {
        var s = document.createElement(&apos;script&apos;);
        s.src = paths[lib];
        s.async = true;
        s.onload = () =&gt; resolve(paths[lib]);
        s.onerror = () =&gt; reject(`Error loading script: ${paths[lib]}`);
        document.getElementsByTagName(&quot;head&quot;)[0].appendChild(s);
      });
    }

    function showError(err) {
      outputDiv.innerHTML = `&lt;div class=&quot;error&quot; style=&quot;color:red;&quot;&gt;${err}&lt;/div&gt;`;
      throw err;
    }

    function displayChart(vegaEmbed) {
      vegaEmbed(outputDiv, spec, embedOpt)
        .catch(err =&gt; showError(`Javascript Error: ${err.message}&lt;br&gt;This usually means there&apos;s a typo in your chart specification. See the javascript console for the full traceback.`));
    }

    if(typeof define === &quot;function&quot; &amp;&amp; define.amd) {
      requirejs.config({paths});
      require([&quot;vega-embed&quot;], displayChart, err =&gt; showError(`Error loading script: ${err.message}`));
    } else if (typeof vegaEmbed === &quot;function&quot;) {
      displayChart(vegaEmbed);
    } else {
      loadScript(&quot;vega&quot;)
        .then(() =&gt; loadScript(&quot;vega-lite&quot;))
        .then(() =&gt; loadScript(&quot;vega-embed&quot;))
        .catch(showError)
        .then(() =&gt; displayChart(vegaEmbed));
    }
  })({&quot;config&quot;: {&quot;view&quot;: {&quot;continuousWidth&quot;: 400, &quot;continuousHeight&quot;: 300}}, &quot;data&quot;: {&quot;name&quot;: &quot;data-2fb8460af59365532c40b2d7e4fe648a&quot;}, &quot;mark&quot;: &quot;line&quot;, &quot;encoding&quot;: {&quot;x&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;cycle&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;MSE&quot;}}, &quot;title&quot;: &quot;Chart 1&quot;, &quot;$schema&quot;: &quot;https://vega.github.io/schema/vega-lite/v4.0.2.json&quot;, &quot;datasets&quot;: {&quot;data-2fb8460af59365532c40b2d7e4fe648a&quot;: [{&quot;MSE&quot;: 0.35, &quot;cycle&quot;: 1}, {&quot;MSE&quot;: 0.15, &quot;cycle&quot;: 2}, {&quot;MSE&quot;: 0.3, &quot;cycle&quot;: 3}, {&quot;MSE&quot;: 0.27, &quot;cycle&quot;: 4}, {&quot;MSE&quot;: 0.14, &quot;cycle&quot;: 5}, {&quot;MSE&quot;: 0.4, &quot;cycle&quot;: 6}, {&quot;MSE&quot;: 0.35, &quot;cycle&quot;: 7}, {&quot;MSE&quot;: 0.12, &quot;cycle&quot;: 8}, {&quot;MSE&quot;: 0.36, &quot;cycle&quot;: 9}, {&quot;MSE&quot;: 0.31, &quot;cycle&quot;: 10}, {&quot;MSE&quot;: 0.15, &quot;cycle&quot;: 11}, {&quot;MSE&quot;: 0.32, &quot;cycle&quot;: 12}]}}, {&quot;mode&quot;: &quot;vega-lite&quot;});
&lt;/script&gt;

&lt;p&gt;An immediate advantage of this approach is the network can take &lt;strong&gt;inputs of any length&lt;/strong&gt;, without having to alter the network architecture at all.&lt;/p&gt;

&lt;p&gt;In the same paper, Elman showed that the &lt;strong&gt;internal (hidden) representations&lt;/strong&gt; learned by the network grouped into meaningful categories, this is, &lt;strong&gt;semantically similar words group together&lt;/strong&gt; when analyzed with &lt;a href=&quot;https://en.wikipedia.org/wiki/Hierarchical_clustering&quot;&gt;hierarchical clustering&lt;/a&gt;. This was remarkable as demonstrated the utility of RNNs as a model of cognition in sequence-based problems.&lt;/p&gt;

&lt;h2 id=&quot;interlude-vanishing-and-exploding-gradients-in-rnns&quot;&gt;Interlude: vanishing and exploding gradients in RNNs&lt;/h2&gt;

&lt;p&gt;Turns out, training recurrent neural networks is hard. Considerably harder than multilayer-perceptrons. When faced with the task of training &lt;strong&gt;very deep networks&lt;/strong&gt;, like RNNs, the gradients have the impolite tendency of either (1) &lt;strong&gt;vanishing&lt;/strong&gt;, or (2) &lt;strong&gt;exploding&lt;/strong&gt; (Bengio et al, 1994; Pascanu et al, 2012). Recall that RNNs can be unfolded so that recurrent connections follow pure feed-forward computations. This unrolled RNN will have as many layers as elements in the sequence. Thus, a sequence of 50 words will be unrolled as an RNN of 50 layers (taking “word” as a unit).&lt;/p&gt;

&lt;p&gt;Concretely, the &lt;strong&gt;vanishing gradient problem&lt;/strong&gt; will make close to impossible to learn &lt;strong&gt;long-term dependencies&lt;/strong&gt; in sequences. Let’s say you have a collection of poems, where the last sentence refers to the first one. Such a dependency will be hard to learn for a deep RNN where gradients vanish as we move backward in the network. The &lt;strong&gt;exploding gradient problem&lt;/strong&gt; will completely derail the learning process. In very deep networks this is often a problem because more layers amplify the effect of large gradients, compounding into very large updates to the network weights, to the point values completely blow up.&lt;/p&gt;

&lt;p&gt;Here is the intuition for the &lt;strong&gt;mechanics of gradient vanishing&lt;/strong&gt;: when gradients &lt;em&gt;begin small&lt;/em&gt;, as you move backward through the network computing gradients, they will get even smaller as you get closer to the input layer. Consequently, when doing the weight update based on such gradients, the weights closer to the output layer will obtain larger updates than weights closer to the input layer. This means that the weights closer to the input layer will hardly change at all, whereas the weights closer to the output layer will change a lot. This is a serious problem when &lt;strong&gt;earlier layers matter for prediction&lt;/strong&gt;: they will keep propagating more or less the same signal forward because no learning (i.e., weight updates) will happen, which may significantly hinder the network performance.&lt;/p&gt;

&lt;p&gt;Here is the intuition for the &lt;strong&gt;mechanics of gradient explosion&lt;/strong&gt;: when gradients &lt;em&gt;begin large&lt;/em&gt;, as you move backward through the network computing gradients, they will get even larger as you get closer to the input layer. Consequently, when doing the weight update based on such gradients, the weights closer to the input layer will obtain larger updates than weights closer to the output layer. Learning can go wrong really fast. Recall that the signal propagated by each layer is the outcome of taking the product between the previous hidden-state and the current hidden-state. If the weights in earlier layers get really large, they will forward-propagate larger and larger signals on each iteration, and the predicted output values will spiral-up out of control, making the error $y-\hat{y}$ so large that the network will be unable to learn at all. In fact, your computer will “overflow” quickly as it would unable to represent numbers that big. Very dramatic.&lt;/p&gt;

&lt;p&gt;The mathematics of gradient vanishing and explosion gets complicated quickly. If you want to delve into the mathematics see &lt;a href=&quot;http://ai.dinfo.unifi.it/paolo/ps/tnn-94-gradient.pdf&quot;&gt;Bengio et all (1994)&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1211.5063&quot;&gt;Pascanu et all (2012)&lt;/a&gt;, and &lt;a href=&quot;https://arxiv.org/abs/1712.05577&quot;&gt;Philipp et all (2017)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For our purposes, I’ll give you a simplified numerical example for intuition. Consider the task of predicting a vector $y = \begin{bmatrix} 1 &amp;amp; 1 \end{bmatrix}$, from inputs $x = \begin{bmatrix} 1 &amp;amp; 1 \end{bmatrix}$, with a multilayer-perceptron with 5 hidden layers and tanh activation functions. We have two cases:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the weight matrix $W_l$ is initialized to large values $w_{ij} = 2$&lt;/li&gt;
  &lt;li&gt;the weight matrix $W_s$ is initialized to small values $w_{ij} = 0.02$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now, let’s compute a single forward-propagation pass:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;W_l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;h1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W_l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;h2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W_l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;h3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W_l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;h4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W_l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;h5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W_l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_hat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W_l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;output for large initial weights: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_hat&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;output for large initial weights: 
 [[3.99730269]
 [3.99730269]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;W_s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.02&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.02&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.02&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.02&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;h1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W_s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;h2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W_s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;h3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W_s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;h4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W_s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;h5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W_s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_hat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W_s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;output for small initial weights: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_hat&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;output for small initial weights: 
 [[4.09381337e-09]
 [4.09381337e-09]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We see that for $W_l$ the output $\hat{y}\approx4$, whereas for $W_s$ the output $\hat{y} \approx 0$. Why does this matter? We haven’t done the gradient computation but you can probably anticipate what it’s going to happen: for the $W_l$ case, the gradient update is going to be very large, and for the $W_s$ very small. If you keep cycling through forward and backward passes these problems will become worse, leading to gradient explosion and vanishing respectively.&lt;/p&gt;

&lt;h3 id=&quot;long-short-term-memory-network&quot;&gt;Long Short-Term Memory Network&lt;/h3&gt;

&lt;p&gt;Several challenges difficulted progress in RNN in the early ’90s (Hochreiter &amp;amp; Schmidhuber, 1997; Pascanu et al, 2012). In addition to vanishing and exploding gradients, we have the fact that the &lt;strong&gt;forward computation is slow&lt;/strong&gt;, as RNNs can’t compute in parallel: to preserve the time-dependencies through the layers, each layer has to be computed sequentially, which naturally takes more time. Elman networks proved to be effective at solving relatively simple problems, but as the sequences scaled in size and complexity, this type of network struggle.&lt;/p&gt;

&lt;p&gt;Several approaches were proposed in the ’90s to address the aforementioned issues like time-delay neural networks (Lang et al, 1990), simulated annealing (Bengio et al., 1994), and others. The architecture that really moved the field forward was the so-called &lt;strong&gt;Long Short-Term Memory (LSTM) Network&lt;/strong&gt;, introduced by &lt;a href=&quot;https://en.wikipedia.org/wiki/Sepp_Hochreiter&quot;&gt;Sepp Hochreiter&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/J%C3%BCrgen_Schmidhuber&quot;&gt;Jurgen Schmidhuber&lt;/a&gt; in 1997. As the name suggests, the defining characteristic of LSTMs is the addition of units combining both short-memory and long-memory capabilities.&lt;/p&gt;

&lt;p&gt;In LSTMs, instead of having a simple memory unit “cloning” values from the hidden unit as in Elman networks, we have a (1) &lt;strong&gt;cell unit&lt;/strong&gt; (a.k.a., memory unit) which effectively acts as long-term memory storage, and (2) a &lt;strong&gt;hidden-state&lt;/strong&gt; which acts as a memory controller. These two elements are integrated as a circuit of logic gates controlling the flow of information at each time-step. Understanding the notation is crucial here, which is depicted in &lt;strong&gt;Figure 5&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 5: LSTM architecture&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-9/lstm-unit.png&quot; width=&quot;70%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In LSTMs $x_t$, $h_t$, and $c_t$ represent vectors of values. Lightish-pink circles represent element-wise operations, and darkish-pink boxes are fully-connected layers with trainable weights. The top part of the diagram acts as a &lt;strong&gt;memory storage&lt;/strong&gt;, whereas the bottom part has a double role: (1) passing the hidden-state information from the previous time-step $t-1$ to the next time step $t$, and (2) to regulate the &lt;strong&gt;influx&lt;/strong&gt; of information from $x_t$ and $h_{t-1}$ &lt;strong&gt;into&lt;/strong&gt; the memory storage, and the &lt;strong&gt;outflux&lt;/strong&gt; of information &lt;strong&gt;from&lt;/strong&gt; the memory storage into the next hidden state $h-t$. The second role is the core idea behind LSTM. You can think about it as making &lt;strong&gt;three decisions&lt;/strong&gt; at each time-step:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Is the &lt;em&gt;old information&lt;/em&gt; $c_{t-1}$ worth to keep in my memory storage $c_t$?&lt;/strong&gt; If so, let the information pass, otherwise, “forget” such information. This is controlled by the &lt;em&gt;forget gate&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Is this &lt;em&gt;new information&lt;/em&gt; (inputs) worth to be saved into my memory storage $c_t$?&lt;/strong&gt; If so, let information flow into $c_t$. This is controlled by the &lt;em&gt;input gate&lt;/em&gt; and the &lt;em&gt;candidate memory cell&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What elements of the information saved in my memory storage $c_t$ are relevant for the computation of the next hidden-state $h_t$?&lt;/strong&gt; Select them from $c_t$, combine them new hidden-state output, and let them pass into the next hidden-state $h_t$. This is controlled by the &lt;em&gt;output gate&lt;/em&gt; and the &lt;em&gt;tanh&lt;/em&gt; function.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Decisions 1 and 2 will determine the information that keeps flowing through the memory storage at the top. Decision 3 will determine the information that flows to the next hidden-state at the bottom. The conjunction of these decisions sometimes is called “memory block”. Now, keep in mind that this &lt;em&gt;sequence of decision&lt;/em&gt; is just a convenient &lt;em&gt;interpretation&lt;/em&gt; of LSTM mechanics. In practice, the weights are the ones determining what each function ends up doing, which may or may not fit well with human intuitions or design objectives.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 6: LSTM as a sequence of decisions&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-9/lstm-choices.png&quot; width=&quot;70%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To put LSTMs in context, imagine the following simplified scenerio: we are trying to &lt;strong&gt;predict the next word in a sequence&lt;/strong&gt;. Let’s say, squences are about sports. From past sequences, we saved in the memory block the type of sport: “soccer”. For the current sequence, we receive a phrase like “A basketball player…”. In such a case, we first want to “forget” the previous type of sport “soccer” (&lt;em&gt;decision 1&lt;/em&gt;) by multplying $c_{t-1} \odot f_t$. Next, we want to “update” memory with the new type of sport, “basketball” (&lt;em&gt;decision 2&lt;/em&gt;), by adding $c_t = (c_{t-1} \odot f_t) + (i_t \odot \tilde{c_t})$. Finally, we want to output (&lt;em&gt;decision 3&lt;/em&gt;) a verb relevant for “A basketball player…”, like “shoot” or “dunk” by $\hat{y_t} = softmax(W_{hz}h_t + b_z)$.&lt;/p&gt;

&lt;p&gt;LSTMs long-term memory capabilities make them good at capturing long-term dependencies. The memory cell effectively counteracts the vanishing gradient problem at preserving information as long the forget gate does not “erase” past information (Graves, 2012). All the above make LSTMs sere](https://en.wikipedia.org/wiki/Long_short-term_memory#Applications)). For instance, when you use &lt;a href=&quot;https://ai.googleblog.com/2015/08/the-neural-networks-behind-google-voice.html&quot;&gt;Google’s Voice Transcription&lt;/a&gt; services an RNN is doing the hard work of recognizing your voice.uccessful in practical applications in sequence-modeling (see a list &lt;a href=&quot;https://en.wikipedia.org/wiki/Long_short-term_memory#Applications&quot;&gt;here&lt;/a&gt;). For instance, when you use &lt;a href=&quot;https://ai.googleblog.com/2015/08/the-neural-networks-behind-google-voice.html&quot;&gt;Google’s Voice Transcription&lt;/a&gt; services an RNN is doing the hard work of recognizing your voice.&lt;/p&gt;

&lt;h3 id=&quot;rnns-and-cognition&quot;&gt;RNNs and cognition&lt;/h3&gt;

&lt;p&gt;As with Convolutional Neural Networks, researchers utilizing RNN for approaching sequential problems like natural language processing (NLP) or time-series prediction, do not &lt;em&gt;necessarily&lt;/em&gt; care about (although some might) how good of a model of cognition and brain-activity are RNNs. What they really care is about solving problems like translation, speech recognition, and stock market prediction, and many advances in the field come from pursuing such goals. Still, RNN has many &lt;strong&gt;desirable traits as a model of neuro-cognitive activity&lt;/strong&gt;, and have been prolifically used to &lt;strong&gt;model several aspects of human cognition and behavior&lt;/strong&gt;: child behavior in an object permanence tasks (Munakata et al, 1997); knowledge-intensive text-comprehension (St. John, 1992); processing in quasi-regular domains, like English word reading (Plaut et al., 1996); human performance in processing recursive language structures (Christiansen &amp;amp; Chater, 1999); human sequential action (Botvinick &amp;amp; Plaut, 2004); movement patterns in typical and atypical developing children (Muñoz-Organero et al., 2019). And many others. Neuroscientists have used RNNs to model a wide variety of aspects as well (for reviews see Barak, 2017, Güçlü &amp;amp; van Gerven, 2017, Jarne &amp;amp; Laje, 2019). Overall, RNN has demonstrated to be a productive tool for modeling cognitive and brain function, in distributed representations paradigm.&lt;/p&gt;

&lt;h2 id=&quot;mathematical-formalization&quot;&gt;Mathematical formalization&lt;/h2&gt;

&lt;p&gt;There are two mathematically complex issues with RNNs: (1) computing hidden-states, and (2) backpropagation. The rest are common operations found in multilayer-perceptrons. LSTMs and its many variants are the facto standards when modeling any kind of sequential problem. Elman networks can be seen as a simplified version of an LSTM, so I’ll focus my attention on LSTMs for the most part. My exposition is based on a combination of sources that you may want to review for extended explanations (Bengio et al., 1994; Hochreiter &amp;amp; Schmidhuber, 1997; Graves, 2012; Chen, 2016; Zhang et al., 2020).&lt;/p&gt;

&lt;p&gt;The LSTM architecture can be desribed by:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Forward pass&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;non-linear forget function&lt;/li&gt;
  &lt;li&gt;non-linear input function&lt;/li&gt;
  &lt;li&gt;non-linear candidate-memory function&lt;/li&gt;
  &lt;li&gt;non-linear output function&lt;/li&gt;
  &lt;li&gt;memory cell function&lt;/li&gt;
  &lt;li&gt;non-linear hidden-state function&lt;/li&gt;
  &lt;li&gt;softmax function (output)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Backward pass&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Cost-function&lt;/li&gt;
  &lt;li&gt;Learning procedure (backpropagation)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Following the indices for each function requires some definitions. I’ll assume we have $h$ hidden units, training sequences of size $n$, and $d$ input units.&lt;/p&gt;

\[\text{input-units} = x_i \in \mathbb{R}^d \\ 
\text{training-sequence} = s_i \in \mathbb{R}^n \\
\text{output-class} = y_i \in \mathbb{R}^k \\
\text{Input-layer} = X_t \in \mathbb{R}^{n\times d} \\
\text{hidden-layer} = H_t \in \mathbb{R}^{n\times h}\]

&lt;h2 id=&quot;forget-function&quot;&gt;Forget function&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;forget function&lt;/strong&gt; is a sigmoidal mapping combining three elements: input vector $x_t$, past hidden-state $h_{t-1}$, and a bias term $b_f$. We didn’t mentioned the bias before, but it is the same bias that all neural networks incorporate, one for each unit in $f$. More formally:&lt;/p&gt;

\[f_t = \sigma(W_{xf}x_t + W_{hf}h_{t-1} + b_f)\]

&lt;p&gt;Each matrix $W$ has dimensionality equal to (number of incoming units, number for connected units). For example, $W_{xf}$ refers to $W_{input-units, forget-units}$. Keep this in mind to read the indices of the $W$ matrices for subsequent definitions.&lt;/p&gt;

&lt;p&gt;Here is an important insight: What would it happen if $f_t = 0$? If you look at the diagram in &lt;strong&gt;Figure 6&lt;/strong&gt;, $f_t$ performs an elementwise multiplication of each element in $c_{t-1}$, meaning that every value would be reduced to $0$. In short, the network would completely “forget” past states. Naturally, if $f_t = 1$, the network would keep its memory intact.&lt;/p&gt;

&lt;h2 id=&quot;input-function-and-candidate-memory-function&quot;&gt;Input function and Candidate memory function&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;input function&lt;/strong&gt; is a sigmoidal mapping combining three elements: input vector $x_t$, past hidden-state $h_{t-1}$, and a bias term $b_f$. It’s defined as:&lt;/p&gt;

\[i_t = \sigma(W_{xi}x_t + W_{hi}h_{t-1} + b_i)\]

&lt;p&gt;The &lt;strong&gt;candidate memory function&lt;/strong&gt; is an hyperbolic tanget function combining the same elements that $i_t$. It’s defined as:&lt;/p&gt;

\[\tilde{c}_t = tanh(W_{xc}x_t + W_{hc}h_{t-1} + b_c)\]

&lt;p&gt;Both functions are combined to update the memory cell.&lt;/p&gt;

&lt;h2 id=&quot;output-function&quot;&gt;Output function&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;output function&lt;/strong&gt; is a sigmoidal mapping combining three elements: input vector $x_t$, past hidden-state $h_{t-1}$, and a bias term $b_f$. Is defined as:&lt;/p&gt;

\[o_t = \sigma(W_{xo}x_t + W_{ho}h_{t-1} + b_o)\]

&lt;h2 id=&quot;memory-cell-function&quot;&gt;Memory cell function&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;memory cell function&lt;/strong&gt; (what I’ve been calling “memory storage” for conceptual clarity), combines the effect of the forget function, input function, and candidate memory function. It’s defined as:&lt;/p&gt;

\[c_t = (c_{t-1} \odot f_t) + (i_t \odot \tilde{c_t})\]

&lt;p&gt;Where $\odot$ implies an elementwise multiplication (instead of the usual dot product). This expands to:&lt;/p&gt;

\[c_t = (c_{t-1} \odot \sigma(W_{xf}x_t + W_{hf}h_{t-1} + b_f)) + (\sigma(W_{xi}x_t + W_{hi}h_{t-1} + b_i) \odot tanh(W_{xc}x_t + W_{hc}h_{t-1} + b_c))\]

&lt;h2 id=&quot;hidden-state-function&quot;&gt;Hidden-state function&lt;/h2&gt;

&lt;p&gt;The next &lt;strong&gt;hidden-state function&lt;/strong&gt; combines the effect of the output function and the contents of the memory cell scaled by a tanh function. It is defined as:&lt;/p&gt;

\[h_t = O_t \odot tanh(c_t)\]

&lt;h2 id=&quot;output-function-1&quot;&gt;Output function&lt;/h2&gt;

&lt;p&gt;The output function will depend upon the problem to be approached. For our our purposes, we will assume a multi-class problem, for which the &lt;strong&gt;softmax function&lt;/strong&gt; is appropiated. For this, we first pass the hidden-state by a linear function, and then the softmax as:&lt;/p&gt;

\[z_t = (W_{hz}h_t + b_z)\\
\hat{y}_t = softmax(z_t) = \frac{e^{z_t}}{\sum_{j=1}^K e^{z_j}}\]

&lt;p&gt;The softmax computes the exponent for each $z_t$ and then normalized by dividing by the sum of every output value exponentiated. In this manner, the output of the softmax can be interpreted as the likelihood value $p$.&lt;/p&gt;

&lt;h2 id=&quot;cost-function&quot;&gt;Cost function&lt;/h2&gt;

&lt;p&gt;As with the output function, the cost function will depend upon the problem. For regression problems, the Mean-Squared Error can be used. For our purposes (classification), the cross-entropy function is appropriated. It’s defined as:&lt;/p&gt;

\[E_i = - \sum_t y_ilog(p_i)\]

&lt;p&gt;Where $y_i$ is the true label for the $ith$ output unit, and $log(p_i)$ is the log of the softmax value for the $ith$ output unit. The summation indicates we need to aggregate the cost at each time-step.&lt;/p&gt;

&lt;h3 id=&quot;learning-procedure-backpropagation-through-time-bptt&quot;&gt;Learning procedure: Backpropagation Through Time (BPTT)&lt;/h3&gt;

&lt;p&gt;Originally, Hochreiter and Schmidhuber (1997) trained LSTMs with a combination of approximate gradient descent computed with a combination of real-time recurrent learning and backpropagation through time (BPTT). Nevertheless, LSTM can be trained with pure backpropagation. Following Graves (2012), I’ll only describe BTT because is more accurate,  easier to debug and to describe.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: we call it &lt;strong&gt;backpropagation through time&lt;/strong&gt; because of the sequential time-dependent structure of RNNs. Recall that each layer represents a time-step, and forward propagation happens in sequence, one layer computed after the other. Hence, when we backpropagate, we do the same but backward (i.e., “through time”).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 7: Three-layer simplified RNN&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-9/simple-rnn.png&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I reviewed backpropagation for a simple multilayer perceptron &lt;a href=&quot;https://pabloinsente.github.io/the-multilayer-perceptron&quot;&gt;here&lt;/a&gt;. Nevertheless, I’ll sketch BPTT for the simplest case as shown in &lt;strong&gt;Figure 7&lt;/strong&gt;, this is, with a generic non-linear hidden-layer similar to Elman network without “context units” (some like to call it “vanilla” RNN, which I avoid because I believe is derogatory against vanilla!). This exercise will allow us to review backpropagation and to understand how it differs from BPTT. We begin by defining a simplified RNN as:&lt;/p&gt;

\[z_t = W_{hz}h_t + b_z\\
h_t = \sigma(W_{hh}h_{t-1} + W_{xh}x_t+b_h)\]

&lt;p&gt;Where $h_t$ and $z_t$ indicates a hidden-state (or layer) and  the output respectively. Therefore, &lt;strong&gt;we have to compute gradients w.r.t. five sets of weights&lt;/strong&gt;: ${W_{hz}, W_{hh}, W_{xh}, b_z, b_h}$.&lt;/p&gt;

&lt;p&gt;First, consider the error derivatives w.r.t. $W_{hz}$ at time $t$, the weight matrix for the linear function at the output layer. Recall that $W_{hz}$ is shared across all time-steps, hence, we can compute the gradients at each time step and then take the sum as:&lt;/p&gt;

\[\frac{\partial{E}}{\partial{W_{hz}}} = \sum_t\frac{\partial{E}}{\partial{z_t}} \frac{\partial{z_t}}{\partial{W_{hz}}}\]

&lt;p&gt;Same for the bias term:&lt;/p&gt;

\[\frac{\partial{E}}{\partial{b_z}} = \sum_t\frac{\partial{E}}{\partial{z_t}} \frac{\partial{z_t}}{\partial{b_z}}\]

&lt;p&gt;That part is straightforward. The issue arises when we try to compute the gradients w.r.t. the wights $W_{hh}$ in the hidden layer. Consider a three layer RNN (i.e., unfolded over three time-steps). In such a case, we have:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$E_3$ depens on $z_3$&lt;/li&gt;
  &lt;li&gt;$z_3$ depends on $h_3$&lt;/li&gt;
  &lt;li&gt;$h_3$ depens on $h_2$&lt;/li&gt;
  &lt;li&gt;$h_2$ depens on $h_1$&lt;/li&gt;
  &lt;li&gt;$h_1$ depens on $h_0$, where $h_0$ is a random starting state.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now, we have that $E_3$ w.r.t to $h_3$ becomes:&lt;/p&gt;

\[\frac{\partial{E_3}}{\partial{W_{hh}}} = 
\frac{\partial{E_3}}{\partial{z_3}}
\frac{\partial{z_3}}{\partial{h_3}}
\frac{\partial{h_3}}{\partial{W_{hh}}}\]

&lt;p&gt;The issue here is that $h_3$ depends on $h_2$, since according to our definition, the $W_{hh}$ is multiplied by $h_{t-1}$, meaning &lt;strong&gt;we can’t compute $\frac{\partial{h_3}}{\partial{W_{hh}}}$ directly&lt;/strong&gt;. Othewise, we would be treating $h_2$ as a constant, which is incorrect: is a function. What we need to do is to &lt;strong&gt;compute the gradients separately&lt;/strong&gt;: the direct contribution of ${W_{hh}}$ on $E$ and the indirect contribution via $h_2$. Following the rules of calculus in multiple variables, we compute them independently and add them up together as:&lt;/p&gt;

\[\frac{\partial{E_3}}{\partial{W_{hh}}} = 
\frac{\partial{E_3}}{\partial{z_3}}
\frac{\partial{z_3}}{\partial{h_3}}
\frac{\partial{h_3}}{\partial{W_{hh}}}+
\frac{\partial{E_3}}{\partial{z_3}}
\frac{\partial{z_3}}{\partial{h_3}}
\frac{\partial{h_3}}{\partial{h_2}}
\frac{\partial{h_2}}{\partial{W_{hh}}}\]

&lt;p&gt;Again, we have that we can’t compute $\frac{\partial{h_2}}{\partial{W_{hh}}}$ directly. Following the same procedure, we have that our full expression becomes:&lt;/p&gt;

\[\frac{\partial{E_3}}{\partial{W_{hh}}} = 
\frac{\partial{E_3}}{\partial{z_3}}
\frac{\partial{z_3}}{\partial{h_3}}
\frac{\partial{h_3}}{\partial{W_{hh}}}+
\frac{\partial{E_3}}{\partial{z_3}}
\frac{\partial{z_3}}{\partial{h_3}}
\frac{\partial{h_3}}{\partial{h_2}}
\frac{\partial{h_2}}{\partial{W_{hh}}}+
\frac{\partial{E_3}}{\partial{z_3}}
\frac{\partial{z_3}}{\partial{h_3}}
\frac{\partial{h_3}}{\partial{h_2}}
\frac{\partial{h_2}}{\partial{h_1}}
\frac{\partial{h_1}}{\partial{W_{hh}}}\]

&lt;p&gt;Essentially, this means that we compute and add the contribution of $W_{hh}$ to $E$ at each time-step. The expression for $b_h$ is the same:&lt;/p&gt;

\[\frac{\partial{E_3}}{\partial{b_h}} = 
\frac{\partial{E_3}}{\partial{z_3}}
\frac{\partial{z_3}}{\partial{h_3}}
\frac{\partial{h_3}}{\partial{b_h}}+
\frac{\partial{E_3}}{\partial{z_3}}
\frac{\partial{z_3}}{\partial{h_3}}
\frac{\partial{h_3}}{\partial{h_2}}
\frac{\partial{h_2}}{\partial{b_h}}+
\frac{\partial{E_3}}{\partial{z_3}}
\frac{\partial{z_3}}{\partial{h_3}}
\frac{\partial{h_3}}{\partial{h_2}}
\frac{\partial{h_2}}{\partial{h_1}}
\frac{\partial{h_1}}{\partial{b_h}}\]

&lt;p&gt;Finally, we need to compute the gradients w.r.t. $W_{xh}$. Here, again, we have to add the contributions of $W_{xh}$ via $h_3$, $h_2$, and $h_1$:&lt;/p&gt;

\[\frac{\partial{E_3}}{\partial{W_{xh}}} = 
\frac{\partial{E_3}}{\partial{z_3}}
\frac{\partial{z_3}}{\partial{h_3}}
\frac{\partial{h_3}}{\partial{W_{xh}}}+
\frac{\partial{E_3}}{\partial{z_3}}
\frac{\partial{z_3}}{\partial{h_3}}
\frac{\partial{h_3}}{\partial{h_2}}
\frac{\partial{h_2}}{\partial{W_{xh}}}+
\frac{\partial{E_3}}{\partial{z_3}}
\frac{\partial{z_3}}{\partial{h_3}}
\frac{\partial{h_3}}{\partial{h_2}}
\frac{\partial{h_2}}{\partial{h_1}}
\frac{\partial{h_1}}{\partial{W_{xh}}}\]

&lt;p&gt;That’s for BPTT for a simple RNN. The math reviewed here generalizes with minimal changes to more complex architectures as LSTMs. Actually, the only difference regarding LSTMs, is that we have more weights to differentiate for. Instead of a single generic $W_{hh}$, we have $W$ for all the gates: forget, input, output, and candidate cell. The rest remains the same. For a detailed derivation of BPTT for the LSTM see &lt;a href=&quot;https://www.cs.toronto.edu/~graves/preprint.pdf&quot;&gt;Graves (2012)&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1610.02583&quot;&gt;Chen (2016)&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;interlude-sequence-data-representation&quot;&gt;Interlude: Sequence-data representation&lt;/h2&gt;

&lt;p&gt;Working with sequence-data, like text or time-series, requires to pre-process it in a manner that is “digestible” for RNNs. As with any neural network, RNN can’t take raw text as an input, we need to &lt;strong&gt;parse&lt;/strong&gt; text sequences and then &lt;strong&gt;“map”&lt;/strong&gt; them into vectors of numbers. Here I’ll briefly review these issues to provide enough context for our example applications. For an extended revision please refer to &lt;a href=&quot;https://web.stanford.edu/~jurafsky/slp3/&quot;&gt;Jurafsky and Martin (2019)&lt;/a&gt;, &lt;a href=&quot;http://u.cs.biu.ac.il/~yogo/nnlp.pdf&quot;&gt;Goldberg (2015)&lt;/a&gt;, &lt;a href=&quot;https://www.manning.com/books/deep-learning-with-python&quot;&gt;Chollet (2017)&lt;/a&gt;, and &lt;a href=&quot;https://d2l.ai/chapter_recurrent-neural-networks/index.html&quot;&gt;Zhang et al (2020)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Parsing can be done in multiple manners, the most common being:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Using &lt;strong&gt;word&lt;/strong&gt; as a unit, which each word represented as a vector&lt;/li&gt;
  &lt;li&gt;Using &lt;strong&gt;character&lt;/strong&gt; as a unit, with each character represented as a vector&lt;/li&gt;
  &lt;li&gt;Using &lt;strong&gt;n-grams&lt;/strong&gt; of words or characters as a unit, with each n-gram represented as a vector. N-grams are sets of words or characters of size “N” or less.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The process of parsing text into smaller units is called “tokenization”, and each resulting unit is called a “token”, the top pane in &lt;strong&gt;Figure 8&lt;/strong&gt; displays a sketch of the tokenization process.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 8: Tokenization&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-9/text-pro.png&quot; width=&quot;60%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Once a corpus of text has been parsed into tokens, we have to map such tokens into numerical vectors. Two common ways to do this are &lt;strong&gt;one-hot encoding&lt;/strong&gt; approach and the &lt;strong&gt;word embeddings&lt;/strong&gt; approach, as depicted in the bottom pane of &lt;strong&gt;Figure 8&lt;/strong&gt;. We used one-hot encodings to transform the MNIST class-labels into vectors of numbers for classification in the &lt;a href=&quot;https://pabloinsente.github.io/the-convolutional-network&quot;&gt;CovNets blogpost&lt;/a&gt;. In a one-hot encoding vector, each token is mapped into a &lt;em&gt;unique&lt;/em&gt; vector of zeros and ones. The vector size is determined by the vocabullary size. For instance, for the set $x= {“cat”, “dog”, “ferret”}$, we could use a 3-dimensional one-hot encoding as:&lt;/p&gt;

\[\text{cat}=
\begin{bmatrix}
1 \\
0 \\
0
\end{bmatrix},
\text{dog}=
\begin{bmatrix}
0 \\
1 \\
0
\end{bmatrix},
\text{ferret}=
\begin{bmatrix}
0 \\
0 \\
1
\end{bmatrix}\]

&lt;p&gt;One-hot encodings have the advantages of being straightforward to implement and to provide a unique identifier for each token. Its main disadvantage is that tends to create really &lt;strong&gt;sparse&lt;/strong&gt; and &lt;strong&gt;high-dimensional&lt;/strong&gt; representations for a large corpus of texts. For instance, if you tried a one-hot encoding for 50,000 tokens, you’d end up with a 50,000x50,000-dimensional matrix, which may be unpractical for most tasks.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Word embeddings&lt;/strong&gt; represent text by mapping tokens into vectors of real-valued numbers instead of only zeros and ones. This significantly increments the representational capacity of vectors, reducing the required dimensionality for a given corpus of text compared to one-hot encodings. For instance, 50,000 tokens could be represented by as little as 2 or 3 vectors (although the representation may not be very good). Taking the same set $x$ as before, we could have a 2-dimensional word embedding like:&lt;/p&gt;

\[\text{cat}=
\begin{bmatrix}
0.1 \\
0.8
\end{bmatrix},
\text{dog}=
\begin{bmatrix}
0.2 \\
1 
\end{bmatrix},
\text{ferret}=
\begin{bmatrix}
0.6 \\
0.2
\end{bmatrix}\]

&lt;p&gt;You may be wondering why to bother with one-hot encodings when word embeddings are much more space-efficient. The main issue with word-embedding is that &lt;strong&gt;there isn’t an obvious way to map tokens into vectors&lt;/strong&gt; as with one-hot encodings. For instance, you could assign tokens to vectors at random (assuming every token is assigned to a &lt;em&gt;unique&lt;/em&gt; vector). The problem with such approach is that the semantic structure in the corpus is broken. Ideally, you want words of similar meaning mapped into similar vectors. We can preserve the semantic structure of a text corpus in the same manner as everything else in machine learning: by &lt;strong&gt;learning from data&lt;/strong&gt;. There are two ways to do this:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Learning the word embeddings &lt;strong&gt;at the same time&lt;/strong&gt; you train the RNN.&lt;/li&gt;
  &lt;li&gt;Utilizing &lt;strong&gt;pretrained&lt;/strong&gt; word embeddings, this is, embeddings learned in a different task. This is a form of “transfer learning”.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Learning word embeddings for your task is advisable as semantic relationships among words tend to be &lt;strong&gt;context dependent&lt;/strong&gt;. For instance, “exploitation” in the context of mining is related to resource “extraction”, hence relative neutral. But, “exploitation” in the context of labor rights is related to the idea of “abuse”, hence a negative connotation. This is more critical when we are dealing with different languages. Nevertheless, learning embeddings for every task sometimes is impractical, either because your corpus is too “small” (i.e., not enough data to extract semantic relationships), or too “large” (i.e., you don’t have enough time and/or resources to learn the embeddings). Examples of freely accessible pretrained word embeddings are Google’s &lt;a href=&quot;https://code.google.com/archive/p/word2vec/&quot;&gt;Word2vec&lt;/a&gt; and the &lt;a href=&quot;https://nlp.stanford.edu/projects/glove/&quot;&gt;Global Vectors for Word Representation&lt;/a&gt; (GloVe).&lt;/p&gt;

&lt;h2 id=&quot;code-implementation&quot;&gt;Code implementation&lt;/h2&gt;

&lt;p&gt;As in previous blogpost, I’ll use Keras to implement both (a modified version of) the Elman Network for the XOR problem and an LSTM for review prediction based on text-sequences.&lt;/p&gt;

&lt;h3 id=&quot;elman-network-1&quot;&gt;Elman Network&lt;/h3&gt;

&lt;p&gt;By now, it may be clear to you that Elman networks are a simple RNN with two neurons, one for each input pattern, in the hidden-state. Originally, Elman trained his architecture with a truncated version of BPTT, meaning that only considered two time-steps for computing the gradients, $t$ and $t-1$. We will implement a modified version of Elman’s architecture bypassing the “context” unit (which does not alter the result at all) and utilizing BPTT instead of its truncated version.&lt;/p&gt;

&lt;h3 id=&quot;generating-data&quot;&gt;Generating data&lt;/h3&gt;

&lt;p&gt;Nowadays, we don’t need to generate the 3,000 bits sequence that Elman used in his original work. We can simply generate a single pair of training and testing sets for the XOR problem as in &lt;strong&gt;Table 1&lt;/strong&gt;, and pass the training sequence (length two) as the inputs, and the expected outputs as the target. This is very much alike any classification task. An important caveat is that simpleRNN layers in Keras expect an input tensor of shape (number-samples, timesteps, number-input-features). In our case, this has to be: &lt;em&gt;number-samples&lt;/em&gt;= 4,  &lt;em&gt;timesteps&lt;/em&gt;=1, &lt;em&gt;number-input-features&lt;/em&gt;=2. No separate encoding is necessary here because we are manually setting the input and output values to binary vector representations. Finally, we won’t worry about training and testing sets for this example, which is way to simple for that (we will do that for the next example).&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Libraries for this section
&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keras.layers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SimpleRNN&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keras.models&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;altair&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# features
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]]).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# expected values
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;training data shape: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;targets data shape: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;training data shape: (4, 1, 2)
targets data shape: (4, 1)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;elman-network-architecture-in-keras&quot;&gt;Elman network architecture in Keras&lt;/h3&gt;

&lt;p&gt;Defining a (modified) in Keras is extremely simple as shown below.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Define a network as a linear stack of layers
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Add a recurrent layer with 2 units
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;SimpleRNN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Add the output layer with a sigmoid activation
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The model summary shows that our architecture yields 13 trainable parameters.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Model: &quot;sequential_1&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
simple_rnn_1 (SimpleRNN)     (None, 2)                 10        
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 3         
=================================================================
Total params: 13
Trainable params: 13
Non-trainable params: 0
_________________________________________________________________
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;elman-network-application-xor-classification&quot;&gt;Elman network Application: XOR classification&lt;/h2&gt;

&lt;p&gt;Next, we compile and fit our model. I’ll utilize &lt;a href=&quot;https://github.com/keras-team/keras/blob/master/keras/optimizers.py#L376&quot;&gt;Adadelta&lt;/a&gt; (to avoid manually adjusting the learning rate) as the optimizer, and the Mean-Squared Error (as in Elman original work). I’ll train the model for 15,000 epochs over the 4 samples dataset.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Adadelta&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
              &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;mean_squared_error&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
              &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;acc&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;history&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Chart 2&lt;/strong&gt; shows the error curve (red, right axis), and the accuracy curve (blue, left axis) for each epoch.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_transformers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;disable_max_rows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;history&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;history&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;accuracy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;history&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;history&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;acc&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;accuracy&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;time-step&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))})&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;base&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mark_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;blue&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;time-step&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;accuracy&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mark_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;red&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;time-step&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;properties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Chart 2&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;resolve_scale&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;independent&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div id=&quot;altair-viz-33474bf3e5cd497eae523fb0ce9d70a9&quot;&gt;&lt;/div&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
  (function(spec, embedOpt){
    const outputDiv = document.getElementById(&quot;altair-viz-33474bf3e5cd497eae523fb0ce9d70a9&quot;);
    const paths = {
      &quot;vega&quot;: &quot;https://cdn.jsdelivr.net/npm//vega@5?noext&quot;,
      &quot;vega-lib&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-lib?noext&quot;,
      &quot;vega-lite&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-lite@4.0.2?noext&quot;,
      &quot;vega-embed&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-embed@6?noext&quot;,
    };

    function loadScript(lib) {
      return new Promise(function(resolve, reject) {
        var s = document.createElement(&apos;script&apos;);
        s.src = paths[lib];
        s.async = true;
        s.onload = () =&gt; resolve(paths[lib]);
        s.onerror = () =&gt; reject(`Error loading script: ${paths[lib]}`);
        document.getElementsByTagName(&quot;head&quot;)[0].appendChild(s);
      });
    }

    function showError(err) {
      outputDiv.innerHTML = `&lt;div class=&quot;error&quot; style=&quot;color:red;&quot;&gt;${err}&lt;/div&gt;`;
      throw err;
    }

    function displayChart(vegaEmbed) {
      vegaEmbed(outputDiv, spec, embedOpt)
        .catch(err =&gt; showError(`Javascript Error: ${err.message}&lt;br&gt;This usually means there&apos;s a typo in your chart specification. See the javascript console for the full traceback.`));
    }

    if(typeof define === &quot;function&quot; &amp;&amp; define.amd) {
      requirejs.config({paths});
      require([&quot;vega-embed&quot;], displayChart, err =&gt; showError(`Error loading script: ${err.message}`));
    } else if (typeof vegaEmbed === &quot;function&quot;) {
      displayChart(vegaEmbed);
    } else {
      loadScript(&quot;vega&quot;)
        .then(() =&gt; loadScript(&quot;vega-lite&quot;))
        .then(() =&gt; loadScript(&quot;vega-embed&quot;))
        .catch(showError)
        .then(() =&gt; displayChart(vegaEmbed));
    }
  })({&quot;config&quot;: {&quot;view&quot;: {&quot;continuousWidth&quot;: 400, &quot;continuousHeight&quot;: 300}}, &quot;layer&quot;: [{&quot;mark&quot;: {&quot;type&quot;: &quot;line&quot;, &quot;color&quot;: &quot;blue&quot;}, &quot;encoding&quot;: {&quot;x&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;time-step&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;accuracy&quot;}}}, {&quot;mark&quot;: {&quot;type&quot;: &quot;line&quot;, &quot;color&quot;: &quot;red&quot;}, &quot;encoding&quot;: {&quot;x&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;time-step&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;loss&quot;}}}], &quot;data&quot;: {&quot;name&quot;: &quot;data-c03349beb361db3898adb0cd3bce46d2&quot;}, &quot;resolve&quot;: {&quot;scale&quot;: {&quot;y&quot;: &quot;independent&quot;}}, &quot;title&quot;: &quot;Chart 2&quot;, &quot;$schema&quot;: &quot;https://vega.github.io/schema/vega-lite/v4.0.2.json&quot;, &quot;datasets&quot;: {&quot;data-c03349beb361db3898adb0cd3bce46d2&quot;: [{&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.5089318752288818, &quot;time-step&quot;: 0}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.5033257007598877, &quot;time-step&quot;: 1}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.497619867324829, &quot;time-step&quot;: 2}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.4918333292007446, &quot;time-step&quot;: 3}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.4859753847122192, &quot;time-step&quot;: 4}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.4800519943237305, &quot;time-step&quot;: 5}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.4740664958953857, &quot;time-step&quot;: 6}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.4680211544036865, &quot;time-step&quot;: 7}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.4619183540344238, &quot;time-step&quot;: 8}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.4557592868804932, &quot;time-step&quot;: 9}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.4495450258255005, &quot;time-step&quot;: 10}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.4432766437530518, &quot;time-step&quot;: 11}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.4369549751281738, &quot;time-step&quot;: 12}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.4305806159973145, &quot;time-step&quot;: 13}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.4241544008255005, &quot;time-step&quot;: 14}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.4176766872406006, &quot;time-step&quot;: 15}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.4111483097076416, &quot;time-step&quot;: 16}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.4045696258544922, &quot;time-step&quot;: 17}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.3979408740997314, &quot;time-step&quot;: 18}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.3912628889083862, &quot;time-step&quot;: 19}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.3845360279083252, &quot;time-step&quot;: 20}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.377760887145996, &quot;time-step&quot;: 21}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.3709375858306885, &quot;time-step&quot;: 22}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.3640669584274292, &quot;time-step&quot;: 23}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.3571492433547974, &quot;time-step&quot;: 24}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.3501852750778198, &quot;time-step&quot;: 25}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.3431754112243652, &quot;time-step&quot;: 26}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.3361200094223022, &quot;time-step&quot;: 27}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.3290200233459473, &quot;time-step&quot;: 28}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.3218756914138794, &quot;time-step&quot;: 29}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.314687728881836, &quot;time-step&quot;: 30}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.3074567317962646, &quot;time-step&quot;: 31}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.3001835346221924, &quot;time-step&quot;: 32}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.2928686141967773, &quot;time-step&quot;: 33}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.2855126857757568, &quot;time-step&quot;: 34}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.2781165838241577, &quot;time-step&quot;: 35}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.2706809043884277, &quot;time-step&quot;: 36}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.2632067203521729, &quot;time-step&quot;: 37}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.2556945085525513, &quot;time-step&quot;: 38}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.2481452226638794, &quot;time-step&quot;: 39}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.240559697151184, &quot;time-step&quot;: 40}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.2329390048980713, &quot;time-step&quot;: 41}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.2252840995788574, &quot;time-step&quot;: 42}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.2175958156585693, &quot;time-step&quot;: 43}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.2098748683929443, &quot;time-step&quot;: 44}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.202122688293457, &quot;time-step&quot;: 45}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.1943401098251343, &quot;time-step&quot;: 46}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.1865283250808716, &quot;time-step&quot;: 47}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.1786885261535645, &quot;time-step&quot;: 48}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.1708214282989502, &quot;time-step&quot;: 49}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.1629289388656616, &quot;time-step&quot;: 50}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.155011534690857, &quot;time-step&quot;: 51}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.1470710039138794, &quot;time-step&quot;: 52}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.1391081809997559, &quot;time-step&quot;: 53}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.131124496459961, &quot;time-step&quot;: 54}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.1231215000152588, &quot;time-step&quot;: 55}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.1151000261306763, &quot;time-step&quot;: 56}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.1070622205734253, &quot;time-step&quot;: 57}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.099008560180664, &quot;time-step&quot;: 58}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.090941071510315, &quot;time-step&quot;: 59}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.0828611850738525, &quot;time-step&quot;: 60}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.0747700929641724, &quot;time-step&quot;: 61}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.0666694641113281, &quot;time-step&quot;: 62}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.0585607290267944, &quot;time-step&quot;: 63}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.050445556640625, &quot;time-step&quot;: 64}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.0423253774642944, &quot;time-step&quot;: 65}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.034201741218567, &quot;time-step&quot;: 66}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.0260764360427856, &quot;time-step&quot;: 67}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.0179507732391357, &quot;time-step&quot;: 68}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.009826421737671, &quot;time-step&quot;: 69}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 1.0017049312591553, &quot;time-step&quot;: 70}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.9935883283615112, &quot;time-step&quot;: 71}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.9854778051376343, &quot;time-step&quot;: 72}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.9773752689361572, &quot;time-step&quot;: 73}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.9692823886871338, &quot;time-step&quot;: 74}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.9612005949020386, &quot;time-step&quot;: 75}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.9531317949295044, &quot;time-step&quot;: 76}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.945077657699585, &quot;time-step&quot;: 77}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.9370394945144653, &quot;time-step&quot;: 78}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.9290193319320679, &quot;time-step&quot;: 79}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.921018660068512, &quot;time-step&quot;: 80}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.9130392670631409, &quot;time-step&quot;: 81}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.9050825834274292, &quot;time-step&quot;: 82}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.8971503973007202, &quot;time-step&quot;: 83}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.8892441391944885, &quot;time-step&quot;: 84}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.8813656568527222, &quot;time-step&quot;: 85}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.8735163807868958, &quot;time-step&quot;: 86}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.865697979927063, &quot;time-step&quot;: 87}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.8579118847846985, &quot;time-step&quot;: 88}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.8501597046852112, &quot;time-step&quot;: 89}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.8424429893493652, &quot;time-step&quot;: 90}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.8347631692886353, &quot;time-step&quot;: 91}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.8271217346191406, &quot;time-step&quot;: 92}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.819520115852356, &quot;time-step&quot;: 93}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.8119597434997559, &quot;time-step&quot;: 94}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.8044420480728149, &quot;time-step&quot;: 95}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.7969682216644287, &quot;time-step&quot;: 96}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.7895396947860718, &quot;time-step&quot;: 97}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.7821577787399292, &quot;time-step&quot;: 98}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.7748238444328308, &quot;time-step&quot;: 99}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.767538845539093, &quot;time-step&quot;: 100}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.7603042721748352, &quot;time-step&quot;: 101}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.753121018409729, &quot;time-step&quot;: 102}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.7459902763366699, &quot;time-step&quot;: 103}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.7389131784439087, &quot;time-step&quot;: 104}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.7318906784057617, &quot;time-step&quot;: 105}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.7249237298965454, &quot;time-step&quot;: 106}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.7180136442184448, &quot;time-step&quot;: 107}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.7111608386039734, &quot;time-step&quot;: 108}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.7043665647506714, &quot;time-step&quot;: 109}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.6976313591003418, &quot;time-step&quot;: 110}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.6909562945365906, &quot;time-step&quot;: 111}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.6843421459197998, &quot;time-step&quot;: 112}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.6777893304824829, &quot;time-step&quot;: 113}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.6712986826896667, &quot;time-step&quot;: 114}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.6648709177970886, &quot;time-step&quot;: 115}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.6585065126419067, &quot;time-step&quot;: 116}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.6522060036659241, &quot;time-step&quot;: 117}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.6459700465202332, &quot;time-step&quot;: 118}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.6397987604141235, &quot;time-step&quot;: 119}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.6336930394172668, &quot;time-step&quot;: 120}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.6276530027389526, &quot;time-step&quot;: 121}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.6216789484024048, &quot;time-step&quot;: 122}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.6157712936401367, &quot;time-step&quot;: 123}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.6099302172660828, &quot;time-step&quot;: 124}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.6041560173034668, &quot;time-step&quot;: 125}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.5984489917755127, &quot;time-step&quot;: 126}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.5928089618682861, &quot;time-step&quot;: 127}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.5872364044189453, &quot;time-step&quot;: 128}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.5817312002182007, &quot;time-step&quot;: 129}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.5762934684753418, &quot;time-step&quot;: 130}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.5709232091903687, &quot;time-step&quot;: 131}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.5656203031539917, &quot;time-step&quot;: 132}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.5603848695755005, &quot;time-step&quot;: 133}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.5552167892456055, &quot;time-step&quot;: 134}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.5501158833503723, &quot;time-step&quot;: 135}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.5450820326805115, &quot;time-step&quot;: 136}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.5401151180267334, &quot;time-step&quot;: 137}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.5352148413658142, &quot;time-step&quot;: 138}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.5303810834884644, &quot;time-step&quot;: 139}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.5256134271621704, &quot;time-step&quot;: 140}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.5209119319915771, &quot;time-step&quot;: 141}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.516275942325592, &quot;time-step&quot;: 142}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.5117052793502808, &quot;time-step&quot;: 143}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.5071996450424194, &quot;time-step&quot;: 144}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.5027585625648499, &quot;time-step&quot;: 145}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.49838173389434814, &quot;time-step&quot;: 146}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.4940686821937561, &quot;time-step&quot;: 147}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.4898190498352051, &quot;time-step&quot;: 148}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.48563235998153687, &quot;time-step&quot;: 149}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.48150819540023804, &quot;time-step&quot;: 150}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.47744596004486084, &quot;time-step&quot;: 151}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.4734453558921814, &quot;time-step&quot;: 152}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.4695056974887848, &quot;time-step&quot;: 153}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.4656265676021576, &quot;time-step&quot;: 154}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.46180739998817444, &quot;time-step&quot;: 155}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.4580475986003876, &quot;time-step&quot;: 156}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.4543467164039612, &quot;time-step&quot;: 157}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.45070409774780273, &quot;time-step&quot;: 158}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.44711923599243164, &quot;time-step&quot;: 159}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.44359150528907776, &quot;time-step&quot;: 160}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.44012027978897095, &quot;time-step&quot;: 161}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.4367050528526306, &quot;time-step&quot;: 162}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.43334513902664185, &quot;time-step&quot;: 163}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.43004003167152405, &quot;time-step&quot;: 164}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.42678892612457275, &quot;time-step&quot;: 165}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.4235913157463074, &quot;time-step&quot;: 166}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.42044660449028015, &quot;time-step&quot;: 167}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.41735410690307617, &quot;time-step&quot;: 168}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.4143132269382477, &quot;time-step&quot;: 169}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.41132330894470215, &quot;time-step&quot;: 170}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.40838363766670227, &quot;time-step&quot;: 171}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.40549367666244507, &quot;time-step&quot;: 172}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.4026528000831604, &quot;time-step&quot;: 173}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.3998602628707886, &quot;time-step&quot;: 174}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.39711546897888184, &quot;time-step&quot;: 175}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.39441773295402527, &quot;time-step&quot;: 176}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.3917664885520935, &quot;time-step&quot;: 177}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.38916105031967163, &quot;time-step&quot;: 178}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.3866007924079895, &quot;time-step&quot;: 179}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.3840850591659546, &quot;time-step&quot;: 180}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.38161322474479675, &quot;time-step&quot;: 181}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.37918463349342346, &quot;time-step&quot;: 182}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.37679868936538696, &quot;time-step&quot;: 183}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.3744547367095947, &quot;time-step&quot;: 184}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.3721521496772766, &quot;time-step&quot;: 185}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.3698903024196625, &quot;time-step&quot;: 186}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.36766859889030457, &quot;time-step&quot;: 187}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.36548638343811035, &quot;time-step&quot;: 188}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.3633430600166321, &quot;time-step&quot;: 189}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.361238032579422, &quot;time-step&quot;: 190}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.35917067527770996, &quot;time-step&quot;: 191}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.3571404218673706, &quot;time-step&quot;: 192}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.35514670610427856, &quot;time-step&quot;: 193}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.3531888425350189, &quot;time-step&quot;: 194}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.35126638412475586, &quot;time-step&quot;: 195}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.3493786156177521, &quot;time-step&quot;: 196}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.34752511978149414, &quot;time-step&quot;: 197}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.34570518136024475, &quot;time-step&quot;: 198}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.3439183235168457, &quot;time-step&quot;: 199}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.34216398000717163, &quot;time-step&quot;: 200}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.3404415249824524, &quot;time-step&quot;: 201}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.33875054121017456, &quot;time-step&quot;: 202}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.33709046244621277, &quot;time-step&quot;: 203}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.3354607820510864, &quot;time-step&quot;: 204}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.3338608741760254, &quot;time-step&quot;: 205}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.3322903513908386, &quot;time-step&quot;: 206}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.33074861764907837, &quot;time-step&quot;: 207}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.32923516631126404, &quot;time-step&quot;: 208}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.3277495503425598, &quot;time-step&quot;: 209}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.3262912631034851, &quot;time-step&quot;: 210}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.3248597979545593, &quot;time-step&quot;: 211}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.32345473766326904, &quot;time-step&quot;: 212}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.32207560539245605, &quot;time-step&quot;: 213}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.320721834897995, &quot;time-step&quot;: 214}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.319393128156662, &quot;time-step&quot;: 215}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.31808891892433167, &quot;time-step&quot;: 216}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.3168087303638458, &quot;time-step&quot;: 217}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.315552294254303, &quot;time-step&quot;: 218}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.31431901454925537, &quot;time-step&quot;: 219}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.3131085932254791, &quot;time-step&quot;: 220}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.31192055344581604, &quot;time-step&quot;: 221}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.3107544779777527, &quot;time-step&quot;: 222}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.30960994958877563, &quot;time-step&quot;: 223}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.3084866404533386, &quot;time-step&quot;: 224}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.30738404393196106, &quot;time-step&quot;: 225}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.30630189180374146, &quot;time-step&quot;: 226}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.3052397668361664, &quot;time-step&quot;: 227}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.3041972815990448, &quot;time-step&quot;: 228}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.30317404866218567, &quot;time-step&quot;: 229}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.3021697998046875, &quot;time-step&quot;: 230}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.3011840879917145, &quot;time-step&quot;: 231}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.3002166152000427, &quot;time-step&quot;: 232}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2992669939994812, &quot;time-step&quot;: 233}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2983349561691284, &quot;time-step&quot;: 234}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.29742011427879333, &quot;time-step&quot;: 235}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2965221703052521, &quot;time-step&quot;: 236}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2956407964229584, &quot;time-step&quot;: 237}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.29477566480636597, &quot;time-step&quot;: 238}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.293926477432251, &quot;time-step&quot;: 239}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2930929958820343, &quot;time-step&quot;: 240}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2922748923301697, &quot;time-step&quot;: 241}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2914717495441437, &quot;time-step&quot;: 242}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.29068344831466675, &quot;time-step&quot;: 243}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2899096608161926, &quot;time-step&quot;: 244}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.28915008902549744, &quot;time-step&quot;: 245}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2884044945240021, &quot;time-step&quot;: 246}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2876725196838379, &quot;time-step&quot;: 247}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.28695398569107056, &quot;time-step&quot;: 248}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.28624868392944336, &quot;time-step&quot;: 249}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.28555622696876526, &quot;time-step&quot;: 250}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2848765254020691, &quot;time-step&quot;: 251}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2842091917991638, &quot;time-step&quot;: 252}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2835541367530823, &quot;time-step&quot;: 253}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2829110026359558, &quot;time-step&quot;: 254}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2822795808315277, &quot;time-step&quot;: 255}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.28165969252586365, &quot;time-step&quot;: 256}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2810510993003845, &quot;time-step&quot;: 257}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2804535925388336, &quot;time-step&quot;: 258}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2798669636249542, &quot;time-step&quot;: 259}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.279291033744812, &quot;time-step&quot;: 260}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2787255346775055, &quot;time-step&quot;: 261}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.27817025780677795, &quot;time-step&quot;: 262}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.27762508392333984, &quot;time-step&quot;: 263}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.27708977460861206, &quot;time-step&quot;: 264}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.27656418085098267, &quot;time-step&quot;: 265}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.27604806423187256, &quot;time-step&quot;: 266}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2755413055419922, &quot;time-step&quot;: 267}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2750437259674072, &quot;time-step&quot;: 268}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2745550274848938, &quot;time-step&quot;: 269}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2740752696990967, &quot;time-step&quot;: 270}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.27360403537750244, &quot;time-step&quot;: 271}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2731413245201111, &quot;time-step&quot;: 272}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2726869583129883, &quot;time-step&quot;: 273}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2722407281398773, &quot;time-step&quot;: 274}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.27180251479148865, &quot;time-step&quot;: 275}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2713721692562103, &quot;time-step&quot;: 276}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2709495425224304, &quot;time-step&quot;: 277}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2705344557762146, &quot;time-step&quot;: 278}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2701268494129181, &quot;time-step&quot;: 279}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2697264850139618, &quot;time-step&quot;: 280}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2693333029747009, &quot;time-step&quot;: 281}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.26894715428352356, &quot;time-step&quot;: 282}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.26856786012649536, &quot;time-step&quot;: 283}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.26819533109664917, &quot;time-step&quot;: 284}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2678294777870178, &quot;time-step&quot;: 285}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2674700915813446, &quot;time-step&quot;: 286}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.26711714267730713, &quot;time-step&quot;: 287}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.26677048206329346, &quot;time-step&quot;: 288}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.26642993092536926, &quot;time-step&quot;: 289}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.26609548926353455, &quot;time-step&quot;: 290}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.265766978263855, &quot;time-step&quot;: 291}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.265444278717041, &quot;time-step&quot;: 292}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2651273012161255, &quot;time-step&quot;: 293}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.264816015958786, &quot;time-step&quot;: 294}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2645101249217987, &quot;time-step&quot;: 295}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2642097473144531, &quot;time-step&quot;: 296}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.26391470432281494, &quot;time-step&quot;: 297}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2636248767375946, &quot;time-step&quot;: 298}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.26334017515182495, &quot;time-step&quot;: 299}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2630605101585388, &quot;time-step&quot;: 300}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2627858519554138, &quot;time-step&quot;: 301}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.26251599192619324, &quot;time-step&quot;: 302}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2622509300708771, &quot;time-step&quot;: 303}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2619905471801758, &quot;time-step&quot;: 304}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.26173487305641174, &quot;time-step&quot;: 305}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.26148369908332825, &quot;time-step&quot;: 306}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.26123690605163574, &quot;time-step&quot;: 307}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.260994553565979, &quot;time-step&quot;: 308}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2607564926147461, &quot;time-step&quot;: 309}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.26052266359329224, &quot;time-step&quot;: 310}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2602929472923279, &quot;time-step&quot;: 311}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.260067343711853, &quot;time-step&quot;: 312}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2598458230495453, &quot;time-step&quot;: 313}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.25962817668914795, &quot;time-step&quot;: 314}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.259414404630661, &quot;time-step&quot;: 315}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2592044472694397, &quot;time-step&quot;: 316}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2589983344078064, &quot;time-step&quot;: 317}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.258795827627182, &quot;time-step&quot;: 318}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2585970163345337, &quot;time-step&quot;: 319}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2584017515182495, &quot;time-step&quot;: 320}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2582099437713623, &quot;time-step&quot;: 321}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.25802165269851685, &quot;time-step&quot;: 322}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.25783678889274597, &quot;time-step&quot;: 323}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.25765520334243774, &quot;time-step&quot;: 324}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.25747689604759216, &quot;time-step&quot;: 325}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.25730183720588684, &quot;time-step&quot;: 326}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2571300268173218, &quot;time-step&quot;: 327}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2569613456726074, &quot;time-step&quot;: 328}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2567957043647766, &quot;time-step&quot;: 329}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.25663313269615173, &quot;time-step&quot;: 330}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2564736008644104, &quot;time-step&quot;: 331}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2563169598579407, &quot;time-step&quot;: 332}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.25616323947906494, &quot;time-step&quot;: 333}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2560123801231384, &quot;time-step&quot;: 334}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.25586438179016113, &quot;time-step&quot;: 335}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2557190954685211, &quot;time-step&quot;: 336}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.25557658076286316, &quot;time-step&quot;: 337}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2554367780685425, &quot;time-step&quot;: 338}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2552995979785919, &quot;time-step&quot;: 339}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2551650106906891, &quot;time-step&quot;: 340}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.25503304600715637, &quot;time-step&quot;: 341}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.254903644323349, &quot;time-step&quot;: 342}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2547766864299774, &quot;time-step&quot;: 343}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.254652202129364, &quot;time-step&quot;: 344}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.254530131816864, &quot;time-step&quot;: 345}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2544105052947998, &quot;time-step&quot;: 346}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.254293292760849, &quot;time-step&quot;: 347}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.25417834520339966, &quot;time-step&quot;: 348}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2540656626224518, &quot;time-step&quot;: 349}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.25395530462265015, &quot;time-step&quot;: 350}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2538471519947052, &quot;time-step&quot;: 351}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.25374117493629456, &quot;time-step&quot;: 352}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2536374032497406, &quot;time-step&quot;: 353}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.25353574752807617, &quot;time-step&quot;: 354}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2534361481666565, &quot;time-step&quot;: 355}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2533387243747711, &quot;time-step&quot;: 356}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2532432973384857, &quot;time-step&quot;: 357}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2531498670578003, &quot;time-step&quot;: 358}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2530584931373596, &quot;time-step&quot;: 359}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.25296899676322937, &quot;time-step&quot;: 360}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2528814971446991, &quot;time-step&quot;: 361}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.25279587507247925, &quot;time-step&quot;: 362}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.25271210074424744, &quot;time-step&quot;: 363}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.25263020396232605, &quot;time-step&quot;: 364}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.25255006551742554, &quot;time-step&quot;: 365}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.25247180461883545, &quot;time-step&quot;: 366}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.25239524245262146, &quot;time-step&quot;: 367}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.25232046842575073, &quot;time-step&quot;: 368}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2522473633289337, &quot;time-step&quot;: 369}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2521759867668152, &quot;time-step&quot;: 370}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2521062195301056, &quot;time-step&quot;: 371}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2520381212234497, &quot;time-step&quot;: 372}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.25197160243988037, &quot;time-step&quot;: 373}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.25190669298171997, &quot;time-step&quot;: 374}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.25184330344200134, &quot;time-step&quot;: 375}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.25178149342536926, &quot;time-step&quot;: 376}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2517211139202118, &quot;time-step&quot;: 377}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2516622543334961, &quot;time-step&quot;: 378}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2516048550605774, &quot;time-step&quot;: 379}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2515488862991333, &quot;time-step&quot;: 380}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.25149428844451904, &quot;time-step&quot;: 381}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2514410614967346, &quot;time-step&quot;: 382}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2513892352581024, &quot;time-step&quot;: 383}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2513387203216553, &quot;time-step&quot;: 384}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2512894868850708, &quot;time-step&quot;: 385}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2512415647506714, &quot;time-step&quot;: 386}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.25119489431381226, &quot;time-step&quot;: 387}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.251149445772171, &quot;time-step&quot;: 388}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2511051893234253, &quot;time-step&quot;: 389}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.25106215476989746, &quot;time-step&quot;: 390}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.251020222902298, &quot;time-step&quot;: 391}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.250979483127594, &quot;time-step&quot;: 392}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.25093984603881836, &quot;time-step&quot;: 393}, {&quot;accuracy&quot;: 0.25, &quot;loss&quot;: 0.2509012520313263, &quot;time-step&quot;: 394}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.2508637309074402, &quot;time-step&quot;: 395}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.25082728266716003, &quot;time-step&quot;: 396}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.2507918179035187, &quot;time-step&quot;: 397}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.2507574260234833, &quot;time-step&quot;: 398}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.2507238984107971, &quot;time-step&quot;: 399}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.25069135427474976, &quot;time-step&quot;: 400}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.2506597638130188, &quot;time-step&quot;: 401}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.2506290376186371, &quot;time-step&quot;: 402}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.2505992352962494, &quot;time-step&quot;: 403}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.25057026743888855, &quot;time-step&quot;: 404}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.25054213404655457, &quot;time-step&quot;: 405}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.25051480531692505, &quot;time-step&quot;: 406}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.2504882514476776, &quot;time-step&quot;: 407}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.25046250224113464, &quot;time-step&quot;: 408}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.2504374086856842, &quot;time-step&quot;: 409}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.25041311979293823, &quot;time-step&quot;: 410}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.2503895163536072, &quot;time-step&quot;: 411}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.25036656856536865, &quot;time-step&quot;: 412}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.25034430623054504, &quot;time-step&quot;: 413}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.2503226399421692, &quot;time-step&quot;: 414}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.25030165910720825, &quot;time-step&quot;: 415}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.2502812147140503, &quot;time-step&quot;: 416}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.2502613365650177, &quot;time-step&quot;: 417}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.2502420246601105, &quot;time-step&quot;: 418}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.25022315979003906, &quot;time-step&quot;: 419}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.2502049207687378, &quot;time-step&quot;: 420}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.25018709897994995, &quot;time-step&quot;: 421}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.2501698136329651, &quot;time-step&quot;: 422}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2501528859138489, &quot;time-step&quot;: 423}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.25013646483421326, &quot;time-step&quot;: 424}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2501204013824463, &quot;time-step&quot;: 425}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.25010472536087036, &quot;time-step&quot;: 426}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2500894367694855, &quot;time-step&quot;: 427}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.25007450580596924, &quot;time-step&quot;: 428}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2500598430633545, &quot;time-step&quot;: 429}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2500455975532532, &quot;time-step&quot;: 430}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.25003162026405334, &quot;time-step&quot;: 431}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2500178813934326, &quot;time-step&quot;: 432}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.25000444054603577, &quot;time-step&quot;: 433}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24999120831489563, &quot;time-step&quot;: 434}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2499781996011734, &quot;time-step&quot;: 435}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24996545910835266, &quot;time-step&quot;: 436}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24995288252830505, &quot;time-step&quot;: 437}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24994046986103058, &quot;time-step&quot;: 438}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24992819130420685, &quot;time-step&quot;: 439}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24991609156131744, &quot;time-step&quot;: 440}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24990414083003998, &quot;time-step&quot;: 441}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24989232420921326, &quot;time-step&quot;: 442}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2498805820941925, &quot;time-step&quot;: 443}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2498689591884613, &quot;time-step&quot;: 444}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24985739588737488, &quot;time-step&quot;: 445}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24984590709209442, &quot;time-step&quot;: 446}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24983450770378113, &quot;time-step&quot;: 447}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24982309341430664, &quot;time-step&quot;: 448}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2498117983341217, &quot;time-step&quot;: 449}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24980048835277557, &quot;time-step&quot;: 450}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24978920817375183, &quot;time-step&quot;: 451}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24977795779705048, &quot;time-step&quot;: 452}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24976672232151031, &quot;time-step&quot;: 453}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24975545704364777, &quot;time-step&quot;: 454}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2497442364692688, &quot;time-step&quot;: 455}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24973294138908386, &quot;time-step&quot;: 456}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2497216761112213, &quot;time-step&quot;: 457}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2497103363275528, &quot;time-step&quot;: 458}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24969902634620667, &quot;time-step&quot;: 459}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24968770146369934, &quot;time-step&quot;: 460}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24967628717422485, &quot;time-step&quot;: 461}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24966487288475037, &quot;time-step&quot;: 462}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2496534287929535, &quot;time-step&quot;: 463}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24964191019535065, &quot;time-step&quot;: 464}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24963034689426422, &quot;time-step&quot;: 465}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2496187686920166, &quot;time-step&quot;: 466}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.249607115983963, &quot;time-step&quot;: 467}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24959540367126465, &quot;time-step&quot;: 468}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24958372116088867, &quot;time-step&quot;: 469}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24957190454006195, &quot;time-step&quot;: 470}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24956010282039642, &quot;time-step&quot;: 471}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24954818189144135, &quot;time-step&quot;: 472}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24953627586364746, &quot;time-step&quot;: 473}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2495243102312088, &quot;time-step&quot;: 474}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24951225519180298, &quot;time-step&quot;: 475}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24950018525123596, &quot;time-step&quot;: 476}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24948804080486298, &quot;time-step&quot;: 477}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24947589635849, &quot;time-step&quot;: 478}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24946367740631104, &quot;time-step&quot;: 479}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2494513988494873, &quot;time-step&quot;: 480}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24943907558918, &quot;time-step&quot;: 481}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24942675232887268, &quot;time-step&quot;: 482}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2494143545627594, &quot;time-step&quot;: 483}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24940189719200134, &quot;time-step&quot;: 484}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2493894100189209, &quot;time-step&quot;: 485}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24937687814235687, &quot;time-step&quot;: 486}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24936431646347046, &quot;time-step&quot;: 487}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24935171008110046, &quot;time-step&quot;: 488}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24933910369873047, &quot;time-step&quot;: 489}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2493264228105545, &quot;time-step&quot;: 490}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24931375682353973, &quot;time-step&quot;: 491}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.249301016330719, &quot;time-step&quot;: 492}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24928824603557587, &quot;time-step&quot;: 493}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24927544593811035, &quot;time-step&quot;: 494}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24926261603832245, &quot;time-step&quot;: 495}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24924971163272858, &quot;time-step&quot;: 496}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24923686683177948, &quot;time-step&quot;: 497}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24922394752502441, &quot;time-step&quot;: 498}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24921099841594696, &quot;time-step&quot;: 499}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2491980344057083, &quot;time-step&quot;: 500}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24918505549430847, &quot;time-step&quot;: 501}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24917200207710266, &quot;time-step&quot;: 502}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24915897846221924, &quot;time-step&quot;: 503}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24914589524269104, &quot;time-step&quot;: 504}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24913281202316284, &quot;time-step&quot;: 505}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24911974370479584, &quot;time-step&quot;: 506}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24910661578178406, &quot;time-step&quot;: 507}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2490934580564499, &quot;time-step&quot;: 508}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24908031523227692, &quot;time-step&quot;: 509}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24906709790229797, &quot;time-step&quot;: 510}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24905389547348022, &quot;time-step&quot;: 511}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24904075264930725, &quot;time-step&quot;: 512}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24902749061584473, &quot;time-step&quot;: 513}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24901428818702698, &quot;time-step&quot;: 514}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24900104105472565, &quot;time-step&quot;: 515}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24898776412010193, &quot;time-step&quot;: 516}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24897445738315582, &quot;time-step&quot;: 517}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2489611804485321, &quot;time-step&quot;: 518}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2489478886127472, &quot;time-step&quot;: 519}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2489345669746399, &quot;time-step&quot;: 520}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2489212304353714, &quot;time-step&quot;: 521}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2489078789949417, &quot;time-step&quot;: 522}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24889454245567322, &quot;time-step&quot;: 523}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24888113141059875, &quot;time-step&quot;: 524}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24886777997016907, &quot;time-step&quot;: 525}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.248854398727417, &quot;time-step&quot;: 526}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24884100258350372, &quot;time-step&quot;: 527}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24882759153842926, &quot;time-step&quot;: 528}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2488141506910324, &quot;time-step&quot;: 529}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24880072474479675, &quot;time-step&quot;: 530}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2487873136997223, &quot;time-step&quot;: 531}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24877387285232544, &quot;time-step&quot;: 532}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2487604022026062, &quot;time-step&quot;: 533}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24874694645404816, &quot;time-step&quot;: 534}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24873346090316772, &quot;time-step&quot;: 535}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2487199902534485, &quot;time-step&quot;: 536}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24870644509792328, &quot;time-step&quot;: 537}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24869295954704285, &quot;time-step&quot;: 538}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24867947399616241, &quot;time-step&quot;: 539}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2486659586429596, &quot;time-step&quot;: 540}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2486523985862732, &quot;time-step&quot;: 541}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24863886833190918, &quot;time-step&quot;: 542}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24862529337406158, &quot;time-step&quot;: 543}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.248611718416214, &quot;time-step&quot;: 544}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2485981285572052, &quot;time-step&quot;: 545}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2485845386981964, &quot;time-step&quot;: 546}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24857090413570404, &quot;time-step&quot;: 547}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24855728447437286, &quot;time-step&quot;: 548}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24854367971420288, &quot;time-step&quot;: 549}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2485300600528717, &quot;time-step&quot;: 550}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24851632118225098, &quot;time-step&quot;: 551}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24850264191627502, &quot;time-step&quot;: 552}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24848894774913788, &quot;time-step&quot;: 553}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24847525358200073, &quot;time-step&quot;: 554}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2484615445137024, &quot;time-step&quot;: 555}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24844782054424286, &quot;time-step&quot;: 556}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24843400716781616, &quot;time-step&quot;: 557}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24842023849487305, &quot;time-step&quot;: 558}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24840641021728516, &quot;time-step&quot;: 559}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24839256703853607, &quot;time-step&quot;: 560}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24837875366210938, &quot;time-step&quot;: 561}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2483648657798767, &quot;time-step&quot;: 562}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24835094809532166, &quot;time-step&quot;: 563}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2483370304107666, &quot;time-step&quot;: 564}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24832311272621155, &quot;time-step&quot;: 565}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24830912053585052, &quot;time-step&quot;: 566}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24829509854316711, &quot;time-step&quot;: 567}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2482810765504837, &quot;time-step&quot;: 568}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24826699495315552, &quot;time-step&quot;: 569}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24825289845466614, &quot;time-step&quot;: 570}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24823877215385437, &quot;time-step&quot;: 571}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24822461605072021, &quot;time-step&quot;: 572}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2482103854417801, &quot;time-step&quot;: 573}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24819612503051758, &quot;time-step&quot;: 574}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24818181991577148, &quot;time-step&quot;: 575}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.248167484998703, &quot;time-step&quot;: 576}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24815309047698975, &quot;time-step&quot;: 577}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2481386810541153, &quot;time-step&quot;: 578}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24812419712543488, &quot;time-step&quot;: 579}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24810966849327087, &quot;time-step&quot;: 580}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2480950802564621, &quot;time-step&quot;: 581}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24808044731616974, &quot;time-step&quot;: 582}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2480657547712326, &quot;time-step&quot;: 583}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24805095791816711, &quot;time-step&quot;: 584}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24803617596626282, &quot;time-step&quot;: 585}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24802130460739136, &quot;time-step&quot;: 586}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24800634384155273, &quot;time-step&quot;: 587}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24799129366874695, &quot;time-step&quot;: 588}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24797624349594116, &quot;time-step&quot;: 589}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24796102941036224, &quot;time-step&quot;: 590}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24794578552246094, &quot;time-step&quot;: 591}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24793049693107605, &quot;time-step&quot;: 592}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24791507422924042, &quot;time-step&quot;: 593}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2478996366262436, &quot;time-step&quot;: 594}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24788400530815125, &quot;time-step&quot;: 595}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24786829948425293, &quot;time-step&quot;: 596}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24785259366035461, &quot;time-step&quot;: 597}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24783667922019958, &quot;time-step&quot;: 598}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24782072007656097, &quot;time-step&quot;: 599}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24780462682247162, &quot;time-step&quot;: 600}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2477884292602539, &quot;time-step&quot;: 601}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2477721869945526, &quot;time-step&quot;: 602}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2477557510137558, &quot;time-step&quot;: 603}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2477392554283142, &quot;time-step&quot;: 604}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2477225810289383, &quot;time-step&quot;: 605}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24770580232143402, &quot;time-step&quot;: 606}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.247688889503479, &quot;time-step&quot;: 607}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24767184257507324, &quot;time-step&quot;: 608}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24765467643737793, &quot;time-step&quot;: 609}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24763740599155426, &quot;time-step&quot;: 610}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2476198971271515, &quot;time-step&quot;: 611}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24760234355926514, &quot;time-step&quot;: 612}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24758456647396088, &quot;time-step&quot;: 613}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24756665527820587, &quot;time-step&quot;: 614}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24754858016967773, &quot;time-step&quot;: 615}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24753035604953766, &quot;time-step&quot;: 616}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24751195311546326, &quot;time-step&quot;: 617}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24749340116977692, &quot;time-step&quot;: 618}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24747467041015625, &quot;time-step&quot;: 619}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24745570123195648, &quot;time-step&quot;: 620}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2474365532398224, &quot;time-step&quot;: 621}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24741728603839874, &quot;time-step&quot;: 622}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.247397780418396, &quot;time-step&quot;: 623}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24737811088562012, &quot;time-step&quot;: 624}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24735818803310394, &quot;time-step&quot;: 625}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24733810126781464, &quot;time-step&quot;: 626}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24731776118278503, &quot;time-step&quot;: 627}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2472972273826599, &quot;time-step&quot;: 628}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24727647006511688, &quot;time-step&quot;: 629}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24725544452667236, &quot;time-step&quot;: 630}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2472342699766159, &quot;time-step&quot;: 631}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24721281230449677, &quot;time-step&quot;: 632}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24719110131263733, &quot;time-step&quot;: 633}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24716916680335999, &quot;time-step&quot;: 634}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24714696407318115, &quot;time-step&quot;: 635}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24712452292442322, &quot;time-step&quot;: 636}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24710184335708618, &quot;time-step&quot;: 637}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24707885086536407, &quot;time-step&quot;: 638}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24705559015274048, &quot;time-step&quot;: 639}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24703212082386017, &quot;time-step&quot;: 640}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2470083236694336, &quot;time-step&quot;: 641}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24698424339294434, &quot;time-step&quot;: 642}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24695982038974762, &quot;time-step&quot;: 643}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24693523347377777, &quot;time-step&quot;: 644}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24691027402877808, &quot;time-step&quot;: 645}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24688498675823212, &quot;time-step&quot;: 646}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24685943126678467, &quot;time-step&quot;: 647}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24683354794979095, &quot;time-step&quot;: 648}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2468073070049286, &quot;time-step&quot;: 649}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24678079783916473, &quot;time-step&quot;: 650}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24675396084785461, &quot;time-step&quot;: 651}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24672678112983704, &quot;time-step&quot;: 652}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2466992288827896, &quot;time-step&quot;: 653}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24667134881019592, &quot;time-step&quot;: 654}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24664315581321716, &quot;time-step&quot;: 655}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24661460518836975, &quot;time-step&quot;: 656}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2465856373310089, &quot;time-step&quot;: 657}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2465563416481018, &quot;time-step&quot;: 658}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24652668833732605, &quot;time-step&quot;: 659}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24649666249752045, &quot;time-step&quot;: 660}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24646621942520142, &quot;time-step&quot;: 661}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24643543362617493, &quot;time-step&quot;: 662}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.246404230594635, &quot;time-step&quot;: 663}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24637261033058167, &quot;time-step&quot;: 664}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24634063243865967, &quot;time-step&quot;: 665}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24630822241306305, &quot;time-step&quot;: 666}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2462754100561142, &quot;time-step&quot;: 667}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24624216556549072, &quot;time-step&quot;: 668}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24620850384235382, &quot;time-step&quot;: 669}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2461744099855423, &quot;time-step&quot;: 670}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24613985419273376, &quot;time-step&quot;: 671}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2461048662662506, &quot;time-step&quot;: 672}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24606946110725403, &quot;time-step&quot;: 673}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24603357911109924, &quot;time-step&quot;: 674}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24599725008010864, &quot;time-step&quot;: 675}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24596042931079865, &quot;time-step&quot;: 676}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24592314660549164, &quot;time-step&quot;: 677}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24588537216186523, &quot;time-step&quot;: 678}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2458471655845642, &quot;time-step&quot;: 679}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2458084225654602, &quot;time-step&quot;: 680}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2457691878080368, &quot;time-step&quot;: 681}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.245729461312294, &quot;time-step&quot;: 682}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24568922817707062, &quot;time-step&quot;: 683}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24564844369888306, &quot;time-step&quot;: 684}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2456071376800537, &quot;time-step&quot;: 685}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24556533992290497, &quot;time-step&quot;: 686}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24552297592163086, &quot;time-step&quot;: 687}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24548007547855377, &quot;time-step&quot;: 688}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2454366683959961, &quot;time-step&quot;: 689}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24539268016815186, &quot;time-step&quot;: 690}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24534806609153748, &quot;time-step&quot;: 691}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2453029751777649, &quot;time-step&quot;: 692}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2452571988105774, &quot;time-step&quot;: 693}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2452109307050705, &quot;time-step&quot;: 694}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24516408145427704, &quot;time-step&quot;: 695}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24511662125587463, &quot;time-step&quot;: 696}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24506855010986328, &quot;time-step&quot;: 697}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2450198531150818, &quot;time-step&quot;: 698}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24497054517269135, &quot;time-step&quot;: 699}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24492061138153076, &quot;time-step&quot;: 700}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24487000703811646, &quot;time-step&quot;: 701}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24481883645057678, &quot;time-step&quot;: 702}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2447669804096222, &quot;time-step&quot;: 703}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2447144091129303, &quot;time-step&quot;: 704}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24466127157211304, &quot;time-step&quot;: 705}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24460746347904205, &quot;time-step&quot;: 706}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24455291032791138, &quot;time-step&quot;: 707}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24449771642684937, &quot;time-step&quot;: 708}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24444180727005005, &quot;time-step&quot;: 709}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24438521265983582, &quot;time-step&quot;: 710}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24432790279388428, &quot;time-step&quot;: 711}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24426990747451782, &quot;time-step&quot;: 712}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24421116709709167, &quot;time-step&quot;: 713}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24415165185928345, &quot;time-step&quot;: 714}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2440914511680603, &quot;time-step&quot;: 715}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2440304458141327, &quot;time-step&quot;: 716}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24396872520446777, &quot;time-step&quot;: 717}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24390622973442078, &quot;time-step&quot;: 718}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2438429743051529, &quot;time-step&quot;: 719}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24377889931201935, &quot;time-step&quot;: 720}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2437140941619873, &quot;time-step&quot;: 721}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2436484396457672, &quot;time-step&quot;: 722}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24358198046684265, &quot;time-step&quot;: 723}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24351468682289124, &quot;time-step&quot;: 724}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24344658851623535, &quot;time-step&quot;: 725}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24337764084339142, &quot;time-step&quot;: 726}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24330788850784302, &quot;time-step&quot;: 727}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2432372123003006, &quot;time-step&quot;: 728}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2431657463312149, &quot;time-step&quot;: 729}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.243093341588974, &quot;time-step&quot;: 730}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24302008748054504, &quot;time-step&quot;: 731}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24294589459896088, &quot;time-step&quot;: 732}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24287086725234985, &quot;time-step&quot;: 733}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24279487133026123, &quot;time-step&quot;: 734}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24271798133850098, &quot;time-step&quot;: 735}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2426401525735855, &quot;time-step&quot;: 736}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24256131052970886, &quot;time-step&quot;: 737}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24248160421848297, &quot;time-step&quot;: 738}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24240082502365112, &quot;time-step&quot;: 739}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24231918156147003, &quot;time-step&quot;: 740}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24223646521568298, &quot;time-step&quot;: 741}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24215275049209595, &quot;time-step&quot;: 742}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24206799268722534, &quot;time-step&quot;: 743}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24198225140571594, &quot;time-step&quot;: 744}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24189543724060059, &quot;time-step&quot;: 745}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24180755019187927, &quot;time-step&quot;: 746}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2417186200618744, &quot;time-step&quot;: 747}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24162855744361877, &quot;time-step&quot;: 748}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2415374219417572, &quot;time-step&quot;: 749}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2414451390504837, &quot;time-step&quot;: 750}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2413516789674759, &quot;time-step&quot;: 751}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24125711619853973, &quot;time-step&quot;: 752}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24116136133670807, &quot;time-step&quot;: 753}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2410643994808197, &quot;time-step&quot;: 754}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24096623063087463, &quot;time-step&quot;: 755}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24086683988571167, &quot;time-step&quot;: 756}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24076618254184723, &quot;time-step&quot;: 757}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24066424369812012, &quot;time-step&quot;: 758}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24056096374988556, &quot;time-step&quot;: 759}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24045640230178833, &quot;time-step&quot;: 760}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24035045504570007, &quot;time-step&quot;: 761}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24024319648742676, &quot;time-step&quot;: 762}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24013447761535645, &quot;time-step&quot;: 763}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.24002432823181152, &quot;time-step&quot;: 764}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23991268873214722, &quot;time-step&quot;: 765}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23979957401752472, &quot;time-step&quot;: 766}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23968487977981567, &quot;time-step&quot;: 767}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23956863582134247, &quot;time-step&quot;: 768}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23945075273513794, &quot;time-step&quot;: 769}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23933126032352448, &quot;time-step&quot;: 770}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23921000957489014, &quot;time-step&quot;: 771}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2390870302915573, &quot;time-step&quot;: 772}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2389622926712036, &quot;time-step&quot;: 773}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2388356477022171, &quot;time-step&quot;: 774}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23870709538459778, &quot;time-step&quot;: 775}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23857663571834564, &quot;time-step&quot;: 776}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23844406008720398, &quot;time-step&quot;: 777}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23830941319465637, &quot;time-step&quot;: 778}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23817265033721924, &quot;time-step&quot;: 779}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23803357779979706, &quot;time-step&quot;: 780}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23789218068122864, &quot;time-step&quot;: 781}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23774844408035278, &quot;time-step&quot;: 782}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.237602099776268, &quot;time-step&quot;: 783}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23745325207710266, &quot;time-step&quot;: 784}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23730166256427765, &quot;time-step&quot;: 785}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.237147256731987, &quot;time-step&quot;: 786}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23698991537094116, &quot;time-step&quot;: 787}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23682954907417297, &quot;time-step&quot;: 788}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2366659939289093, &quot;time-step&quot;: 789}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2364991009235382, &quot;time-step&quot;: 790}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23632881045341492, &quot;time-step&quot;: 791}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23615488409996033, &quot;time-step&quot;: 792}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2359772026538849, &quot;time-step&quot;: 793}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23579555749893188, &quot;time-step&quot;: 794}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23560982942581177, &quot;time-step&quot;: 795}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23541975021362305, &quot;time-step&quot;: 796}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23522521555423737, &quot;time-step&quot;: 797}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23502600193023682, &quot;time-step&quot;: 798}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23482179641723633, &quot;time-step&quot;: 799}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23461253941059113, &quot;time-step&quot;: 800}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23439787328243256, &quot;time-step&quot;: 801}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23417752981185913, &quot;time-step&quot;: 802}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2339514046907425, &quot;time-step&quot;: 803}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23371915519237518, &quot;time-step&quot;: 804}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23348048329353333, &quot;time-step&quot;: 805}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23323510587215424, &quot;time-step&quot;: 806}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23298276960849762, &quot;time-step&quot;: 807}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23272311687469482, &quot;time-step&quot;: 808}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23245592415332794, &quot;time-step&quot;: 809}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23218081891536713, &quot;time-step&quot;: 810}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2318975031375885, &quot;time-step&quot;: 811}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2316056489944458, &quot;time-step&quot;: 812}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23130486905574799, &quot;time-step&quot;: 813}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23099485039710999, &quot;time-step&quot;: 814}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2306753247976303, &quot;time-step&quot;: 815}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23034575581550598, &quot;time-step&quot;: 816}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.23000594973564148, &quot;time-step&quot;: 817}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.22965547442436218, &quot;time-step&quot;: 818}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2292940318584442, &quot;time-step&quot;: 819}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2289211004972458, &quot;time-step&quot;: 820}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2285364717245102, &quot;time-step&quot;: 821}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.22813966870307922, &quot;time-step&quot;: 822}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.22773034870624542, &quot;time-step&quot;: 823}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.22730816900730133, &quot;time-step&quot;: 824}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.22687269747257233, &quot;time-step&quot;: 825}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.22642365097999573, &quot;time-step&quot;: 826}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.22596056759357452, &quot;time-step&quot;: 827}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.22548316419124603, &quot;time-step&quot;: 828}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.22499100863933563, &quot;time-step&quot;: 829}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.22448375821113586, &quot;time-step&quot;: 830}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.22396109998226166, &quot;time-step&quot;: 831}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.22342267632484436, &quot;time-step&quot;: 832}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.22286808490753174, &quot;time-step&quot;: 833}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2222970724105835, &quot;time-step&quot;: 834}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.22170940041542053, &quot;time-step&quot;: 835}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.22110462188720703, &quot;time-step&quot;: 836}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2204824686050415, &quot;time-step&quot;: 837}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.2198428213596344, &quot;time-step&quot;: 838}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.21918538212776184, &quot;time-step&quot;: 839}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.21850985288619995, &quot;time-step&quot;: 840}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.21781617403030396, &quot;time-step&quot;: 841}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.21710409224033356, &quot;time-step&quot;: 842}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.21637342870235443, &quot;time-step&quot;: 843}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.21562430262565613, &quot;time-step&quot;: 844}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.21485650539398193, &quot;time-step&quot;: 845}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.21407000720500946, &quot;time-step&quot;: 846}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.21326494216918945, &quot;time-step&quot;: 847}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.21244125068187714, &quot;time-step&quot;: 848}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.21159915626049042, &quot;time-step&quot;: 849}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.21073879301548004, &quot;time-step&quot;: 850}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.20986036956310272, &quot;time-step&quot;: 851}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.20896410942077637, &quot;time-step&quot;: 852}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.2080504149198532, &quot;time-step&quot;: 853}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.20711955428123474, &quot;time-step&quot;: 854}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.20617210865020752, &quot;time-step&quot;: 855}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.20520833134651184, &quot;time-step&quot;: 856}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.20422884821891785, &quot;time-step&quot;: 857}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.20323427021503448, &quot;time-step&quot;: 858}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.20222511887550354, &quot;time-step&quot;: 859}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.20120209455490112, &quot;time-step&quot;: 860}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.20016583800315857, &quot;time-step&quot;: 861}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.19911719858646393, &quot;time-step&quot;: 862}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.19805675745010376, &quot;time-step&quot;: 863}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.19698548316955566, &quot;time-step&quot;: 864}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.19590400159358978, &quot;time-step&quot;: 865}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.19481319189071655, &quot;time-step&quot;: 866}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.19371400773525238, &quot;time-step&quot;: 867}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.19260713458061218, &quot;time-step&quot;: 868}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.1914934515953064, &quot;time-step&quot;: 869}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.19037394225597382, &quot;time-step&quot;: 870}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.18924929201602936, &quot;time-step&quot;: 871}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.18812039494514465, &quot;time-step&quot;: 872}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.1869879961013794, &quot;time-step&quot;: 873}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.18585297465324402, &quot;time-step&quot;: 874}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.184716135263443, &quot;time-step&quot;: 875}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.18357811868190765, &quot;time-step&quot;: 876}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.18243971467018127, &quot;time-step&quot;: 877}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.1813015639781952, &quot;time-step&quot;: 878}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.18016430735588074, &quot;time-step&quot;: 879}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.17902854084968567, &quot;time-step&quot;: 880}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.17789486050605774, &quot;time-step&quot;: 881}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.1767638921737671, &quot;time-step&quot;: 882}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.1756359040737152, &quot;time-step&quot;: 883}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.1745116114616394, &quot;time-step&quot;: 884}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.173391193151474, &quot;time-step&quot;: 885}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.17227506637573242, &quot;time-step&quot;: 886}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.1711636334657669, &quot;time-step&quot;: 887}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.17005710303783417, &quot;time-step&quot;: 888}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.16895562410354614, &quot;time-step&quot;: 889}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.16785959899425507, &quot;time-step&quot;: 890}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.1667691171169281, &quot;time-step&quot;: 891}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.16568425297737122, &quot;time-step&quot;: 892}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.16460508108139038, &quot;time-step&quot;: 893}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.16353166103363037, &quot;time-step&quot;: 894}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.16246408224105835, &quot;time-step&quot;: 895}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.16140228509902954, &quot;time-step&quot;: 896}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.16034623980522156, &quot;time-step&quot;: 897}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.15929582715034485, &quot;time-step&quot;: 898}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.15825104713439941, &quot;time-step&quot;: 899}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.15721166133880615, &quot;time-step&quot;: 900}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.15617774426937103, &quot;time-step&quot;: 901}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.1551489531993866, &quot;time-step&quot;: 902}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.15412506461143494, &quot;time-step&quot;: 903}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.15310604870319366, &quot;time-step&quot;: 904}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.15209168195724487, &quot;time-step&quot;: 905}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.1510816365480423, &quot;time-step&quot;: 906}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.15007568895816803, &quot;time-step&quot;: 907}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.14907363057136536, &quot;time-step&quot;: 908}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.1480751931667328, &quot;time-step&quot;: 909}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.1470799744129181, &quot;time-step&quot;: 910}, {&quot;accuracy&quot;: 0.5, &quot;loss&quot;: 0.1460878700017929, &quot;time-step&quot;: 911}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.145098477602005, &quot;time-step&quot;: 912}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.14411142468452454, &quot;time-step&quot;: 913}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.14312644302845, &quot;time-step&quot;: 914}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.14214323461055756, &quot;time-step&quot;: 915}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.14116138219833374, &quot;time-step&quot;: 916}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.1401805430650711, &quot;time-step&quot;: 917}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.13920041918754578, &quot;time-step&quot;: 918}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.13822059333324432, &quot;time-step&quot;: 919}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.1372406929731369, &quot;time-step&quot;: 920}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.13626039028167725, &quot;time-step&quot;: 921}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.13527914881706238, &quot;time-step&quot;: 922}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.13429684937000275, &quot;time-step&quot;: 923}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.1333128660917282, &quot;time-step&quot;: 924}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.1323268711566925, &quot;time-step&quot;: 925}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.131338432431221, &quot;time-step&quot;: 926}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.13034725189208984, &quot;time-step&quot;: 927}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.12935280799865723, &quot;time-step&quot;: 928}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.1283547580242157, &quot;time-step&quot;: 929}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.12735262513160706, &quot;time-step&quot;: 930}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.12634606659412384, &quot;time-step&quot;: 931}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.125334694981575, &quot;time-step&quot;: 932}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.12431798875331879, &quot;time-step&quot;: 933}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.1232956275343895, &quot;time-step&quot;: 934}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.12226720154285431, &quot;time-step&quot;: 935}, {&quot;accuracy&quot;: 0.75, &quot;loss&quot;: 0.12123221158981323, &quot;time-step&quot;: 936}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.12019031494855881, &quot;time-step&quot;: 937}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.1191411241889, &quot;time-step&quot;: 938}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.11808420717716217, &quot;time-step&quot;: 939}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.11701927334070206, &quot;time-step&quot;: 940}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.11594580858945847, &quot;time-step&quot;: 941}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.11486349999904633, &quot;time-step&quot;: 942}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.11377200484275818, &quot;time-step&quot;: 943}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.11267095804214478, &quot;time-step&quot;: 944}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.11155995726585388, &quot;time-step&quot;: 945}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.11043877899646759, &quot;time-step&quot;: 946}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.10930699110031128, &quot;time-step&quot;: 947}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.10816442966461182, &quot;time-step&quot;: 948}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.1070108413696289, &quot;time-step&quot;: 949}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.10584581643342972, &quot;time-step&quot;: 950}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.10466928035020828, &quot;time-step&quot;: 951}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.1034809798002243, &quot;time-step&quot;: 952}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.10228095948696136, &quot;time-step&quot;: 953}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.10106883198022842, &quot;time-step&quot;: 954}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.09984508156776428, &quot;time-step&quot;: 955}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.09860888123512268, &quot;time-step&quot;: 956}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.09736178815364838, &quot;time-step&quot;: 957}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.09610174596309662, &quot;time-step&quot;: 958}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.09483177959918976, &quot;time-step&quot;: 959}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.09354835748672485, &quot;time-step&quot;: 960}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.09225624054670334, &quot;time-step&quot;: 961}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.09095024317502975, &quot;time-step&quot;: 962}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.08963748067617416, &quot;time-step&quot;: 963}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.08831043541431427, &quot;time-step&quot;: 964}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.08697907626628876, &quot;time-step&quot;: 965}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.08563310652971268, &quot;time-step&quot;: 966}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.08428596705198288, &quot;time-step&quot;: 967}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0829240083694458, &quot;time-step&quot;: 968}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.08156482875347137, &quot;time-step&quot;: 969}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.08019039034843445, &quot;time-step&quot;: 970}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.07882388681173325, &quot;time-step&quot;: 971}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.07744128257036209, &quot;time-step&quot;: 972}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.07607310265302658, &quot;time-step&quot;: 973}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.07468760013580322, &quot;time-step&quot;: 974}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.07332432270050049, &quot;time-step&quot;: 975}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.07194184511899948, &quot;time-step&quot;: 976}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.07059114426374435, &quot;time-step&quot;: 977}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0692184567451477, &quot;time-step&quot;: 978}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.06788836419582367, &quot;time-step&quot;: 979}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.06653304398059845, &quot;time-step&quot;: 980}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.06523188948631287, &quot;time-step&quot;: 981}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.06390216946601868, &quot;time-step&quot;: 982}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.06263817101716995, &quot;time-step&quot;: 983}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.061343226581811905, &quot;time-step&quot;: 984}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.060123734176158905, &quot;time-step&quot;: 985}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.05887332558631897, &quot;time-step&quot;: 986}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.05770482122898102, &quot;time-step&quot;: 987}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.05650946497917175, &quot;time-step&quot;: 988}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.05539662390947342, &quot;time-step&quot;: 989}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.05426725745201111, &quot;time-step&quot;: 990}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.053213030099868774, &quot;time-step&quot;: 991}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.05216085910797119, &quot;time-step&quot;: 992}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.051165621727705, &quot;time-step&quot;: 993}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.05020182579755783, &quot;time-step&quot;: 994}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.049263034015893936, &quot;time-step&quot;: 995}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.048398569226264954, &quot;time-step&quot;: 996}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.04751051962375641, &quot;time-step&quot;: 997}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.04675581306219101, &quot;time-step&quot;: 998}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.045909978449344635, &quot;time-step&quot;: 999}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.04527479410171509, &quot;time-step&quot;: 1000}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.044459693133831024, &quot;time-step&quot;: 1001}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.04395335912704468, &quot;time-step&quot;: 1002}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.04315529018640518, &quot;time-step&quot;: 1003}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.04278681427240372, &quot;time-step&quot;: 1004}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.041989970952272415, &quot;time-step&quot;: 1005}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.04176823049783707, &quot;time-step&quot;: 1006}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.04095545411109924, &quot;time-step&quot;: 1007}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.04088909551501274, &quot;time-step&quot;: 1008}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.04004218056797981, &quot;time-step&quot;: 1009}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.040140021592378616, &quot;time-step&quot;: 1010}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0392397977411747, &quot;time-step&quot;: 1011}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.039510201662778854, &quot;time-step&quot;: 1012}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03853726014494896, &quot;time-step&quot;: 1013}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.038988277316093445, &quot;time-step&quot;: 1014}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.037923093885183334, &quot;time-step&quot;: 1015}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03856203705072403, &quot;time-step&quot;: 1016}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03738584369421005, &quot;time-step&quot;: 1017}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03821903094649315, &quot;time-step&quot;: 1018}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03691423684358597, &quot;time-step&quot;: 1019}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03794679045677185, &quot;time-step&quot;: 1020}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.036497704684734344, &quot;time-step&quot;: 1021}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03773330897092819, &quot;time-step&quot;: 1022}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03612663969397545, &quot;time-step&quot;: 1023}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03756755590438843, &quot;time-step&quot;: 1024}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.035792432725429535, &quot;time-step&quot;: 1025}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03743947669863701, &quot;time-step&quot;: 1026}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03548794984817505, &quot;time-step&quot;: 1027}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03734058886766434, &quot;time-step&quot;: 1028}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03520739823579788, &quot;time-step&quot;: 1029}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.037263937294483185, &quot;time-step&quot;: 1030}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03494612127542496, &quot;time-step&quot;: 1031}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.037203676998615265, &quot;time-step&quot;: 1032}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.034700553864240646, &quot;time-step&quot;: 1033}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03715544566512108, &quot;time-step&quot;: 1034}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0344681553542614, &quot;time-step&quot;: 1035}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03711593151092529, &quot;time-step&quot;: 1036}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03424680978059769, &quot;time-step&quot;: 1037}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03708237037062645, &quot;time-step&quot;: 1038}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03403501585125923, &quot;time-step&quot;: 1039}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.037052784115076065, &quot;time-step&quot;: 1040}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.033831723034381866, &quot;time-step&quot;: 1041}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.037025921046733856, &quot;time-step&quot;: 1042}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03363615646958351, &quot;time-step&quot;: 1043}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03700060024857521, &quot;time-step&quot;: 1044}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03344758227467537, &quot;time-step&quot;: 1045}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03697604686021805, &quot;time-step&quot;: 1046}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03326553851366043, &quot;time-step&quot;: 1047}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03695162013173103, &quot;time-step&quot;: 1048}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03308945521712303, &quot;time-step&quot;: 1049}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03692666441202164, &quot;time-step&quot;: 1050}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03291904181241989, &quot;time-step&quot;: 1051}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03690089285373688, &quot;time-step&quot;: 1052}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03275385871529579, &quot;time-step&quot;: 1053}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03687392920255661, &quot;time-step&quot;: 1054}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.032593630254268646, &quot;time-step&quot;: 1055}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03684549778699875, &quot;time-step&quot;: 1056}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03243802860379219, &quot;time-step&quot;: 1057}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03681543469429016, &quot;time-step&quot;: 1058}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03228673338890076, &quot;time-step&quot;: 1059}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.036783378571271896, &quot;time-step&quot;: 1060}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03213958442211151, &quot;time-step&quot;: 1061}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03674937039613724, &quot;time-step&quot;: 1062}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.031996242702007294, &quot;time-step&quot;: 1063}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.036713212728500366, &quot;time-step&quot;: 1064}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03185631334781647, &quot;time-step&quot;: 1065}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03667450323700905, &quot;time-step&quot;: 1066}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.031719692051410675, &quot;time-step&quot;: 1067}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03663345053792, &quot;time-step&quot;: 1068}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03158624470233917, &quot;time-step&quot;: 1069}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03658987209200859, &quot;time-step&quot;: 1070}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.031455401331186295, &quot;time-step&quot;: 1071}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03654346615076065, &quot;time-step&quot;: 1072}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03132728487253189, &quot;time-step&quot;: 1073}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03649444133043289, &quot;time-step&quot;: 1074}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03120161034166813, &quot;time-step&quot;: 1075}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03644275292754173, &quot;time-step&quot;: 1076}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.031078152358531952, &quot;time-step&quot;: 1077}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03638822212815285, &quot;time-step&quot;: 1078}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.030956782400608063, &quot;time-step&quot;: 1079}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03633086010813713, &quot;time-step&quot;: 1080}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.030837232246994972, &quot;time-step&quot;: 1081}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03627074509859085, &quot;time-step&quot;: 1082}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.030719567090272903, &quot;time-step&quot;: 1083}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.036207862198352814, &quot;time-step&quot;: 1084}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03060349076986313, &quot;time-step&quot;: 1085}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.036142174154520035, &quot;time-step&quot;: 1086}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.030488822609186172, &quot;time-step&quot;: 1087}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.036073651164770126, &quot;time-step&quot;: 1088}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03037554956972599, &quot;time-step&quot;: 1089}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03600246086716652, &quot;time-step&quot;: 1090}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.030263591557741165, &quot;time-step&quot;: 1091}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03592861443758011, &quot;time-step&quot;: 1092}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0301528237760067, &quot;time-step&quot;: 1093}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.035852156579494476, &quot;time-step&quot;: 1094}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.030043065547943115, &quot;time-step&quot;: 1095}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03577306494116783, &quot;time-step&quot;: 1096}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.029934337362647057, &quot;time-step&quot;: 1097}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03569144010543823, &quot;time-step&quot;: 1098}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02982647903263569, &quot;time-step&quot;: 1099}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.035607367753982544, &quot;time-step&quot;: 1100}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.029719457030296326, &quot;time-step&quot;: 1101}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.035520877689123154, &quot;time-step&quot;: 1102}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.029613249003887177, &quot;time-step&quot;: 1103}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.035432081669569016, &quot;time-step&quot;: 1104}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.029507696628570557, &quot;time-step&quot;: 1105}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.035340916365385056, &quot;time-step&quot;: 1106}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.029402881860733032, &quot;time-step&quot;: 1107}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03524773567914963, &quot;time-step&quot;: 1108}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.029298575595021248, &quot;time-step&quot;: 1109}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03515227511525154, &quot;time-step&quot;: 1110}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.029194947332143784, &quot;time-step&quot;: 1111}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03505488112568855, &quot;time-step&quot;: 1112}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02909175492823124, &quot;time-step&quot;: 1113}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0349554643034935, &quot;time-step&quot;: 1114}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.028989091515541077, &quot;time-step&quot;: 1115}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03485419601202011, &quot;time-step&quot;: 1116}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02888689935207367, &quot;time-step&quot;: 1117}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03475112095475197, &quot;time-step&quot;: 1118}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.028785187751054764, &quot;time-step&quot;: 1119}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03464648127555847, &quot;time-step&quot;: 1120}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.028683871030807495, &quot;time-step&quot;: 1121}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.034540124237537384, &quot;time-step&quot;: 1122}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.028582941740751266, &quot;time-step&quot;: 1123}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03443213924765587, &quot;time-step&quot;: 1124}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02848239243030548, &quot;time-step&quot;: 1125}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.034322772175073624, &quot;time-step&quot;: 1126}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.028382154181599617, &quot;time-step&quot;: 1127}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03421185910701752, &quot;time-step&quot;: 1128}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.028282269835472107, &quot;time-step&quot;: 1129}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03409970551729202, &quot;time-step&quot;: 1130}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02818278782069683, &quot;time-step&quot;: 1131}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03398643061518669, &quot;time-step&quot;: 1132}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02808353491127491, &quot;time-step&quot;: 1133}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03387177363038063, &quot;time-step&quot;: 1134}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02798452042043209, &quot;time-step&quot;: 1135}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03375602513551712, &quot;time-step&quot;: 1136}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02788597159087658, &quot;time-step&quot;: 1137}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03363935276865959, &quot;time-step&quot;: 1138}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.027787627652287483, &quot;time-step&quot;: 1139}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0335216261446476, &quot;time-step&quot;: 1140}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.027689501643180847, &quot;time-step&quot;: 1141}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03340292349457741, &quot;time-step&quot;: 1142}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0275917649269104, &quot;time-step&quot;: 1143}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0332835428416729, &quot;time-step&quot;: 1144}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02749428153038025, &quot;time-step&quot;: 1145}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03316320851445198, &quot;time-step&quot;: 1146}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.027396971359848976, &quot;time-step&quot;: 1147}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03304218128323555, &quot;time-step&quot;: 1148}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.027299996465444565, &quot;time-step&quot;: 1149}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03292052447795868, &quot;time-step&quot;: 1150}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02720329537987709, &quot;time-step&quot;: 1151}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.032798297703266144, &quot;time-step&quot;: 1152}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02710685133934021, &quot;time-step&quot;: 1153}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.032675400376319885, &quot;time-step&quot;: 1154}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.027010556310415268, &quot;time-step&quot;: 1155}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03255201131105423, &quot;time-step&quot;: 1156}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.026914719492197037, &quot;time-step&quot;: 1157}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0324283093214035, &quot;time-step&quot;: 1158}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.026819050312042236, &quot;time-step&quot;: 1159}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.032304052263498306, &quot;time-step&quot;: 1160}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02672361582517624, &quot;time-step&quot;: 1161}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0321795716881752, &quot;time-step&quot;: 1162}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02662854827940464, &quot;time-step&quot;: 1163}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.032054681330919266, &quot;time-step&quot;: 1164}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0265335850417614, &quot;time-step&quot;: 1165}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03192945569753647, &quot;time-step&quot;: 1166}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02643892914056778, &quot;time-step&quot;: 1167}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03180404379963875, &quot;time-step&quot;: 1168}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.026344604790210724, &quot;time-step&quot;: 1169}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03167841210961342, &quot;time-step&quot;: 1170}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.026250487193465233, &quot;time-step&quot;: 1171}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03155267983675003, &quot;time-step&quot;: 1172}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.026156682521104813, &quot;time-step&quot;: 1173}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0314268134534359, &quot;time-step&quot;: 1174}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.026063062250614166, &quot;time-step&quot;: 1175}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03130084276199341, &quot;time-step&quot;: 1176}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.025969862937927246, &quot;time-step&quot;: 1177}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.031174909323453903, &quot;time-step&quot;: 1178}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.025876760482788086, &quot;time-step&quot;: 1179}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.031048765406012535, &quot;time-step&quot;: 1180}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02578401193022728, &quot;time-step&quot;: 1181}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.030922815203666687, &quot;time-step&quot;: 1182}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.025691594928503036, &quot;time-step&quot;: 1183}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03079686313867569, &quot;time-step&quot;: 1184}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.025599345564842224, &quot;time-step&quot;: 1185}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.030670901760458946, &quot;time-step&quot;: 1186}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.025507444515824318, &quot;time-step&quot;: 1187}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.030545057728886604, &quot;time-step&quot;: 1188}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.025415819138288498, &quot;time-step&quot;: 1189}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03041943907737732, &quot;time-step&quot;: 1190}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.025324515998363495, &quot;time-step&quot;: 1191}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.030294014140963554, &quot;time-step&quot;: 1192}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.025233451277017593, &quot;time-step&quot;: 1193}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.03016853518784046, &quot;time-step&quot;: 1194}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.025142621248960495, &quot;time-step&quot;: 1195}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.030043330043554306, &quot;time-step&quot;: 1196}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.025052104145288467, &quot;time-step&quot;: 1197}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02991832047700882, &quot;time-step&quot;: 1198}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02496197074651718, &quot;time-step&quot;: 1199}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02979363687336445, &quot;time-step&quot;: 1200}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.024872014299035072, &quot;time-step&quot;: 1201}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02966904640197754, &quot;time-step&quot;: 1202}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02478235960006714, &quot;time-step&quot;: 1203}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.029544804245233536, &quot;time-step&quot;: 1204}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.024693049490451813, &quot;time-step&quot;: 1205}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02942080795764923, &quot;time-step&quot;: 1206}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02460402250289917, &quot;time-step&quot;: 1207}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.029297107830643654, &quot;time-step&quot;: 1208}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.024515211582183838, &quot;time-step&quot;: 1209}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.029173707589507103, &quot;time-step&quot;: 1210}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.024426830932497978, &quot;time-step&quot;: 1211}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02905074506998062, &quot;time-step&quot;: 1212}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02433866262435913, &quot;time-step&quot;: 1213}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02892804704606533, &quot;time-step&quot;: 1214}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02425079233944416, &quot;time-step&quot;: 1215}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.028805628418922424, &quot;time-step&quot;: 1216}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02416321076452732, &quot;time-step&quot;: 1217}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.028683632612228394, &quot;time-step&quot;: 1218}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02407592162489891, &quot;time-step&quot;: 1219}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.028561905026435852, &quot;time-step&quot;: 1220}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.023988991975784302, &quot;time-step&quot;: 1221}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0284406878054142, &quot;time-step&quot;: 1222}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02390229143202305, &quot;time-step&quot;: 1223}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.028319815173745155, &quot;time-step&quot;: 1224}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.023815933614969254, &quot;time-step&quot;: 1225}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02819938212633133, &quot;time-step&quot;: 1226}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.023729931563138962, &quot;time-step&quot;: 1227}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02807939052581787, &quot;time-step&quot;: 1228}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02364415116608143, &quot;time-step&quot;: 1229}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02795969322323799, &quot;time-step&quot;: 1230}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.023558666929602623, &quot;time-step&quot;: 1231}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.027840469032526016, &quot;time-step&quot;: 1232}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.023473519831895828, &quot;time-step&quot;: 1233}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.027721691876649857, &quot;time-step&quot;: 1234}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02338869869709015, &quot;time-step&quot;: 1235}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.027603330090641975, &quot;time-step&quot;: 1236}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.023304123431444168, &quot;time-step&quot;: 1237}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.027485491707921028, &quot;time-step&quot;: 1238}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.023219965398311615, &quot;time-step&quot;: 1239}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.027368148788809776, &quot;time-step&quot;: 1240}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.023136012256145477, &quot;time-step&quot;: 1241}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.027251116931438446, &quot;time-step&quot;: 1242}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02305237017571926, &quot;time-step&quot;: 1243}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02713456004858017, &quot;time-step&quot;: 1244}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.022968990728259087, &quot;time-step&quot;: 1245}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.027018416672945023, &quot;time-step&quot;: 1246}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.022885965183377266, &quot;time-step&quot;: 1247}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.026902811601758003, &quot;time-step&quot;: 1248}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.022803233936429024, &quot;time-step&quot;: 1249}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02678765542805195, &quot;time-step&quot;: 1250}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.022720780223608017, &quot;time-step&quot;: 1251}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.026673024520277977, &quot;time-step&quot;: 1252}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02263864502310753, &quot;time-step&quot;: 1253}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.026558753103017807, &quot;time-step&quot;: 1254}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.022556805983185768, &quot;time-step&quot;: 1255}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.026445042341947556, &quot;time-step&quot;: 1256}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02247520349919796, &quot;time-step&quot;: 1257}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.026331709697842598, &quot;time-step&quot;: 1258}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02239396423101425, &quot;time-step&quot;: 1259}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02621900849044323, &quot;time-step&quot;: 1260}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02231311984360218, &quot;time-step&quot;: 1261}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.026106836274266243, &quot;time-step&quot;: 1262}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.022232426330447197, &quot;time-step&quot;: 1263}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.025994978845119476, &quot;time-step&quot;: 1264}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02215208113193512, &quot;time-step&quot;: 1265}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02588372863829136, &quot;time-step&quot;: 1266}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0220720823854208, &quot;time-step&quot;: 1267}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02577297016978264, &quot;time-step&quot;: 1268}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02199234440922737, &quot;time-step&quot;: 1269}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.025662608444690704, &quot;time-step&quot;: 1270}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0219128355383873, &quot;time-step&quot;: 1271}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.025552699342370033, &quot;time-step&quot;: 1272}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.021833665668964386, &quot;time-step&quot;: 1273}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02544335275888443, &quot;time-step&quot;: 1274}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.021754838526248932, &quot;time-step&quot;: 1275}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.025334561243653297, &quot;time-step&quot;: 1276}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.021676240488886833, &quot;time-step&quot;: 1277}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02522621676325798, &quot;time-step&quot;: 1278}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.021598005667328835, &quot;time-step&quot;: 1279}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.025118328630924225, &quot;time-step&quot;: 1280}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.021519968286156654, &quot;time-step&quot;: 1281}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.025010891258716583, &quot;time-step&quot;: 1282}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.021442223340272903, &quot;time-step&quot;: 1283}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.024903949350118637, &quot;time-step&quot;: 1284}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.021364856511354446, &quot;time-step&quot;: 1285}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02479763701558113, &quot;time-step&quot;: 1286}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02128777466714382, &quot;time-step&quot;: 1287}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02469174563884735, &quot;time-step&quot;: 1288}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.021210884675383568, &quot;time-step&quot;: 1289}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02458622306585312, &quot;time-step&quot;: 1290}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02113434299826622, &quot;time-step&quot;: 1291}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.024481285363435745, &quot;time-step&quot;: 1292}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02105802670121193, &quot;time-step&quot;: 1293}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02437678724527359, &quot;time-step&quot;: 1294}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.020982027053833008, &quot;time-step&quot;: 1295}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02427286095917225, &quot;time-step&quot;: 1296}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.020906398072838783, &quot;time-step&quot;: 1297}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02416939288377762, &quot;time-step&quot;: 1298}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02083096280694008, &quot;time-step&quot;: 1299}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.024066399782896042, &quot;time-step&quot;: 1300}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.020755857229232788, &quot;time-step&quot;: 1301}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02396385185420513, &quot;time-step&quot;: 1302}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02068093605339527, &quot;time-step&quot;: 1303}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.023861749097704887, &quot;time-step&quot;: 1304}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0206063874065876, &quot;time-step&quot;: 1305}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.023760203272104263, &quot;time-step&quot;: 1306}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.020532039925456047, &quot;time-step&quot;: 1307}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.023659072816371918, &quot;time-step&quot;: 1308}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02045804262161255, &quot;time-step&quot;: 1309}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02355855144560337, &quot;time-step&quot;: 1310}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02038433961570263, &quot;time-step&quot;: 1311}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02345842868089676, &quot;time-step&quot;: 1312}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02031087689101696, &quot;time-step&quot;: 1313}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.023358788341283798, &quot;time-step&quot;: 1314}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.020237674936652184, &quot;time-step&quot;: 1315}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02325957641005516, &quot;time-step&quot;: 1316}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.020164769142866135, &quot;time-step&quot;: 1317}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02316087856888771, &quot;time-step&quot;: 1318}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02009212225675583, &quot;time-step&quot;: 1319}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02306266874074936, &quot;time-step&quot;: 1320}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.020019780844449997, &quot;time-step&quot;: 1321}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.022964898496866226, &quot;time-step&quot;: 1322}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.019947681576013565, &quot;time-step&quot;: 1323}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.022867579013109207, &quot;time-step&quot;: 1324}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.019875824451446533, &quot;time-step&quot;: 1325}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.022770727053284645, &quot;time-step&quot;: 1326}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.019804270938038826, &quot;time-step&quot;: 1327}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02267433889210224, &quot;time-step&quot;: 1328}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01973298378288746, &quot;time-step&quot;: 1329}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02257835492491722, &quot;time-step&quot;: 1330}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.019661827012896538, &quot;time-step&quot;: 1331}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.022482749074697495, &quot;time-step&quot;: 1332}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01959099993109703, &quot;time-step&quot;: 1333}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02238771691918373, &quot;time-step&quot;: 1334}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01952051930129528, &quot;time-step&quot;: 1335}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.022293182089924812, &quot;time-step&quot;: 1336}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.019450321793556213, &quot;time-step&quot;: 1337}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02219908870756626, &quot;time-step&quot;: 1338}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.019380338490009308, &quot;time-step&quot;: 1339}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.022105425596237183, &quot;time-step&quot;: 1340}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.019310589879751205, &quot;time-step&quot;: 1341}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.022012146189808846, &quot;time-step&quot;: 1342}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.019241079688072205, &quot;time-step&quot;: 1343}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.021919352933764458, &quot;time-step&quot;: 1344}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01917189359664917, &quot;time-step&quot;: 1345}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.021827001124620438, &quot;time-step&quot;: 1346}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.019102878868579865, &quot;time-step&quot;: 1347}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.021735060960054398, &quot;time-step&quot;: 1348}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.019034145399928093, &quot;time-step&quot;: 1349}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02164357155561447, &quot;time-step&quot;: 1350}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.018965689465403557, &quot;time-step&quot;: 1351}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0215525534003973, &quot;time-step&quot;: 1352}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01889750361442566, &quot;time-step&quot;: 1353}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02146189846098423, &quot;time-step&quot;: 1354}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.018829533830285072, &quot;time-step&quot;: 1355}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.021371757611632347, &quot;time-step&quot;: 1356}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.018761875107884407, &quot;time-step&quot;: 1357}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.021282045170664787, &quot;time-step&quot;: 1358}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.018694471567869186, &quot;time-step&quot;: 1359}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.021192779764533043, &quot;time-step&quot;: 1360}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.018627211451530457, &quot;time-step&quot;: 1361}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02110375463962555, &quot;time-step&quot;: 1362}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.018560199066996574, &quot;time-step&quot;: 1363}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.021015282720327377, &quot;time-step&quot;: 1364}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.018493494018912315, &quot;time-step&quot;: 1365}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.020927229896187782, &quot;time-step&quot;: 1366}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01842712238430977, &quot;time-step&quot;: 1367}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.020839707925915718, &quot;time-step&quot;: 1368}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.018360942602157593, &quot;time-step&quot;: 1369}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.020752480253577232, &quot;time-step&quot;: 1370}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.018294909968972206, &quot;time-step&quot;: 1371}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.020665623247623444, &quot;time-step&quot;: 1372}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.018229210749268532, &quot;time-step&quot;: 1373}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0205792635679245, &quot;time-step&quot;: 1374}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.018163712695240974, &quot;time-step&quot;: 1375}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.020493298768997192, &quot;time-step&quot;: 1376}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01809847354888916, &quot;time-step&quot;: 1377}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.020407766103744507, &quot;time-step&quot;: 1378}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.018033519387245178, &quot;time-step&quot;: 1379}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.020322585478425026, &quot;time-step&quot;: 1380}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.017968716099858284, &quot;time-step&quot;: 1381}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.020237786695361137, &quot;time-step&quot;: 1382}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.017904222011566162, &quot;time-step&quot;: 1383}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.020153433084487915, &quot;time-step&quot;: 1384}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01783984713256359, &quot;time-step&quot;: 1385}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.02006937377154827, &quot;time-step&quot;: 1386}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01777580939233303, &quot;time-step&quot;: 1387}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.019985901191830635, &quot;time-step&quot;: 1388}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.017712002620100975, &quot;time-step&quot;: 1389}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.019902680069208145, &quot;time-step&quot;: 1390}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.017648380249738693, &quot;time-step&quot;: 1391}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.019819872453808784, &quot;time-step&quot;: 1392}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01758502423763275, &quot;time-step&quot;: 1393}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.019737474620342255, &quot;time-step&quot;: 1394}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.017521923407912254, &quot;time-step&quot;: 1395}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.019655529409646988, &quot;time-step&quot;: 1396}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01745905913412571, &quot;time-step&quot;: 1397}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.019573932513594627, &quot;time-step&quot;: 1398}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01739642769098282, &quot;time-step&quot;: 1399}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.019492723047733307, &quot;time-step&quot;: 1400}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.017333997413516045, &quot;time-step&quot;: 1401}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.019411878660321236, &quot;time-step&quot;: 1402}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.017271822318434715, &quot;time-step&quot;: 1403}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01933145895600319, &quot;time-step&quot;: 1404}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.017209816724061966, &quot;time-step&quot;: 1405}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.019251354038715363, &quot;time-step&quot;: 1406}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.017148131504654884, &quot;time-step&quot;: 1407}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.019171686843037605, &quot;time-step&quot;: 1408}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01708655059337616, &quot;time-step&quot;: 1409}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.019092269241809845, &quot;time-step&quot;: 1410}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.017025265842676163, &quot;time-step&quot;: 1411}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01901336759328842, &quot;time-step&quot;: 1412}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.016964249312877655, &quot;time-step&quot;: 1413}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01893479749560356, &quot;time-step&quot;: 1414}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.016903391107916832, &quot;time-step&quot;: 1415}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.018856622278690338, &quot;time-step&quot;: 1416}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.016842808574438095, &quot;time-step&quot;: 1417}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.018778754398226738, &quot;time-step&quot;: 1418}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.016782408580183983, &quot;time-step&quot;: 1419}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01870124042034149, &quot;time-step&quot;: 1420}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.016722148284316063, &quot;time-step&quot;: 1421}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0186240803450346, &quot;time-step&quot;: 1422}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.016662271693348885, &quot;time-step&quot;: 1423}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01854741759598255, &quot;time-step&quot;: 1424}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.016602544113993645, &quot;time-step&quot;: 1425}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0184710081666708, &quot;time-step&quot;: 1426}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01654297485947609, &quot;time-step&quot;: 1427}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.018394887447357178, &quot;time-step&quot;: 1428}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01648365519940853, &quot;time-step&quot;: 1429}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.018319198861718178, &quot;time-step&quot;: 1430}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01642456278204918, &quot;time-step&quot;: 1431}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01824386790394783, &quot;time-step&quot;: 1432}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01636573299765587, &quot;time-step&quot;: 1433}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01816890388727188, &quot;time-step&quot;: 1434}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.016307029873132706, &quot;time-step&quot;: 1435}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.018094222992658615, &quot;time-step&quot;: 1436}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.016248615458607674, &quot;time-step&quot;: 1437}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.018020015209913254, &quot;time-step&quot;: 1438}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01619044318795204, &quot;time-step&quot;: 1439}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.017946122214198112, &quot;time-step&quot;: 1440}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.016132425516843796, &quot;time-step&quot;: 1441}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.017872458323836327, &quot;time-step&quot;: 1442}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.016074562445282936, &quot;time-step&quot;: 1443}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01779918745160103, &quot;time-step&quot;: 1444}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0160170067101717, &quot;time-step&quot;: 1445}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.017726296558976173, &quot;time-step&quot;: 1446}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.015959622338414192, &quot;time-step&quot;: 1447}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.017653707414865494, &quot;time-step&quot;: 1448}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01590241864323616, &quot;time-step&quot;: 1449}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.017581479623913765, &quot;time-step&quot;: 1450}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01584547571837902, &quot;time-step&quot;: 1451}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.017509566619992256, &quot;time-step&quot;: 1452}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.015788761898875237, &quot;time-step&quot;: 1453}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01743803545832634, &quot;time-step&quot;: 1454}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.015732239931821823, &quot;time-step&quot;: 1455}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.017366867512464523, &quot;time-step&quot;: 1456}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.015675928443670273, &quot;time-step&quot;: 1457}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01729602739214897, &quot;time-step&quot;: 1458}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01561982836574316, &quot;time-step&quot;: 1459}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.017225472256541252, &quot;time-step&quot;: 1460}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.015563876368105412, &quot;time-step&quot;: 1461}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01715514063835144, &quot;time-step&quot;: 1462}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.015508139505982399, &quot;time-step&quot;: 1463}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.017085272818803787, &quot;time-step&quot;: 1464}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.015452686697244644, &quot;time-step&quot;: 1465}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01701570302248001, &quot;time-step&quot;: 1466}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.015397327020764351, &quot;time-step&quot;: 1467}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01694643869996071, &quot;time-step&quot;: 1468}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01534225046634674, &quot;time-step&quot;: 1469}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.016877535730600357, &quot;time-step&quot;: 1470}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.015287394635379314, &quot;time-step&quot;: 1471}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.016808925196528435, &quot;time-step&quot;: 1472}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.015232700854539871, &quot;time-step&quot;: 1473}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.016740625724196434, &quot;time-step&quot;: 1474}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01517820917069912, &quot;time-step&quot;: 1475}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.016672682017087936, &quot;time-step&quot;: 1476}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.015123971737921238, &quot;time-step&quot;: 1477}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01660502329468727, &quot;time-step&quot;: 1478}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.015069892629981041, &quot;time-step&quot;: 1479}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.016537729650735855, &quot;time-step&quot;: 1480}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.015015954151749611, &quot;time-step&quot;: 1481}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.016470598056912422, &quot;time-step&quot;: 1482}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.014962218701839447, &quot;time-step&quot;: 1483}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.016403822228312492, &quot;time-step&quot;: 1484}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.014908695593476295, &quot;time-step&quot;: 1485}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.016337363049387932, &quot;time-step&quot;: 1486}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.014855436980724335, &quot;time-step&quot;: 1487}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01627127267420292, &quot;time-step&quot;: 1488}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.014802252873778343, &quot;time-step&quot;: 1489}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01620541140437126, &quot;time-step&quot;: 1490}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.014749359339475632, &quot;time-step&quot;: 1491}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01613987423479557, &quot;time-step&quot;: 1492}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01469656452536583, &quot;time-step&quot;: 1493}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.016074616461992264, &quot;time-step&quot;: 1494}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.014644077979028225, &quot;time-step&quot;: 1495}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.016009779646992683, &quot;time-step&quot;: 1496}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.014591773971915245, &quot;time-step&quot;: 1497}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01594516448676586, &quot;time-step&quot;: 1498}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.014539660885930061, &quot;time-step&quot;: 1499}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.015880892053246498, &quot;time-step&quot;: 1500}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.014487712644040585, &quot;time-step&quot;: 1501}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.015816885977983475, &quot;time-step&quot;: 1502}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.014435974881052971, &quot;time-step&quot;: 1503}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01575322076678276, &quot;time-step&quot;: 1504}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.014384478330612183, &quot;time-step&quot;: 1505}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.015689855441451073, &quot;time-step&quot;: 1506}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.014333155937492847, &quot;time-step&quot;: 1507}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01562676578760147, &quot;time-step&quot;: 1508}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.014282011426985264, &quot;time-step&quot;: 1509}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.015563988126814365, &quot;time-step&quot;: 1510}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.014231054112315178, &quot;time-step&quot;: 1511}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0155014144256711, &quot;time-step&quot;: 1512}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.014180183410644531, &quot;time-step&quot;: 1513}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.015439072623848915, &quot;time-step&quot;: 1514}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.014129582792520523, &quot;time-step&quot;: 1515}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.015377131290733814, &quot;time-step&quot;: 1516}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.014079204760491848, &quot;time-step&quot;: 1517}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.015315471217036247, &quot;time-step&quot;: 1518}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.014028920792043209, &quot;time-step&quot;: 1519}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.015253983438014984, &quot;time-step&quot;: 1520}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.013978819362819195, &quot;time-step&quot;: 1521}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0151927899569273, &quot;time-step&quot;: 1522}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.013928917236626148, &quot;time-step&quot;: 1523}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.015131952241063118, &quot;time-step&quot;: 1524}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.013879240490496159, &quot;time-step&quot;: 1525}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.015071452595293522, &quot;time-step&quot;: 1526}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.013829859904944897, &quot;time-step&quot;: 1527}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01501127053052187, &quot;time-step&quot;: 1528}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.013780616223812103, &quot;time-step&quot;: 1529}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.014951279386878014, &quot;time-step&quot;: 1530}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.013731485232710838, &quot;time-step&quot;: 1531}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01489159744232893, &quot;time-step&quot;: 1532}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.013682611286640167, &quot;time-step&quot;: 1533}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.014832187443971634, &quot;time-step&quot;: 1534}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.013633891940116882, &quot;time-step&quot;: 1535}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.014773078262805939, &quot;time-step&quot;: 1536}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.013585338369011879, &quot;time-step&quot;: 1537}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01471409760415554, &quot;time-step&quot;: 1538}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.013536826707422733, &quot;time-step&quot;: 1539}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.014655394479632378, &quot;time-step&quot;: 1540}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.013488641940057278, &quot;time-step&quot;: 1541}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.014597037807106972, &quot;time-step&quot;: 1542}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01344066858291626, &quot;time-step&quot;: 1543}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.014539000578224659, &quot;time-step&quot;: 1544}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.013392830267548561, &quot;time-step&quot;: 1545}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01448114775121212, &quot;time-step&quot;: 1546}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01334511861205101, &quot;time-step&quot;: 1547}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.014423585496842861, &quot;time-step&quot;: 1548}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.013297615572810173, &quot;time-step&quot;: 1549}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.014366261661052704, &quot;time-step&quot;: 1550}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01325029693543911, &quot;time-step&quot;: 1551}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01430920697748661, &quot;time-step&quot;: 1552}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.013203159905970097, &quot;time-step&quot;: 1553}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.014252479188144207, &quot;time-step&quot;: 1554}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.013156306929886341, &quot;time-step&quot;: 1555}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.014196033589541912, &quot;time-step&quot;: 1556}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.013109520077705383, &quot;time-step&quot;: 1557}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.014139821752905846, &quot;time-step&quot;: 1558}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.013062936253845692, &quot;time-step&quot;: 1559}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.014083772897720337, &quot;time-step&quot;: 1560}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.013016492128372192, &quot;time-step&quot;: 1561}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.014027981087565422, &quot;time-step&quot;: 1562}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.012970157898962498, &quot;time-step&quot;: 1563}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.013972459360957146, &quot;time-step&quot;: 1564}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.012924104928970337, &quot;time-step&quot;: 1565}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.013917279429733753, &quot;time-step&quot;: 1566}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.012878242880105972, &quot;time-step&quot;: 1567}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.013862352818250656, &quot;time-step&quot;: 1568}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.012832501903176308, &quot;time-step&quot;: 1569}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.013807648792862892, &quot;time-step&quot;: 1570}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.012787005864083767, &quot;time-step&quot;: 1571}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01375325582921505, &quot;time-step&quot;: 1572}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.012741648592054844, &quot;time-step&quot;: 1573}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.013698997907340527, &quot;time-step&quot;: 1574}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.012696430087089539, &quot;time-step&quot;: 1575}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.013645105995237827, &quot;time-step&quot;: 1576}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.012651436030864716, &quot;time-step&quot;: 1577}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.013591360300779343, &quot;time-step&quot;: 1578}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.012606472708284855, &quot;time-step&quot;: 1579}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01353782694786787, &quot;time-step&quot;: 1580}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.012561832554638386, &quot;time-step&quot;: 1581}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.013484605588018894, &quot;time-step&quot;: 1582}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01251728180795908, &quot;time-step&quot;: 1583}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.013431616127490997, &quot;time-step&quot;: 1584}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.012472914531826973, &quot;time-step&quot;: 1585}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.013378913514316082, &quot;time-step&quot;: 1586}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.012428765185177326, &quot;time-step&quot;: 1587}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.013326430693268776, &quot;time-step&quot;: 1588}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.012384729459881783, &quot;time-step&quot;: 1589}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.013274195604026318, &quot;time-step&quot;: 1590}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.012340916320681572, &quot;time-step&quot;: 1591}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.013222165405750275, &quot;time-step&quot;: 1592}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.012297236360609531, &quot;time-step&quot;: 1593}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.013170425780117512, &quot;time-step&quot;: 1594}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.012253767810761929, &quot;time-step&quot;: 1595}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.013118930160999298, &quot;time-step&quot;: 1596}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0122104212641716, &quot;time-step&quot;: 1597}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.013067617081105709, &quot;time-step&quot;: 1598}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.012167224660515785, &quot;time-step&quot;: 1599}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.013016547076404095, &quot;time-step&quot;: 1600}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01212423574179411, &quot;time-step&quot;: 1601}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.012965732254087925, &quot;time-step&quot;: 1602}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.012081331573426723, &quot;time-step&quot;: 1603}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01291513629257679, &quot;time-step&quot;: 1604}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.012038717046380043, &quot;time-step&quot;: 1605}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.012864826247096062, &quot;time-step&quot;: 1606}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.011996209621429443, &quot;time-step&quot;: 1607}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.012814700603485107, &quot;time-step&quot;: 1608}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.011953867971897125, &quot;time-step&quot;: 1609}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.012764772400259972, &quot;time-step&quot;: 1610}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.011911618523299694, &quot;time-step&quot;: 1611}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01271507702767849, &quot;time-step&quot;: 1612}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01186957024037838, &quot;time-step&quot;: 1613}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.012665623798966408, &quot;time-step&quot;: 1614}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01182771846652031, &quot;time-step&quot;: 1615}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.012616443447768688, &quot;time-step&quot;: 1616}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.011786055751144886, &quot;time-step&quot;: 1617}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.012567475438117981, &quot;time-step&quot;: 1618}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.011744476854801178, &quot;time-step&quot;: 1619}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01251864992082119, &quot;time-step&quot;: 1620}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01170302927494049, &quot;time-step&quot;: 1621}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.012470067478716373, &quot;time-step&quot;: 1622}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.011661826632916927, &quot;time-step&quot;: 1623}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.012421770952641964, &quot;time-step&quot;: 1624}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.011620782315731049, &quot;time-step&quot;: 1625}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.012373726814985275, &quot;time-step&quot;: 1626}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.011579882353544235, &quot;time-step&quot;: 1627}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.012325873598456383, &quot;time-step&quot;: 1628}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.011539117433130741, &quot;time-step&quot;: 1629}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.012278186157345772, &quot;time-step&quot;: 1630}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.011498527601361275, &quot;time-step&quot;: 1631}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.012230792082846165, &quot;time-step&quot;: 1632}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.011458111926913261, &quot;time-step&quot;: 1633}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01218356192111969, &quot;time-step&quot;: 1634}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.011417856439948082, &quot;time-step&quot;: 1635}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01213662326335907, &quot;time-step&quot;: 1636}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.011377759277820587, &quot;time-step&quot;: 1637}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.012089846655726433, &quot;time-step&quot;: 1638}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01133782509714365, &quot;time-step&quot;: 1639}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.012043291702866554, &quot;time-step&quot;: 1640}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.011297984048724174, &quot;time-step&quot;: 1641}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.011996876448392868, &quot;time-step&quot;: 1642}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.011258276179432869, &quot;time-step&quot;: 1643}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.011950649321079254, &quot;time-step&quot;: 1644}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.011218744330108166, &quot;time-step&quot;: 1645}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.011904720216989517, &quot;time-step&quot;: 1646}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.011179391294717789, &quot;time-step&quot;: 1647}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01185902114957571, &quot;time-step&quot;: 1648}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01114021334797144, &quot;time-step&quot;: 1649}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01181358564645052, &quot;time-step&quot;: 1650}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.011101260781288147, &quot;time-step&quot;: 1651}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01176831778138876, &quot;time-step&quot;: 1652}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.011062365025281906, &quot;time-step&quot;: 1653}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01172318123281002, &quot;time-step&quot;: 1654}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.011023596860468388, &quot;time-step&quot;: 1655}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.011678295210003853, &quot;time-step&quot;: 1656}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010985004715621471, &quot;time-step&quot;: 1657}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.011633647605776787, &quot;time-step&quot;: 1658}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010946592316031456, &quot;time-step&quot;: 1659}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01158914715051651, &quot;time-step&quot;: 1660}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010908364318311214, &quot;time-step&quot;: 1661}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.011544973589479923, &quot;time-step&quot;: 1662}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010870265774428844, &quot;time-step&quot;: 1663}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.011500900611281395, &quot;time-step&quot;: 1664}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010832270607352257, &quot;time-step&quot;: 1665}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.011456998065114021, &quot;time-step&quot;: 1666}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010794364847242832, &quot;time-step&quot;: 1667}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.011413254775106907, &quot;time-step&quot;: 1668}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010756637901067734, &quot;time-step&quot;: 1669}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01136975921690464, &quot;time-step&quot;: 1670}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01071912981569767, &quot;time-step&quot;: 1671}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.011326532810926437, &quot;time-step&quot;: 1672}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010681717656552792, &quot;time-step&quot;: 1673}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.011283478699624538, &quot;time-step&quot;: 1674}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01064455509185791, &quot;time-step&quot;: 1675}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.011240672320127487, &quot;time-step&quot;: 1676}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010607396252453327, &quot;time-step&quot;: 1677}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.011197932995855808, &quot;time-step&quot;: 1678}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010570451617240906, &quot;time-step&quot;: 1679}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.011155447922647, &quot;time-step&quot;: 1680}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010533631779253483, &quot;time-step&quot;: 1681}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.011113142594695091, &quot;time-step&quot;: 1682}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010496939532458782, &quot;time-step&quot;: 1683}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.011071027256548405, &quot;time-step&quot;: 1684}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010460424236953259, &quot;time-step&quot;: 1685}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.011029166169464588, &quot;time-step&quot;: 1686}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010424096137285233, &quot;time-step&quot;: 1687}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010987498797476292, &quot;time-step&quot;: 1688}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010387898422777653, &quot;time-step&quot;: 1689}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010945998132228851, &quot;time-step&quot;: 1690}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01035179290920496, &quot;time-step&quot;: 1691}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010904655791819096, &quot;time-step&quot;: 1692}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01031583733856678, &quot;time-step&quot;: 1693}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010863522067666054, &quot;time-step&quot;: 1694}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010280036367475986, &quot;time-step&quot;: 1695}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010822540149092674, &quot;time-step&quot;: 1696}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010244358330965042, &quot;time-step&quot;: 1697}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010781805962324142, &quot;time-step&quot;: 1698}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010208847932517529, &quot;time-step&quot;: 1699}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010741219855844975, &quot;time-step&quot;: 1700}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010173463262617588, &quot;time-step&quot;: 1701}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010700799524784088, &quot;time-step&quot;: 1702}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010138212703168392, &quot;time-step&quot;: 1703}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01066066324710846, &quot;time-step&quot;: 1704}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01010318286716938, &quot;time-step&quot;: 1705}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01062062755227089, &quot;time-step&quot;: 1706}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01006814744323492, &quot;time-step&quot;: 1707}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010580734349787235, &quot;time-step&quot;: 1708}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010033324360847473, &quot;time-step&quot;: 1709}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010541166178882122, &quot;time-step&quot;: 1710}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009998725727200508, &quot;time-step&quot;: 1711}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010501750744879246, &quot;time-step&quot;: 1712}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009964222088456154, &quot;time-step&quot;: 1713}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01046249270439148, &quot;time-step&quot;: 1714}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009929823689162731, &quot;time-step&quot;: 1715}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010423436760902405, &quot;time-step&quot;: 1716}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009895591996610165, &quot;time-step&quot;: 1717}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01038446370512247, &quot;time-step&quot;: 1718}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009861433878540993, &quot;time-step&quot;: 1719}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010345706716179848, &quot;time-step&quot;: 1720}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009827409870922565, &quot;time-step&quot;: 1721}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01030709594488144, &quot;time-step&quot;: 1722}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009793545119464397, &quot;time-step&quot;: 1723}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010268734768033028, &quot;time-step&quot;: 1724}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009759863838553429, &quot;time-step&quot;: 1725}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010230551473796368, &quot;time-step&quot;: 1726}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009726288728415966, &quot;time-step&quot;: 1727}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01019255630671978, &quot;time-step&quot;: 1728}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009692879393696785, &quot;time-step&quot;: 1729}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.01015465147793293, &quot;time-step&quot;: 1730}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00965953804552555, &quot;time-step&quot;: 1731}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010116969235241413, &quot;time-step&quot;: 1732}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009626359678804874, &quot;time-step&quot;: 1733}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010079476051032543, &quot;time-step&quot;: 1734}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009593313559889793, &quot;time-step&quot;: 1735}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010042082518339157, &quot;time-step&quot;: 1736}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009560327976942062, &quot;time-step&quot;: 1737}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.010004847310483456, &quot;time-step&quot;: 1738}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009527537040412426, &quot;time-step&quot;: 1739}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009967860765755177, &quot;time-step&quot;: 1740}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009494876489043236, &quot;time-step&quot;: 1741}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009931019507348537, &quot;time-step&quot;: 1742}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009462352842092514, &quot;time-step&quot;: 1743}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009894308634102345, &quot;time-step&quot;: 1744}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009429964236915112, &quot;time-step&quot;: 1745}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009857822209596634, &quot;time-step&quot;: 1746}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00939769484102726, &quot;time-step&quot;: 1747}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00982155092060566, &quot;time-step&quot;: 1748}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009365648962557316, &quot;time-step&quot;: 1749}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009785441681742668, &quot;time-step&quot;: 1750}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009333662688732147, &quot;time-step&quot;: 1751}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009749449789524078, &quot;time-step&quot;: 1752}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009301791898906231, &quot;time-step&quot;: 1753}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009713608771562576, &quot;time-step&quot;: 1754}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0092700170353055, &quot;time-step&quot;: 1755}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009677896276116371, &quot;time-step&quot;: 1756}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009238367900252342, &quot;time-step&quot;: 1757}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009642324410378933, &quot;time-step&quot;: 1758}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009206822142004967, &quot;time-step&quot;: 1759}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009606908075511456, &quot;time-step&quot;: 1760}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009175437502563, &quot;time-step&quot;: 1761}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009571781381964684, &quot;time-step&quot;: 1762}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00914421584457159, &quot;time-step&quot;: 1763}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009536714293062687, &quot;time-step&quot;: 1764}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009113115258514881, &quot;time-step&quot;: 1765}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009501887485384941, &quot;time-step&quot;: 1766}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009082144126296043, &quot;time-step&quot;: 1767}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00946720875799656, &quot;time-step&quot;: 1768}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009051285684108734, &quot;time-step&quot;: 1769}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009432678110897541, &quot;time-step&quot;: 1770}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00902058556675911, &quot;time-step&quot;: 1771}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00939828623086214, &quot;time-step&quot;: 1772}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00898993294686079, &quot;time-step&quot;: 1773}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009364031255245209, &quot;time-step&quot;: 1774}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00895945355296135, &quot;time-step&quot;: 1775}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009329969063401222, &quot;time-step&quot;: 1776}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008929100818932056, &quot;time-step&quot;: 1777}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009296047501266003, &quot;time-step&quot;: 1778}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00889887660741806, &quot;time-step&quot;: 1779}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009262311272323132, &quot;time-step&quot;: 1780}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008868756704032421, &quot;time-step&quot;: 1781}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009228707291185856, &quot;time-step&quot;: 1782}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008838756009936333, &quot;time-step&quot;: 1783}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009195193648338318, &quot;time-step&quot;: 1784}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008808787912130356, &quot;time-step&quot;: 1785}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009161875583231449, &quot;time-step&quot;: 1786}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008779099211096764, &quot;time-step&quot;: 1787}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009128735400736332, &quot;time-step&quot;: 1788}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008749411441385746, &quot;time-step&quot;: 1789}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00909571535885334, &quot;time-step&quot;: 1790}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008719897828996181, &quot;time-step&quot;: 1791}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009062876924872398, &quot;time-step&quot;: 1792}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008690545335412025, &quot;time-step&quot;: 1793}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.009030220098793507, &quot;time-step&quot;: 1794}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008661314845085144, &quot;time-step&quot;: 1795}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008997691795229912, &quot;time-step&quot;: 1796}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008632145822048187, &quot;time-step&quot;: 1797}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008965269662439823, &quot;time-step&quot;: 1798}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008603089489042759, &quot;time-step&quot;: 1799}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008933036588132381, &quot;time-step&quot;: 1800}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00857422687113285, &quot;time-step&quot;: 1801}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008900970220565796, &quot;time-step&quot;: 1802}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00854544062167406, &quot;time-step&quot;: 1803}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008869036100804806, &quot;time-step&quot;: 1804}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00851674098521471, &quot;time-step&quot;: 1805}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008837188594043255, &quot;time-step&quot;: 1806}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008488163352012634, &quot;time-step&quot;: 1807}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008805477991700172, &quot;time-step&quot;: 1808}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008459660224616528, &quot;time-step&quot;: 1809}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008773934096097946, &quot;time-step&quot;: 1810}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00843135081231594, &quot;time-step&quot;: 1811}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00874259416013956, &quot;time-step&quot;: 1812}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008403141051530838, &quot;time-step&quot;: 1813}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008711335249245167, &quot;time-step&quot;: 1814}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00837500486522913, &quot;time-step&quot;: 1815}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008680223487317562, &quot;time-step&quot;: 1816}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008347021415829659, &quot;time-step&quot;: 1817}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008649296127259731, &quot;time-step&quot;: 1818}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008319167420268059, &quot;time-step&quot;: 1819}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008618470281362534, &quot;time-step&quot;: 1820}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00829138420522213, &quot;time-step&quot;: 1821}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008587803691625595, &quot;time-step&quot;: 1822}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008263742551207542, &quot;time-step&quot;: 1823}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008557233959436417, &quot;time-step&quot;: 1824}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008236164227128029, &quot;time-step&quot;: 1825}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00852684024721384, &quot;time-step&quot;: 1826}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008208765648305416, &quot;time-step&quot;: 1827}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008496607653796673, &quot;time-step&quot;: 1828}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008181469514966011, &quot;time-step&quot;: 1829}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008466491475701332, &quot;time-step&quot;: 1830}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008154276758432388, &quot;time-step&quot;: 1831}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008436532691121101, &quot;time-step&quot;: 1832}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008127202279865742, &quot;time-step&quot;: 1833}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00840671919286251, &quot;time-step&quot;: 1834}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008100271224975586, &quot;time-step&quot;: 1835}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008377023972570896, &quot;time-step&quot;: 1836}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008073368109762669, &quot;time-step&quot;: 1837}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008347420021891594, &quot;time-step&quot;: 1838}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008046630769968033, &quot;time-step&quot;: 1839}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00831795483827591, &quot;time-step&quot;: 1840}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008019909262657166, &quot;time-step&quot;: 1841}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008288581855595112, &quot;time-step&quot;: 1842}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007993310689926147, &quot;time-step&quot;: 1843}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00825939979404211, &quot;time-step&quot;: 1844}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007966909557580948, &quot;time-step&quot;: 1845}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008230376057326794, &quot;time-step&quot;: 1846}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007940586656332016, &quot;time-step&quot;: 1847}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008201462216675282, &quot;time-step&quot;: 1848}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007914379239082336, &quot;time-step&quot;: 1849}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00817269366234541, &quot;time-step&quot;: 1850}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00788826309144497, &quot;time-step&quot;: 1851}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008144024759531021, &quot;time-step&quot;: 1852}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007862245664000511, &quot;time-step&quot;: 1853}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008115546777844429, &quot;time-step&quot;: 1854}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007836399599909782, &quot;time-step&quot;: 1855}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00808718428015709, &quot;time-step&quot;: 1856}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007810586132109165, &quot;time-step&quot;: 1857}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008058860898017883, &quot;time-step&quot;: 1858}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007784857880324125, &quot;time-step&quot;: 1859}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.008030695840716362, &quot;time-step&quot;: 1860}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007759273983538151, &quot;time-step&quot;: 1861}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00800267606973648, &quot;time-step&quot;: 1862}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007733779959380627, &quot;time-step&quot;: 1863}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007974805310368538, &quot;time-step&quot;: 1864}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007708407007157803, &quot;time-step&quot;: 1865}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007947087287902832, &quot;time-step&quot;: 1866}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007683188188821077, &quot;time-step&quot;: 1867}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00791945867240429, &quot;time-step&quot;: 1868}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007658025249838829, &quot;time-step&quot;: 1869}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00789197813719511, &quot;time-step&quot;: 1870}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0076330117881298065, &quot;time-step&quot;: 1871}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007864649407565594, &quot;time-step&quot;: 1872}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007608057931065559, &quot;time-step&quot;: 1873}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007837366312742233, &quot;time-step&quot;: 1874}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007583173457533121, &quot;time-step&quot;: 1875}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0078102112747728825, &quot;time-step&quot;: 1876}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007558413781225681, &quot;time-step&quot;: 1877}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007783218752592802, &quot;time-step&quot;: 1878}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0075337584130465984, &quot;time-step&quot;: 1879}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007756280712783337, &quot;time-step&quot;: 1880}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007509130053222179, &quot;time-step&quot;: 1881}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007729462347924709, &quot;time-step&quot;: 1882}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007484687492251396, &quot;time-step&quot;: 1883}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0077028293162584305, &quot;time-step&quot;: 1884}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007460325490683317, &quot;time-step&quot;: 1885}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007676294539123774, &quot;time-step&quot;: 1886}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007436074316501617, &quot;time-step&quot;: 1887}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007649905048310757, &quot;time-step&quot;: 1888}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007411966100335121, &quot;time-step&quot;: 1889}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00762363662943244, &quot;time-step&quot;: 1890}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007387866266071796, &quot;time-step&quot;: 1891}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007597426883876324, &quot;time-step&quot;: 1892}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007363910786807537, &quot;time-step&quot;: 1893}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007571373600512743, &quot;time-step&quot;: 1894}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007340059150010347, &quot;time-step&quot;: 1895}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00754549540579319, &quot;time-step&quot;: 1896}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0073163434863090515, &quot;time-step&quot;: 1897}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007519686594605446, &quot;time-step&quot;: 1898}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007292683236300945, &quot;time-step&quot;: 1899}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007493963465094566, &quot;time-step&quot;: 1900}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0072690751403570175, &quot;time-step&quot;: 1901}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007468344643712044, &quot;time-step&quot;: 1902}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007245583925396204, &quot;time-step&quot;: 1903}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007442871108651161, &quot;time-step&quot;: 1904}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007222270127385855, &quot;time-step&quot;: 1905}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00741759967058897, &quot;time-step&quot;: 1906}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007199005223810673, &quot;time-step&quot;: 1907}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007392344530671835, &quot;time-step&quot;: 1908}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007175837643444538, &quot;time-step&quot;: 1909}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007367216050624847, &quot;time-step&quot;: 1910}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007152758538722992, &quot;time-step&quot;: 1911}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00734220864251256, &quot;time-step&quot;: 1912}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00712979631498456, &quot;time-step&quot;: 1913}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007317338604480028, &quot;time-step&quot;: 1914}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007106910925358534, &quot;time-step&quot;: 1915}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007292529102414846, &quot;time-step&quot;: 1916}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007084054872393608, &quot;time-step&quot;: 1917}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007267806679010391, &quot;time-step&quot;: 1918}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007061357144266367, &quot;time-step&quot;: 1919}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007243271451443434, &quot;time-step&quot;: 1920}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007038759533315897, &quot;time-step&quot;: 1921}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007218805141746998, &quot;time-step&quot;: 1922}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007016237825155258, &quot;time-step&quot;: 1923}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007194428239017725, &quot;time-step&quot;: 1924}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006993766874074936, &quot;time-step&quot;: 1925}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007170185912400484, &quot;time-step&quot;: 1926}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006971411872655153, &quot;time-step&quot;: 1927}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007146033924072981, &quot;time-step&quot;: 1928}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006949215196073055, &quot;time-step&quot;: 1929}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007122064009308815, &quot;time-step&quot;: 1930}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006927064619958401, &quot;time-step&quot;: 1931}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007098136469721794, &quot;time-step&quot;: 1932}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006904990412294865, &quot;time-step&quot;: 1933}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007074309047311544, &quot;time-step&quot;: 1934}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006883033085614443, &quot;time-step&quot;: 1935}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.007050645537674427, &quot;time-step&quot;: 1936}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006861109286546707, &quot;time-step&quot;: 1937}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0070269848220050335, &quot;time-step&quot;: 1938}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006839324254542589, &quot;time-step&quot;: 1939}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00700351782143116, &quot;time-step&quot;: 1940}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006817622110247612, &quot;time-step&quot;: 1941}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006980110891163349, &quot;time-step&quot;: 1942}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006795984227210283, &quot;time-step&quot;: 1943}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006956843659281731, &quot;time-step&quot;: 1944}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006774488836526871, &quot;time-step&quot;: 1945}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006933627650141716, &quot;time-step&quot;: 1946}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006752979941666126, &quot;time-step&quot;: 1947}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006910487078130245, &quot;time-step&quot;: 1948}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006731576286256313, &quot;time-step&quot;: 1949}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006887521594762802, &quot;time-step&quot;: 1950}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006710369139909744, &quot;time-step&quot;: 1951}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00686468742787838, &quot;time-step&quot;: 1952}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0066892108879983425, &quot;time-step&quot;: 1953}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0068419440649449825, &quot;time-step&quot;: 1954}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006668149493634701, &quot;time-step&quot;: 1955}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00681929849088192, &quot;time-step&quot;: 1956}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006647147238254547, &quot;time-step&quot;: 1957}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006796724628657103, &quot;time-step&quot;: 1958}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0066262464970350266, &quot;time-step&quot;: 1959}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006774268113076687, &quot;time-step&quot;: 1960}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006605386734008789, &quot;time-step&quot;: 1961}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0067518665455281734, &quot;time-step&quot;: 1962}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006584628019481897, &quot;time-step&quot;: 1963}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006729611195623875, &quot;time-step&quot;: 1964}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006563958711922169, &quot;time-step&quot;: 1965}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00670738285407424, &quot;time-step&quot;: 1966}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006543324328958988, &quot;time-step&quot;: 1967}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006685296073555946, &quot;time-step&quot;: 1968}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006522851996123791, &quot;time-step&quot;: 1969}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006663323380053043, &quot;time-step&quot;: 1970}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0065024616196751595, &quot;time-step&quot;: 1971}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006641479674726725, &quot;time-step&quot;: 1972}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00648213317617774, &quot;time-step&quot;: 1973}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006619680672883987, &quot;time-step&quot;: 1974}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006461861077696085, &quot;time-step&quot;: 1975}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006597989704459906, &quot;time-step&quot;: 1976}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006441700272262096, &quot;time-step&quot;: 1977}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006576379761099815, &quot;time-step&quot;: 1978}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006421588361263275, &quot;time-step&quot;: 1979}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0065548657439649105, &quot;time-step&quot;: 1980}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006401606369763613, &quot;time-step&quot;: 1981}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006533449981361628, &quot;time-step&quot;: 1982}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006381634157150984, &quot;time-step&quot;: 1983}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006512125954031944, &quot;time-step&quot;: 1984}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006361796520650387, &quot;time-step&quot;: 1985}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006490867584943771, &quot;time-step&quot;: 1986}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006342017091810703, &quot;time-step&quot;: 1987}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006469763815402985, &quot;time-step&quot;: 1988}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006322366185486317, &quot;time-step&quot;: 1989}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0064487699419260025, &quot;time-step&quot;: 1990}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006302817724645138, &quot;time-step&quot;: 1991}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006427873857319355, &quot;time-step&quot;: 1992}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006283348426222801, &quot;time-step&quot;: 1993}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006407079752534628, &quot;time-step&quot;: 1994}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006263946183025837, &quot;time-step&quot;: 1995}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006386335473507643, &quot;time-step&quot;: 1996}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006244594696909189, &quot;time-step&quot;: 1997}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006365708075463772, &quot;time-step&quot;: 1998}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006225310266017914, &quot;time-step&quot;: 1999}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006345120258629322, &quot;time-step&quot;: 2000}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006206111516803503, &quot;time-step&quot;: 2001}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006324637681245804, &quot;time-step&quot;: 2002}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006186976563185453, &quot;time-step&quot;: 2003}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0063042351976037025, &quot;time-step&quot;: 2004}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006167899817228317, &quot;time-step&quot;: 2005}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006283904891461134, &quot;time-step&quot;: 2006}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006148975342512131, &quot;time-step&quot;: 2007}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006263714283704758, &quot;time-step&quot;: 2008}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006130051799118519, &quot;time-step&quot;: 2009}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00624358095228672, &quot;time-step&quot;: 2010}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006111229304224253, &quot;time-step&quot;: 2011}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006223530508577824, &quot;time-step&quot;: 2012}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0060924869030714035, &quot;time-step&quot;: 2013}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006203577388077974, &quot;time-step&quot;: 2014}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006073847878724337, &quot;time-step&quot;: 2015}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006183759309351444, &quot;time-step&quot;: 2016}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006055300123989582, &quot;time-step&quot;: 2017}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006163998506963253, &quot;time-step&quot;: 2018}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006036762613803148, &quot;time-step&quot;: 2019}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006144313141703606, &quot;time-step&quot;: 2020}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006018369924277067, &quot;time-step&quot;: 2021}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006124740932136774, &quot;time-step&quot;: 2022}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006000017747282982, &quot;time-step&quot;: 2023}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0061052106320858, &quot;time-step&quot;: 2024}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005981756839901209, &quot;time-step&quot;: 2025}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006085831206291914, &quot;time-step&quot;: 2026}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005963563919067383, &quot;time-step&quot;: 2027}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00606645829975605, &quot;time-step&quot;: 2028}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0059454115107655525, &quot;time-step&quot;: 2029}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006047168746590614, &quot;time-step&quot;: 2030}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005927293561398983, &quot;time-step&quot;: 2031}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.006027943920344114, &quot;time-step&quot;: 2032}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005909244064241648, &quot;time-step&quot;: 2033}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0060088071040809155, &quot;time-step&quot;: 2034}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005891352891921997, &quot;time-step&quot;: 2035}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005989814642816782, &quot;time-step&quot;: 2036}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005873472895473242, &quot;time-step&quot;: 2037}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005970857106149197, &quot;time-step&quot;: 2038}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0058557577431201935, &quot;time-step&quot;: 2039}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005952070467174053, &quot;time-step&quot;: 2040}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005838069599121809, &quot;time-step&quot;: 2041}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005933303851634264, &quot;time-step&quot;: 2042}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005820471793413162, &quot;time-step&quot;: 2043}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005914666224271059, &quot;time-step&quot;: 2044}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005802904721349478, &quot;time-step&quot;: 2045}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005896029062569141, &quot;time-step&quot;: 2046}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005785360001027584, &quot;time-step&quot;: 2047}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005877463612705469, &quot;time-step&quot;: 2048}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005767961964011192, &quot;time-step&quot;: 2049}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00585902389138937, &quot;time-step&quot;: 2050}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005750603973865509, &quot;time-step&quot;: 2051}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005840699188411236, &quot;time-step&quot;: 2052}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005733385682106018, &quot;time-step&quot;: 2053}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005822456907480955, &quot;time-step&quot;: 2054}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005716205574572086, &quot;time-step&quot;: 2055}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005804232321679592, &quot;time-step&quot;: 2056}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0056990706361830235, &quot;time-step&quot;: 2057}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005786116700619459, &quot;time-step&quot;: 2058}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005682001356035471, &quot;time-step&quot;: 2059}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005768083035945892, &quot;time-step&quot;: 2060}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005665022414177656, &quot;time-step&quot;: 2061}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005750128533691168, &quot;time-step&quot;: 2062}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005648073274642229, &quot;time-step&quot;: 2063}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005732204765081406, &quot;time-step&quot;: 2064}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005631194915622473, &quot;time-step&quot;: 2065}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005714393220841885, &quot;time-step&quot;: 2066}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005614462774246931, &quot;time-step&quot;: 2067}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005696702748537064, &quot;time-step&quot;: 2068}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005597766023129225, &quot;time-step&quot;: 2069}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005679083988070488, &quot;time-step&quot;: 2070}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005581140052527189, &quot;time-step&quot;: 2071}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00566151412203908, &quot;time-step&quot;: 2072}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005564522929489613, &quot;time-step&quot;: 2073}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005643955431878567, &quot;time-step&quot;: 2074}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005547946318984032, &quot;time-step&quot;: 2075}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005626491736620665, &quot;time-step&quot;: 2076}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005531522445380688, &quot;time-step&quot;: 2077}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0056091658771038055, &quot;time-step&quot;: 2078}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005515153054147959, &quot;time-step&quot;: 2079}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005591896362602711, &quot;time-step&quot;: 2080}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005498834419995546, &quot;time-step&quot;: 2081}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005574685521423817, &quot;time-step&quot;: 2082}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005482593551278114, &quot;time-step&quot;: 2083}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005557587835937738, &quot;time-step&quot;: 2084}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005466413218528032, &quot;time-step&quot;: 2085}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005540487356483936, &quot;time-step&quot;: 2086}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005450247786939144, &quot;time-step&quot;: 2087}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005523479077965021, &quot;time-step&quot;: 2088}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005434185266494751, &quot;time-step&quot;: 2089}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005506571382284164, &quot;time-step&quot;: 2090}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0054181586019694805, &quot;time-step&quot;: 2091}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005489706061780453, &quot;time-step&quot;: 2092}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005402262322604656, &quot;time-step&quot;: 2093}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005472949240356684, &quot;time-step&quot;: 2094}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005386370234191418, &quot;time-step&quot;: 2095}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00545622268691659, &quot;time-step&quot;: 2096}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005370555445551872, &quot;time-step&quot;: 2097}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005439596250653267, &quot;time-step&quot;: 2098}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00535479886457324, &quot;time-step&quot;: 2099}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005423033144325018, &quot;time-step&quot;: 2100}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005339138209819794, &quot;time-step&quot;: 2101}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005406518932431936, &quot;time-step&quot;: 2102}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005323498509824276, &quot;time-step&quot;: 2103}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005390121601521969, &quot;time-step&quot;: 2104}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005307971965521574, &quot;time-step&quot;: 2105}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005373809486627579, &quot;time-step&quot;: 2106}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00529249245300889, &quot;time-step&quot;: 2107}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005357454065233469, &quot;time-step&quot;: 2108}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005277012474834919, &quot;time-step&quot;: 2109}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0053412653505802155, &quot;time-step&quot;: 2110}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005261670332401991, &quot;time-step&quot;: 2111}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005325144622474909, &quot;time-step&quot;: 2112}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005246362648904324, &quot;time-step&quot;: 2113}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005309063941240311, &quot;time-step&quot;: 2114}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005231109447777271, &quot;time-step&quot;: 2115}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0052930391393601894, &quot;time-step&quot;: 2116}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005215931683778763, &quot;time-step&quot;: 2117}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005277110263705254, &quot;time-step&quot;: 2118}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005200807936489582, &quot;time-step&quot;: 2119}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005261257756501436, &quot;time-step&quot;: 2120}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005185727030038834, &quot;time-step&quot;: 2121}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005245448090136051, &quot;time-step&quot;: 2122}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00517073180526495, &quot;time-step&quot;: 2123}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005229711998254061, &quot;time-step&quot;: 2124}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005155797116458416, &quot;time-step&quot;: 2125}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005214042961597443, &quot;time-step&quot;: 2126}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0051409173756837845, &quot;time-step&quot;: 2127}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005198458209633827, &quot;time-step&quot;: 2128}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005126094911247492, &quot;time-step&quot;: 2129}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005182893015444279, &quot;time-step&quot;: 2130}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005111325066536665, &quot;time-step&quot;: 2131}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0051674311980605125, &quot;time-step&quot;: 2132}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0050966073758900166, &quot;time-step&quot;: 2133}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0051519847474992275, &quot;time-step&quot;: 2134}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005081892944872379, &quot;time-step&quot;: 2135}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005136582069098949, &quot;time-step&quot;: 2136}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005067277234047651, &quot;time-step&quot;: 2137}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005121320486068726, &quot;time-step&quot;: 2138}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005052743013948202, &quot;time-step&quot;: 2139}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0051060826517641544, &quot;time-step&quot;: 2140}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005038246978074312, &quot;time-step&quot;: 2141}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005090933293104172, &quot;time-step&quot;: 2142}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005023860838264227, &quot;time-step&quot;: 2143}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0050758542492985725, &quot;time-step&quot;: 2144}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005009484943002462, &quot;time-step&quot;: 2145}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005060823634266853, &quot;time-step&quot;: 2146}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0049951449036598206, &quot;time-step&quot;: 2147}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0050458298064768314, &quot;time-step&quot;: 2148}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004980916623026133, &quot;time-step&quot;: 2149}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00503094308078289, &quot;time-step&quot;: 2150}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004966725595295429, &quot;time-step&quot;: 2151}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0050161173567175865, &quot;time-step&quot;: 2152}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004952599760144949, &quot;time-step&quot;: 2153}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.005001355893909931, &quot;time-step&quot;: 2154}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004938539583235979, &quot;time-step&quot;: 2155}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004986652173101902, &quot;time-step&quot;: 2156}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0049245283007621765, &quot;time-step&quot;: 2157}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004972022958099842, &quot;time-step&quot;: 2158}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0049105542711913586, &quot;time-step&quot;: 2159}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004957425408065319, &quot;time-step&quot;: 2160}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004896638449281454, &quot;time-step&quot;: 2161}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00494290329515934, &quot;time-step&quot;: 2162}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0048827603459358215, &quot;time-step&quot;: 2163}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004928414709866047, &quot;time-step&quot;: 2164}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00486897025257349, &quot;time-step&quot;: 2165}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004913998302072287, &quot;time-step&quot;: 2166}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004855198785662651, &quot;time-step&quot;: 2167}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004899626597762108, &quot;time-step&quot;: 2168}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0048414864577353, &quot;time-step&quot;: 2169}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004885321483016014, &quot;time-step&quot;: 2170}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0048278020694851875, &quot;time-step&quot;: 2171}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004871054086834192, &quot;time-step&quot;: 2172}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004814174026250839, &quot;time-step&quot;: 2173}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0048568397760391235, &quot;time-step&quot;: 2174}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0048006330616772175, &quot;time-step&quot;: 2175}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004842719063162804, &quot;time-step&quot;: 2176}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0047871265560388565, &quot;time-step&quot;: 2177}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00482866819947958, &quot;time-step&quot;: 2178}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004773750901222229, &quot;time-step&quot;: 2179}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004814718384295702, &quot;time-step&quot;: 2180}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0047603752464056015, &quot;time-step&quot;: 2181}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004800783004611731, &quot;time-step&quot;: 2182}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0047470517456531525, &quot;time-step&quot;: 2183}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004786856006830931, &quot;time-step&quot;: 2184}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004733739420771599, &quot;time-step&quot;: 2185}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004773052409291267, &quot;time-step&quot;: 2186}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0047205230221152306, &quot;time-step&quot;: 2187}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004759244155138731, &quot;time-step&quot;: 2188}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004707373678684235, &quot;time-step&quot;: 2189}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004745589103549719, &quot;time-step&quot;: 2190}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004694262519478798, &quot;time-step&quot;: 2191}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004731937311589718, &quot;time-step&quot;: 2192}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004681185353547335, &quot;time-step&quot;: 2193}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00471834558993578, &quot;time-step&quot;: 2194}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004668162204325199, &quot;time-step&quot;: 2195}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0047047752887010574, &quot;time-step&quot;: 2196}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004655197262763977, &quot;time-step&quot;: 2197}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004691267851740122, &quot;time-step&quot;: 2198}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004642252344638109, &quot;time-step&quot;: 2199}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004677834454923868, &quot;time-step&quot;: 2200}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004629385657608509, &quot;time-step&quot;: 2201}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004664449021220207, &quot;time-step&quot;: 2202}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0046165501698851585, &quot;time-step&quot;: 2203}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004651093855500221, &quot;time-step&quot;: 2204}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004603771027177572, &quot;time-step&quot;: 2205}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004637759644538164, &quot;time-step&quot;: 2206}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004590979311615229, &quot;time-step&quot;: 2207}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004624516237527132, &quot;time-step&quot;: 2208}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004578280728310347, &quot;time-step&quot;: 2209}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004611330572515726, &quot;time-step&quot;: 2210}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004565650597214699, &quot;time-step&quot;: 2211}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004598222207278013, &quot;time-step&quot;: 2212}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00455308984965086, &quot;time-step&quot;: 2213}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004585167858749628, &quot;time-step&quot;: 2214}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004540553316473961, &quot;time-step&quot;: 2215}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004572155885398388, &quot;time-step&quot;: 2216}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00452808290719986, &quot;time-step&quot;: 2217}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0045592146925628185, &quot;time-step&quot;: 2218}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0045156339183449745, &quot;time-step&quot;: 2219}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004546281415969133, &quot;time-step&quot;: 2220}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0045032501220703125, &quot;time-step&quot;: 2221}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004533437080681324, &quot;time-step&quot;: 2222}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004490940365940332, &quot;time-step&quot;: 2223}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00452068354934454, &quot;time-step&quot;: 2224}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004478659480810165, &quot;time-step&quot;: 2225}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004507926758378744, &quot;time-step&quot;: 2226}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004466412588953972, &quot;time-step&quot;: 2227}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004495246335864067, &quot;time-step&quot;: 2228}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004454245790839195, &quot;time-step&quot;: 2229}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004482615273445845, &quot;time-step&quot;: 2230}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0044420743361115456, &quot;time-step&quot;: 2231}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004469984211027622, &quot;time-step&quot;: 2232}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004429957363754511, &quot;time-step&quot;: 2233}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004457393195480108, &quot;time-step&quot;: 2234}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0044178650714457035, &quot;time-step&quot;: 2235}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0044448962435126305, &quot;time-step&quot;: 2236}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0044058579951524734, &quot;time-step&quot;: 2237}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00443245517089963, &quot;time-step&quot;: 2238}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004393878858536482, &quot;time-step&quot;: 2239}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004420044831931591, &quot;time-step&quot;: 2240}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004381946288049221, &quot;time-step&quot;: 2241}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0044076950289309025, &quot;time-step&quot;: 2242}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004370079841464758, &quot;time-step&quot;: 2243}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004395387135446072, &quot;time-step&quot;: 2244}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004358219914138317, &quot;time-step&quot;: 2245}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004383107181638479, &quot;time-step&quot;: 2246}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004346431698650122, &quot;time-step&quot;: 2247}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004370903130620718, &quot;time-step&quot;: 2248}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004334662109613419, &quot;time-step&quot;: 2249}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004358734469860792, &quot;time-step&quot;: 2250}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004322986584156752, &quot;time-step&quot;: 2251}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004346692468971014, &quot;time-step&quot;: 2252}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004311359021812677, &quot;time-step&quot;: 2253}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00433461694046855, &quot;time-step&quot;: 2254}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004299727734178305, &quot;time-step&quot;: 2255}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004322563298046589, &quot;time-step&quot;: 2256}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004288112744688988, &quot;time-step&quot;: 2257}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004310606047511101, &quot;time-step&quot;: 2258}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004276589956134558, &quot;time-step&quot;: 2259}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00429869070649147, &quot;time-step&quot;: 2260}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004265138879418373, &quot;time-step&quot;: 2261}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004286793991923332, &quot;time-step&quot;: 2262}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00425363564863801, &quot;time-step&quot;: 2263}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004274945706129074, &quot;time-step&quot;: 2264}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00424223905429244, &quot;time-step&quot;: 2265}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004263159818947315, &quot;time-step&quot;: 2266}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004230894614011049, &quot;time-step&quot;: 2267}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004251427948474884, &quot;time-step&quot;: 2268}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004219566937536001, &quot;time-step&quot;: 2269}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0042397817596793175, &quot;time-step&quot;: 2270}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004208365920931101, &quot;time-step&quot;: 2271}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004228188190609217, &quot;time-step&quot;: 2272}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0041971649043262005, &quot;time-step&quot;: 2273}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004216591361910105, &quot;time-step&quot;: 2274}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004185977391898632, &quot;time-step&quot;: 2275}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00420505553483963, &quot;time-step&quot;: 2276}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004174846690148115, &quot;time-step&quot;: 2277}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00419356394559145, &quot;time-step&quot;: 2278}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00416375370696187, &quot;time-step&quot;: 2279}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004182079806923866, &quot;time-step&quot;: 2280}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00415265467017889, &quot;time-step&quot;: 2281}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004170677158981562, &quot;time-step&quot;: 2282}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004141639452427626, &quot;time-step&quot;: 2283}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004159281961619854, &quot;time-step&quot;: 2284}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0041306233033537865, &quot;time-step&quot;: 2285}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004147905856370926, &quot;time-step&quot;: 2286}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004119659774005413, &quot;time-step&quot;: 2287}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004136630333960056, &quot;time-step&quot;: 2288}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004108768422156572, &quot;time-step&quot;: 2289}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004125392064452171, &quot;time-step&quot;: 2290}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004097946919500828, &quot;time-step&quot;: 2291}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004114230629056692, &quot;time-step&quot;: 2292}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004087093286216259, &quot;time-step&quot;: 2293}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00410302123054862, &quot;time-step&quot;: 2294}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004076266661286354, &quot;time-step&quot;: 2295}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0040918514132499695, &quot;time-step&quot;: 2296}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004065486136823893, &quot;time-step&quot;: 2297}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004080779850482941, &quot;time-step&quot;: 2298}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00405478710308671, &quot;time-step&quot;: 2299}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004069735761731863, &quot;time-step&quot;: 2300}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0040441351011395454, &quot;time-step&quot;: 2301}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004058788064867258, &quot;time-step&quot;: 2302}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004033524077385664, &quot;time-step&quot;: 2303}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0040478273294866085, &quot;time-step&quot;: 2304}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004022910725325346, &quot;time-step&quot;: 2305}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004036933183670044, &quot;time-step&quot;: 2306}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004012379329651594, &quot;time-step&quot;: 2307}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004026089329272509, &quot;time-step&quot;: 2308}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004001888446509838, &quot;time-step&quot;: 2309}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0040152231231331825, &quot;time-step&quot;: 2310}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003991383593529463, &quot;time-step&quot;: 2311}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.004004483576864004, &quot;time-step&quot;: 2312}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003980968147516251, &quot;time-step&quot;: 2313}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003993726335465908, &quot;time-step&quot;: 2314}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003970569930970669, &quot;time-step&quot;: 2315}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0039830394089221954, &quot;time-step&quot;: 2316}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003960207104682922, &quot;time-step&quot;: 2317}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003972351551055908, &quot;time-step&quot;: 2318}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003949877806007862, &quot;time-step&quot;: 2319}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003961741924285889, &quot;time-step&quot;: 2320}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003939573187381029, &quot;time-step&quot;: 2321}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003951141145080328, &quot;time-step&quot;: 2322}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00392930768430233, &quot;time-step&quot;: 2323}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003940606489777565, &quot;time-step&quot;: 2324}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003919119015336037, &quot;time-step&quot;: 2325}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003930141683667898, &quot;time-step&quot;: 2326}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0039089578203856945, &quot;time-step&quot;: 2327}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003919678740203381, &quot;time-step&quot;: 2328}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0038988403975963593, &quot;time-step&quot;: 2329}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003909297753125429, &quot;time-step&quot;: 2330}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0038887308910489082, &quot;time-step&quot;: 2331}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0038988797459751368, &quot;time-step&quot;: 2332}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0038786304648965597, &quot;time-step&quot;: 2333}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0038884789682924747, &quot;time-step&quot;: 2334}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003868542145937681, &quot;time-step&quot;: 2335}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0038781235925853252, &quot;time-step&quot;: 2336}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0038585131987929344, &quot;time-step&quot;: 2337}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003867814550176263, &quot;time-step&quot;: 2338}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0038485098630189896, &quot;time-step&quot;: 2339}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003857589792460203, &quot;time-step&quot;: 2340}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0038385905791074038, &quot;time-step&quot;: 2341}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003847368760034442, &quot;time-step&quot;: 2342}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0038286822382360697, &quot;time-step&quot;: 2343}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0038372152484953403, &quot;time-step&quot;: 2344}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0038188081234693527, &quot;time-step&quot;: 2345}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0038270624354481697, &quot;time-step&quot;: 2346}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003808970097452402, &quot;time-step&quot;: 2347}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003816966200247407, &quot;time-step&quot;: 2348}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003799146506935358, &quot;time-step&quot;: 2349}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0038068995345383883, &quot;time-step&quot;: 2350}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003789417678490281, &quot;time-step&quot;: 2351}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0037969232071191072, &quot;time-step&quot;: 2352}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003779694205150008, &quot;time-step&quot;: 2353}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0037869377993047237, &quot;time-step&quot;: 2354}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0037699751555919647, &quot;time-step&quot;: 2355}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0037769810296595097, &quot;time-step&quot;: 2356}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0037603178061544895, &quot;time-step&quot;: 2357}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003767054993659258, &quot;time-step&quot;: 2358}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003750663483515382, &quot;time-step&quot;: 2359}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0037571671418845654, &quot;time-step&quot;: 2360}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0037410841323435307, &quot;time-step&quot;: 2361}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0037473468109965324, &quot;time-step&quot;: 2362}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003731498261913657, &quot;time-step&quot;: 2363}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0037375143729150295, &quot;time-step&quot;: 2364}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0037219710648059845, &quot;time-step&quot;: 2365}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0037277517840266228, &quot;time-step&quot;: 2366}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0037124562077224255, &quot;time-step&quot;: 2367}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003718032967299223, &quot;time-step&quot;: 2368}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0037029925733804703, &quot;time-step&quot;: 2369}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003708341158926487, &quot;time-step&quot;: 2370}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0036935494281351566, &quot;time-step&quot;: 2371}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0036986302584409714, &quot;time-step&quot;: 2372}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003684095572680235, &quot;time-step&quot;: 2373}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0036889463663101196, &quot;time-step&quot;: 2374}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003674685023725033, &quot;time-step&quot;: 2375}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00367930019274354, &quot;time-step&quot;: 2376}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003665281692519784, &quot;time-step&quot;: 2377}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003669699653983116, &quot;time-step&quot;: 2378}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0036559617146849632, &quot;time-step&quot;: 2379}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003660169430077076, &quot;time-step&quot;: 2380}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0036466645542532206, &quot;time-step&quot;: 2381}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0036506380420178175, &quot;time-step&quot;: 2382}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003637386951595545, &quot;time-step&quot;: 2383}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0036411467008292675, &quot;time-step&quot;: 2384}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00362817058339715, &quot;time-step&quot;: 2385}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003631698666140437, &quot;time-step&quot;: 2386}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0036189304664731026, &quot;time-step&quot;: 2387}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003622243879362941, &quot;time-step&quot;: 2388}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003609702456742525, &quot;time-step&quot;: 2389}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0036128084175288677, &quot;time-step&quot;: 2390}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0036005349829792976, &quot;time-step&quot;: 2391}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0036034637596458197, &quot;time-step&quot;: 2392}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003591405227780342, &quot;time-step&quot;: 2393}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003594097448512912, &quot;time-step&quot;: 2394}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003582307603210211, &quot;time-step&quot;: 2395}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003584827994927764, &quot;time-step&quot;: 2396}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003573280991986394, &quot;time-step&quot;: 2397}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0035755906719714403, &quot;time-step&quot;: 2398}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003564247628673911, &quot;time-step&quot;: 2399}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0035663479939103127, &quot;time-step&quot;: 2400}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0035551812034100294, &quot;time-step&quot;: 2401}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0035570813342928886, &quot;time-step&quot;: 2402}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003546204185113311, &quot;time-step&quot;: 2403}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0035478821955621243, &quot;time-step&quot;: 2404}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003537161508575082, &quot;time-step&quot;: 2405}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0035386879462748766, &quot;time-step&quot;: 2406}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0035282380413264036, &quot;time-step&quot;: 2407}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003529590554535389, &quot;time-step&quot;: 2408}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0035193185321986675, &quot;time-step&quot;: 2409}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003520464990288019, &quot;time-step&quot;: 2410}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003510409966111183, &quot;time-step&quot;: 2411}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0035113911144435406, &quot;time-step&quot;: 2412}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0035015419125556946, &quot;time-step&quot;: 2413}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003502295818179846, &quot;time-step&quot;: 2414}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003492655698210001, &quot;time-step&quot;: 2415}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0034932619892060757, &quot;time-step&quot;: 2416}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0034838642459362745, &quot;time-step&quot;: 2417}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003484271001070738, &quot;time-step&quot;: 2418}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0034750388003885746, &quot;time-step&quot;: 2419}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00347528257407248, &quot;time-step&quot;: 2420}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0034662873949855566, &quot;time-step&quot;: 2421}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003466350259259343, &quot;time-step&quot;: 2422}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0034575590398162603, &quot;time-step&quot;: 2423}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003457456361502409, &quot;time-step&quot;: 2424}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003448845585808158, &quot;time-step&quot;: 2425}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0034485631622374058, &quot;time-step&quot;: 2426}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00344009418040514, &quot;time-step&quot;: 2427}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0034396543633192778, &quot;time-step&quot;: 2428}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003431442892178893, &quot;time-step&quot;: 2429}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003430891316384077, &quot;time-step&quot;: 2430}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003422841429710388, &quot;time-step&quot;: 2431}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003422064008191228, &quot;time-step&quot;: 2432}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0034142020158469677, &quot;time-step&quot;: 2433}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0034132699947804213, &quot;time-step&quot;: 2434}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0034056054428219795, &quot;time-step&quot;: 2435}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0034044990316033363, &quot;time-step&quot;: 2436}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0033970128279179335, &quot;time-step&quot;: 2437}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0033957839477807283, &quot;time-step&quot;: 2438}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003388485638424754, &quot;time-step&quot;: 2439}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003387089353054762, &quot;time-step&quot;: 2440}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0033799505326896906, &quot;time-step&quot;: 2441}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003378399880602956, &quot;time-step&quot;: 2442}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003371421480551362, &quot;time-step&quot;: 2443}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0033697374165058136, &quot;time-step&quot;: 2444}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0033629557583481073, &quot;time-step&quot;: 2445}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0033611226826906204, &quot;time-step&quot;: 2446}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0033544888719916344, &quot;time-step&quot;: 2447}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003352539148181677, &quot;time-step&quot;: 2448}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003346112323924899, &quot;time-step&quot;: 2449}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003344035940244794, &quot;time-step&quot;: 2450}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0033377737272530794, &quot;time-step&quot;: 2451}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0033355168998241425, &quot;time-step&quot;: 2452}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0033294097520411015, &quot;time-step&quot;: 2453}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0033270670101046562, &quot;time-step&quot;: 2454}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003321101889014244, &quot;time-step&quot;: 2455}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0033185970969498158, &quot;time-step&quot;: 2456}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0033128135837614536, &quot;time-step&quot;: 2457}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0033102035522460938, &quot;time-step&quot;: 2458}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0033045883756130934, &quot;time-step&quot;: 2459}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0033018947578966618, &quot;time-step&quot;: 2460}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003296440467238426, &quot;time-step&quot;: 2461}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0032935969065874815, &quot;time-step&quot;: 2462}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003288313979282975, &quot;time-step&quot;: 2463}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0032853714656084776, &quot;time-step&quot;: 2464}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003280248725786805, &quot;time-step&quot;: 2465}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00327717955224216, &quot;time-step&quot;: 2466}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0032722316682338715, &quot;time-step&quot;: 2467}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0032690633088350296, &quot;time-step&quot;: 2468}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0032642812002450228, &quot;time-step&quot;: 2469}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0032610187772661448, &quot;time-step&quot;: 2470}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003256391966715455, &quot;time-step&quot;: 2471}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003252999857068062, &quot;time-step&quot;: 2472}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003248540684580803, &quot;time-step&quot;: 2473}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003245045430958271, &quot;time-step&quot;: 2474}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0032407541293650866, &quot;time-step&quot;: 2475}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0032371939159929752, &quot;time-step&quot;: 2476}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003233032999560237, &quot;time-step&quot;: 2477}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003229366149753332, &quot;time-step&quot;: 2478}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0032253731042146683, &quot;time-step&quot;: 2479}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003221612423658371, &quot;time-step&quot;: 2480}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003217805176973343, &quot;time-step&quot;: 2481}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003213941352441907, &quot;time-step&quot;: 2482}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003210279159247875, &quot;time-step&quot;: 2483}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0032063063699752092, &quot;time-step&quot;: 2484}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0032028069254010916, &quot;time-step&quot;: 2485}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003198751248419285, &quot;time-step&quot;: 2486}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0031953901052474976, &quot;time-step&quot;: 2487}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003191236173734069, &quot;time-step&quot;: 2488}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0031879935413599014, &quot;time-step&quot;: 2489}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0031837308779358864, &quot;time-step&quot;: 2490}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0031806686893105507, &quot;time-step&quot;: 2491}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0031763436272740364, &quot;time-step&quot;: 2492}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003173429286107421, &quot;time-step&quot;: 2493}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0031689973548054695, &quot;time-step&quot;: 2494}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0031662210822105408, &quot;time-step&quot;: 2495}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003161713248118758, &quot;time-step&quot;: 2496}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003159087151288986, &quot;time-step&quot;: 2497}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0031544314697384834, &quot;time-step&quot;: 2498}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0031519487965852022, &quot;time-step&quot;: 2499}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003147243056446314, &quot;time-step&quot;: 2500}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003144897986203432, &quot;time-step&quot;: 2501}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0031400849111378193, &quot;time-step&quot;: 2502}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003137868596240878, &quot;time-step&quot;: 2503}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003132941434159875, &quot;time-step&quot;: 2504}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003130868077278137, &quot;time-step&quot;: 2505}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0031258398666977882, &quot;time-step&quot;: 2506}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0031238640658557415, &quot;time-step&quot;: 2507}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003118740161880851, &quot;time-step&quot;: 2508}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003116914536803961, &quot;time-step&quot;: 2509}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003111722646281123, &quot;time-step&quot;: 2510}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00311002298258245, &quot;time-step&quot;: 2511}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0031047258526086807, &quot;time-step&quot;: 2512}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003103183815255761, &quot;time-step&quot;: 2513}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003097774926573038, &quot;time-step&quot;: 2514}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003096325322985649, &quot;time-step&quot;: 2515}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003090800018981099, &quot;time-step&quot;: 2516}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003089462872594595, &quot;time-step&quot;: 2517}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0030838805250823498, &quot;time-step&quot;: 2518}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003082692390307784, &quot;time-step&quot;: 2519}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003077024593949318, &quot;time-step&quot;: 2520}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0030759612563997507, &quot;time-step&quot;: 2521}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0030701530631631613, &quot;time-step&quot;: 2522}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0030691740103065968, &quot;time-step&quot;: 2523}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0030632754787802696, &quot;time-step&quot;: 2524}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00306245987303555, &quot;time-step&quot;: 2525}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003056492656469345, &quot;time-step&quot;: 2526}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0030557741411030293, &quot;time-step&quot;: 2527}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0030496963299810886, &quot;time-step&quot;: 2528}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003049073973670602, &quot;time-step&quot;: 2529}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0030429151374846697, &quot;time-step&quot;: 2530}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0030424196738749743, &quot;time-step&quot;: 2531}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0030361642129719257, &quot;time-step&quot;: 2532}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0030357895884662867, &quot;time-step&quot;: 2533}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0030294423922896385, &quot;time-step&quot;: 2534}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0030291832517832518, &quot;time-step&quot;: 2535}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0030227454844862223, &quot;time-step&quot;: 2536}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0030226141680032015, &quot;time-step&quot;: 2537}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003016085596755147, &quot;time-step&quot;: 2538}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0030160353053361177, &quot;time-step&quot;: 2539}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0030094124376773834, &quot;time-step&quot;: 2540}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0030094680842012167, &quot;time-step&quot;: 2541}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0030027474276721478, &quot;time-step&quot;: 2542}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.003002940444275737, &quot;time-step&quot;: 2543}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002996144350618124, &quot;time-step&quot;: 2544}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0029964670538902283, &quot;time-step&quot;: 2545}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0029895741026848555, &quot;time-step&quot;: 2546}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0029899962246418, &quot;time-step&quot;: 2547}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002983053447678685, &quot;time-step&quot;: 2548}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0029835570603609085, &quot;time-step&quot;: 2549}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0029765439685434103, &quot;time-step&quot;: 2550}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002977171912789345, &quot;time-step&quot;: 2551}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002970006549730897, &quot;time-step&quot;: 2552}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002970711560919881, &quot;time-step&quot;: 2553}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0029634935781359673, &quot;time-step&quot;: 2554}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002964335260912776, &quot;time-step&quot;: 2555}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002957048127427697, &quot;time-step&quot;: 2556}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002957974560558796, &quot;time-step&quot;: 2557}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0029505854472517967, &quot;time-step&quot;: 2558}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0029515819624066353, &quot;time-step&quot;: 2559}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0029441057704389095, &quot;time-step&quot;: 2560}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0029452359303832054, &quot;time-step&quot;: 2561}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0029377087485045195, &quot;time-step&quot;: 2562}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0029389429837465286, &quot;time-step&quot;: 2563}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002931330818682909, &quot;time-step&quot;: 2564}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002932673553004861, &quot;time-step&quot;: 2565}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0029249624349176884, &quot;time-step&quot;: 2566}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002926390618085861, &quot;time-step&quot;: 2567}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0029186150059103966, &quot;time-step&quot;: 2568}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002920137718319893, &quot;time-step&quot;: 2569}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0029122689738869667, &quot;time-step&quot;: 2570}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0029138843528926373, &quot;time-step&quot;: 2571}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002905944362282753, &quot;time-step&quot;: 2572}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0029076894279569387, &quot;time-step&quot;: 2573}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002899679820984602, &quot;time-step&quot;: 2574}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002901468425989151, &quot;time-step&quot;: 2575}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002893387572839856, &quot;time-step&quot;: 2576}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0028953026048839092, &quot;time-step&quot;: 2577}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0028871544636785984, &quot;time-step&quot;: 2578}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002889165887609124, &quot;time-step&quot;: 2579}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0028809551149606705, &quot;time-step&quot;: 2580}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0028830391820520163, &quot;time-step&quot;: 2581}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002874712459743023, &quot;time-step&quot;: 2582}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002876891987398267, &quot;time-step&quot;: 2583}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0028685168363153934, &quot;time-step&quot;: 2584}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0028708046302199364, &quot;time-step&quot;: 2585}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0028623640537261963, &quot;time-step&quot;: 2586}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0028647282160818577, &quot;time-step&quot;: 2587}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002856220118701458, &quot;time-step&quot;: 2588}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0028587111737579107, &quot;time-step&quot;: 2589}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002850132528692484, &quot;time-step&quot;: 2590}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0028527299873530865, &quot;time-step&quot;: 2591}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00284411758184433, &quot;time-step&quot;: 2592}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0028467969968914986, &quot;time-step&quot;: 2593}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002838080981746316, &quot;time-step&quot;: 2594}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0028408162761479616, &quot;time-step&quot;: 2595}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0028320166748017073, &quot;time-step&quot;: 2596}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0028348425403237343, &quot;time-step&quot;: 2597}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0028259954415261745, &quot;time-step&quot;: 2598}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0028289291076362133, &quot;time-step&quot;: 2599}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002820006338879466, &quot;time-step&quot;: 2600}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0028229833114892244, &quot;time-step&quot;: 2601}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0028139720670878887, &quot;time-step&quot;: 2602}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0028170219156891108, &quot;time-step&quot;: 2603}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002807960147038102, &quot;time-step&quot;: 2604}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0028111147694289684, &quot;time-step&quot;: 2605}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002801977563649416, &quot;time-step&quot;: 2606}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0028052187990397215, &quot;time-step&quot;: 2607}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002796051325276494, &quot;time-step&quot;: 2608}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002799385692924261, &quot;time-step&quot;: 2609}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0027901241555809975, &quot;time-step&quot;: 2610}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0027935646940022707, &quot;time-step&quot;: 2611}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0027842805720865726, &quot;time-step&quot;: 2612}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002787795150652528, &quot;time-step&quot;: 2613}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0027784372214227915, &quot;time-step&quot;: 2614}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0027820419054478407, &quot;time-step&quot;: 2615}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002772608771920204, &quot;time-step&quot;: 2616}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002776257460936904, &quot;time-step&quot;: 2617}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0027667616959661245, &quot;time-step&quot;: 2618}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002770512131974101, &quot;time-step&quot;: 2619}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0027609493117779493, &quot;time-step&quot;: 2620}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002764791017398238, &quot;time-step&quot;: 2621}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002755179535597563, &quot;time-step&quot;: 2622}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0027590792160481215, &quot;time-step&quot;: 2623}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002749413251876831, &quot;time-step&quot;: 2624}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0027534067630767822, &quot;time-step&quot;: 2625}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0027436716482043266, &quot;time-step&quot;: 2626}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0027477561961859465, &quot;time-step&quot;: 2627}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002737964503467083, &quot;time-step&quot;: 2628}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0027421119157224894, &quot;time-step&quot;: 2629}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0027322652749717236, &quot;time-step&quot;: 2630}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002736467868089676, &quot;time-step&quot;: 2631}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0027265651151537895, &quot;time-step&quot;: 2632}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0027308575809001923, &quot;time-step&quot;: 2633}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00272089964710176, &quot;time-step&quot;: 2634}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002725271973758936, &quot;time-step&quot;: 2635}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0027152695693075657, &quot;time-step&quot;: 2636}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00271970103494823, &quot;time-step&quot;: 2637}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002709611551836133, &quot;time-step&quot;: 2638}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00271410821005702, &quot;time-step&quot;: 2639}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00270399684086442, &quot;time-step&quot;: 2640}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002708598505705595, &quot;time-step&quot;: 2641}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002698424505069852, &quot;time-step&quot;: 2642}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002703102072700858, &quot;time-step&quot;: 2643}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002692870330065489, &quot;time-step&quot;: 2644}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0026976163499057293, &quot;time-step&quot;: 2645}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0026873392052948475, &quot;time-step&quot;: 2646}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002692130161449313, &quot;time-step&quot;: 2647}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002681792713701725, &quot;time-step&quot;: 2648}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0026866821572184563, &quot;time-step&quot;: 2649}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0026762892957776785, &quot;time-step&quot;: 2650}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002681228332221508, &quot;time-step&quot;: 2651}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0026707923971116543, &quot;time-step&quot;: 2652}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002675787080079317, &quot;time-step&quot;: 2653}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0026652682572603226, &quot;time-step&quot;: 2654}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0026703353505581617, &quot;time-step&quot;: 2655}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0026598067488521338, &quot;time-step&quot;: 2656}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0026649765204638243, &quot;time-step&quot;: 2657}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002654371317476034, &quot;time-step&quot;: 2658}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0026595669332891703, &quot;time-step&quot;: 2659}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0026489305309951305, &quot;time-step&quot;: 2660}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0026542090345174074, &quot;time-step&quot;: 2661}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002643533516675234, &quot;time-step&quot;: 2662}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0026488848961889744, &quot;time-step&quot;: 2663}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002638156060129404, &quot;time-step&quot;: 2664}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0026435512118041515, &quot;time-step&quot;: 2665}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002632784191519022, &quot;time-step&quot;: 2666}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002638254314661026, &quot;time-step&quot;: 2667}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0026274321135133505, &quot;time-step&quot;: 2668}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0026329767424613237, &quot;time-step&quot;: 2669}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0026221037842333317, &quot;time-step&quot;: 2670}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0026277038268744946, &quot;time-step&quot;: 2671}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00261678546667099, &quot;time-step&quot;: 2672}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002622464206069708, &quot;time-step&quot;: 2673}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0026114950887858868, &quot;time-step&quot;: 2674}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0026172276120632887, &quot;time-step&quot;: 2675}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0026062210090458393, &quot;time-step&quot;: 2676}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002612013602629304, &quot;time-step&quot;: 2677}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0026009632274508476, &quot;time-step&quot;: 2678}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0026068075094372034, &quot;time-step&quot;: 2679}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0025956938043236732, &quot;time-step&quot;: 2680}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0026016116607934237, &quot;time-step&quot;: 2681}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002590468619018793, &quot;time-step&quot;: 2682}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0025964502710849047, &quot;time-step&quot;: 2683}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0025852511171251535, &quot;time-step&quot;: 2684}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002591286087408662, &quot;time-step&quot;: 2685}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002580077853053808, &quot;time-step&quot;: 2686}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002586185932159424, &quot;time-step&quot;: 2687}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0025749343913048506, &quot;time-step&quot;: 2688}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002581121399998665, &quot;time-step&quot;: 2689}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0025697967503219843, &quot;time-step&quot;: 2690}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0025760007556527853, &quot;time-step&quot;: 2691}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002564632333815098, &quot;time-step&quot;: 2692}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0025709220208227634, &quot;time-step&quot;: 2693}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0025595128536224365, &quot;time-step&quot;: 2694}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0025658365339040756, &quot;time-step&quot;: 2695}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002554391510784626, &quot;time-step&quot;: 2696}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002560768276453018, &quot;time-step&quot;: 2697}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002549287397414446, &quot;time-step&quot;: 2698}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002555716782808304, &quot;time-step&quot;: 2699}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002544195158407092, &quot;time-step&quot;: 2700}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0025507016107439995, &quot;time-step&quot;: 2701}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0025391385424882174, &quot;time-step&quot;: 2702}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0025457185693085194, &quot;time-step&quot;: 2703}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0025341457221657038, &quot;time-step&quot;: 2704}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002540756016969681, &quot;time-step&quot;: 2705}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0025291331112384796, &quot;time-step&quot;: 2706}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0025358162820339203, &quot;time-step&quot;: 2707}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002524150302633643, &quot;time-step&quot;: 2708}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0025308928452432156, &quot;time-step&quot;: 2709}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0025192105676978827, &quot;time-step&quot;: 2710}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0025259966496378183, &quot;time-step&quot;: 2711}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0025142456870526075, &quot;time-step&quot;: 2712}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002521062269806862, &quot;time-step&quot;: 2713}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002509255660697818, &quot;time-step&quot;: 2714}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0025161351077258587, &quot;time-step&quot;: 2715}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0025043173227459192, &quot;time-step&quot;: 2716}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0025112612638622522, &quot;time-step&quot;: 2717}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0024994106497615576, &quot;time-step&quot;: 2718}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002506389981135726, &quot;time-step&quot;: 2719}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0024945067707449198, &quot;time-step&quot;: 2720}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002501542679965496, &quot;time-step&quot;: 2721}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002489596838131547, &quot;time-step&quot;: 2722}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0024966790806502104, &quot;time-step&quot;: 2723}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0024847236927598715, &quot;time-step&quot;: 2724}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0024918578565120697, &quot;time-step&quot;: 2725}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002479858696460724, &quot;time-step&quot;: 2726}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0024870517663657665, &quot;time-step&quot;: 2727}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0024750265292823315, &quot;time-step&quot;: 2728}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002482285024598241, &quot;time-step&quot;: 2729}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002470224630087614, &quot;time-step&quot;: 2730}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002477518515661359, &quot;time-step&quot;: 2731}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0024654266890138388, &quot;time-step&quot;: 2732}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0024727554991841316, &quot;time-step&quot;: 2733}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0024606038350611925, &quot;time-step&quot;: 2734}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0024679885245859623, &quot;time-step&quot;: 2735}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0024558145087212324, &quot;time-step&quot;: 2736}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00246324110776186, &quot;time-step&quot;: 2737}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0024510552175343037, &quot;time-step&quot;: 2738}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002458535134792328, &quot;time-step&quot;: 2739}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002446312690153718, &quot;time-step&quot;: 2740}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002453843131661415, &quot;time-step&quot;: 2741}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0024415794759988785, &quot;time-step&quot;: 2742}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002449165331199765, &quot;time-step&quot;: 2743}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0024368923623114824, &quot;time-step&quot;: 2744}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002444501267746091, &quot;time-step&quot;: 2745}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002432163106277585, &quot;time-step&quot;: 2746}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002439821371808648, &quot;time-step&quot;: 2747}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002427462488412857, &quot;time-step&quot;: 2748}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0024351938627660275, &quot;time-step&quot;: 2749}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0024228342808783054, &quot;time-step&quot;: 2750}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0024306043051183224, &quot;time-step&quot;: 2751}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002418214688077569, &quot;time-step&quot;: 2752}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0024260191712528467, &quot;time-step&quot;: 2753}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00241358345374465, &quot;time-step&quot;: 2754}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0024214359000325203, &quot;time-step&quot;: 2755}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002408974338322878, &quot;time-step&quot;: 2756}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0024168696254491806, &quot;time-step&quot;: 2757}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0024043796584010124, &quot;time-step&quot;: 2758}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002412309404462576, &quot;time-step&quot;: 2759}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0023997719399631023, &quot;time-step&quot;: 2760}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0024077552370727062, &quot;time-step&quot;: 2761}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002395212184637785, &quot;time-step&quot;: 2762}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002403222257271409, &quot;time-step&quot;: 2763}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002390634035691619, &quot;time-step&quot;: 2764}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002398681128397584, &quot;time-step&quot;: 2765}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0023860586807131767, &quot;time-step&quot;: 2766}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002394177718088031, &quot;time-step&quot;: 2767}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0023815312888473272, &quot;time-step&quot;: 2768}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00238966871984303, &quot;time-step&quot;: 2769}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0023769850376993418, &quot;time-step&quot;: 2770}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002385176485404372, &quot;time-step&quot;: 2771}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002372484188526869, &quot;time-step&quot;: 2772}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002380707301199436, &quot;time-step&quot;: 2773}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002367980545386672, &quot;time-step&quot;: 2774}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0023762318305671215, &quot;time-step&quot;: 2775}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0023634834215044975, &quot;time-step&quot;: 2776}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0023718010634183884, &quot;time-step&quot;: 2777}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0023590160999447107, &quot;time-step&quot;: 2778}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0023673411924391985, &quot;time-step&quot;: 2779}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0023545543663203716, &quot;time-step&quot;: 2780}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002362930215895176, &quot;time-step&quot;: 2781}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002350108465179801, &quot;time-step&quot;: 2782}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0023585122544318438, &quot;time-step&quot;: 2783}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002345672808587551, &quot;time-step&quot;: 2784}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0023541483096778393, &quot;time-step&quot;: 2785}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0023412832524627447, &quot;time-step&quot;: 2786}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002349790185689926, &quot;time-step&quot;: 2787}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0023368981201201677, &quot;time-step&quot;: 2788}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002345446962863207, &quot;time-step&quot;: 2789}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00233254861086607, &quot;time-step&quot;: 2790}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0023411326110363007, &quot;time-step&quot;: 2791}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0023281779140233994, &quot;time-step&quot;: 2792}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0023367826361209154, &quot;time-step&quot;: 2793}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002323838183656335, &quot;time-step&quot;: 2794}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0023324834182858467, &quot;time-step&quot;: 2795}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0023194930981844664, &quot;time-step&quot;: 2796}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002328191651031375, &quot;time-step&quot;: 2797}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0023151752538979053, &quot;time-step&quot;: 2798}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0023238887079060078, &quot;time-step&quot;: 2799}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002310853684321046, &quot;time-step&quot;: 2800}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002319596242159605, &quot;time-step&quot;: 2801}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0023065374698489904, &quot;time-step&quot;: 2802}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0023153505753725767, &quot;time-step&quot;: 2803}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002302283188328147, &quot;time-step&quot;: 2804}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0023110925685614347, &quot;time-step&quot;: 2805}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00229798280633986, &quot;time-step&quot;: 2806}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002306840382516384, &quot;time-step&quot;: 2807}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0022937231697142124, &quot;time-step&quot;: 2808}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0023026387207210064, &quot;time-step&quot;: 2809}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002289505209773779, &quot;time-step&quot;: 2810}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002298434730619192, &quot;time-step&quot;: 2811}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0022852683905512094, &quot;time-step&quot;: 2812}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0022942356299608946, &quot;time-step&quot;: 2813}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0022810758091509342, &quot;time-step&quot;: 2814}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002290072152391076, &quot;time-step&quot;: 2815}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002276868559420109, &quot;time-step&quot;: 2816}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002285891678184271, &quot;time-step&quot;: 2817}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002272660844027996, &quot;time-step&quot;: 2818}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002281717024743557, &quot;time-step&quot;: 2819}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0022684915456920862, &quot;time-step&quot;: 2820}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002277601743116975, &quot;time-step&quot;: 2821}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002264325274154544, &quot;time-step&quot;: 2822}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002273442456498742, &quot;time-step&quot;: 2823}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002260172041133046, &quot;time-step&quot;: 2824}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002269335091114044, &quot;time-step&quot;: 2825}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002256042556837201, &quot;time-step&quot;: 2826}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0022652309853583574, &quot;time-step&quot;: 2827}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0022519046906381845, &quot;time-step&quot;: 2828}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0022611222229897976, &quot;time-step&quot;: 2829}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002247805241495371, &quot;time-step&quot;: 2830}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0022570653818547726, &quot;time-step&quot;: 2831}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0022437232546508312, &quot;time-step&quot;: 2832}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002253003418445587, &quot;time-step&quot;: 2833}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002239614725112915, &quot;time-step&quot;: 2834}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0022489342372864485, &quot;time-step&quot;: 2835}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0022355576511472464, &quot;time-step&quot;: 2836}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002244888339191675, &quot;time-step&quot;: 2837}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002231488237157464, &quot;time-step&quot;: 2838}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0022408717777580023, &quot;time-step&quot;: 2839}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0022274598013609648, &quot;time-step&quot;: 2840}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0022368726786226034, &quot;time-step&quot;: 2841}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0022234327625483274, &quot;time-step&quot;: 2842}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002232857048511505, &quot;time-step&quot;: 2843}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002219398971647024, &quot;time-step&quot;: 2844}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0022288584150373936, &quot;time-step&quot;: 2845}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002215383108705282, &quot;time-step&quot;: 2846}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002224863739684224, &quot;time-step&quot;: 2847}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0022113891318440437, &quot;time-step&quot;: 2848}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0022209053859114647, &quot;time-step&quot;: 2849}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002207418903708458, &quot;time-step&quot;: 2850}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0022169656585901976, &quot;time-step&quot;: 2851}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0022034568246454, &quot;time-step&quot;: 2852}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0022130031138658524, &quot;time-step&quot;: 2853}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002199472626671195, &quot;time-step&quot;: 2854}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002209059428423643, &quot;time-step&quot;: 2855}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002195526845753193, &quot;time-step&quot;: 2856}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0022051676642149687, &quot;time-step&quot;: 2857}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002191595733165741, &quot;time-step&quot;: 2858}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00220125587657094, &quot;time-step&quot;: 2859}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0021876629907637835, &quot;time-step&quot;: 2860}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0021973170805722475, &quot;time-step&quot;: 2861}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002183707896620035, &quot;time-step&quot;: 2862}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00219340855255723, &quot;time-step&quot;: 2863}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00217980844900012, &quot;time-step&quot;: 2864}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002189546125009656, &quot;time-step&quot;: 2865}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002175932750105858, &quot;time-step&quot;: 2866}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002185707213357091, &quot;time-step&quot;: 2867}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0021720840595662594, &quot;time-step&quot;: 2868}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00218186411075294, &quot;time-step&quot;: 2869}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002168213715776801, &quot;time-step&quot;: 2870}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0021780263632535934, &quot;time-step&quot;: 2871}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0021643629297614098, &quot;time-step&quot;: 2872}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0021742028184235096, &quot;time-step&quot;: 2873}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00216052052564919, &quot;time-step&quot;: 2874}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0021703678648918867, &quot;time-step&quot;: 2875}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002156693022698164, &quot;time-step&quot;: 2876}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0021665971726179123, &quot;time-step&quot;: 2877}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0021529104560613632, &quot;time-step&quot;: 2878}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0021628327667713165, &quot;time-step&quot;: 2879}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0021491204388439655, &quot;time-step&quot;: 2880}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00215906067751348, &quot;time-step&quot;: 2881}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002145336475223303, &quot;time-step&quot;: 2882}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0021552867256104946, &quot;time-step&quot;: 2883}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0021415348164737225, &quot;time-step&quot;: 2884}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0021515008993446827, &quot;time-step&quot;: 2885}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002137732459232211, &quot;time-step&quot;: 2886}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0021477537229657173, &quot;time-step&quot;: 2887}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002133989240974188, &quot;time-step&quot;: 2888}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00214401469565928, &quot;time-step&quot;: 2889}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0021302185487002134, &quot;time-step&quot;: 2890}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0021402728743851185, &quot;time-step&quot;: 2891}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002126486739143729, &quot;time-step&quot;: 2892}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00213656690903008, &quot;time-step&quot;: 2893}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0021227803081274033, &quot;time-step&quot;: 2894}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002132866531610489, &quot;time-step&quot;: 2895}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0021190373227000237, &quot;time-step&quot;: 2896}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0021291570737957954, &quot;time-step&quot;: 2897}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0021153392735868692, &quot;time-step&quot;: 2898}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002125486731529236, &quot;time-step&quot;: 2899}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00211165240034461, &quot;time-step&quot;: 2900}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0021217975299805403, &quot;time-step&quot;: 2901}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002107930136844516, &quot;time-step&quot;: 2902}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0021181090269237757, &quot;time-step&quot;: 2903}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002104264684021473, &quot;time-step&quot;: 2904}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002114493167027831, &quot;time-step&quot;: 2905}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0021006499882787466, &quot;time-step&quot;: 2906}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0021109094377607107, &quot;time-step&quot;: 2907}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0020970574114471674, &quot;time-step&quot;: 2908}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0021073140669614077, &quot;time-step&quot;: 2909}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002093430608510971, &quot;time-step&quot;: 2910}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0021036898251622915, &quot;time-step&quot;: 2911}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002089802175760269, &quot;time-step&quot;: 2912}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002100096084177494, &quot;time-step&quot;: 2913}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0020861865486949682, &quot;time-step&quot;: 2914}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0020964902359992266, &quot;time-step&quot;: 2915}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002082565799355507, &quot;time-step&quot;: 2916}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002092893933877349, &quot;time-step&quot;: 2917}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0020789955742657185, &quot;time-step&quot;: 2918}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0020893511828035116, &quot;time-step&quot;: 2919}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0020754323340952396, &quot;time-step&quot;: 2920}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002085795160382986, &quot;time-step&quot;: 2921}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002071848837658763, &quot;time-step&quot;: 2922}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002082236809656024, &quot;time-step&quot;: 2923}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002068291651085019, &quot;time-step&quot;: 2924}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002078705932945013, &quot;time-step&quot;: 2925}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002064741915091872, &quot;time-step&quot;: 2926}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0020751620177179575, &quot;time-step&quot;: 2927}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0020612210500985384, &quot;time-step&quot;: 2928}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002071667928248644, &quot;time-step&quot;: 2929}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0020576752722263336, &quot;time-step&quot;: 2930}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0020681272726505995, &quot;time-step&quot;: 2931}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0020541450940072536, &quot;time-step&quot;: 2932}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002064642496407032, &quot;time-step&quot;: 2933}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0020506668370217085, &quot;time-step&quot;: 2934}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002061167499050498, &quot;time-step&quot;: 2935}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0020471783354878426, &quot;time-step&quot;: 2936}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002057707402855158, &quot;time-step&quot;: 2937}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0020437005441635847, &quot;time-step&quot;: 2938}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002054214710369706, &quot;time-step&quot;: 2939}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0020401899237185717, &quot;time-step&quot;: 2940}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0020507383160293102, &quot;time-step&quot;: 2941}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002036712598055601, &quot;time-step&quot;: 2942}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002047295216470957, &quot;time-step&quot;: 2943}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0020332783460617065, &quot;time-step&quot;: 2944}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0020438439678400755, &quot;time-step&quot;: 2945}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0020298033487051725, &quot;time-step&quot;: 2946}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0020404120441526175, &quot;time-step&quot;: 2947}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0020263760816305876, &quot;time-step&quot;: 2948}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0020369996782392263, &quot;time-step&quot;: 2949}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002022950677201152, &quot;time-step&quot;: 2950}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002033587545156479, &quot;time-step&quot;: 2951}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0020195282995700836, &quot;time-step&quot;: 2952}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0020301761105656624, &quot;time-step&quot;: 2953}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0020161019638180733, &quot;time-step&quot;: 2954}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002026746980845928, &quot;time-step&quot;: 2955}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002012668875977397, &quot;time-step&quot;: 2956}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0020233530085533857, &quot;time-step&quot;: 2957}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0020092909689992666, &quot;time-step&quot;: 2958}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00201999768614769, &quot;time-step&quot;: 2959}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0020059330854564905, &quot;time-step&quot;: 2960}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0020166554022580385, &quot;time-step&quot;: 2961}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.002002573339268565, &quot;time-step&quot;: 2962}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0020133061334490776, &quot;time-step&quot;: 2963}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001999208703637123, &quot;time-step&quot;: 2964}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00200994242914021, &quot;time-step&quot;: 2965}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0019958456978201866, &quot;time-step&quot;: 2966}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0020065989810973406, &quot;time-step&quot;: 2967}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001992485485970974, &quot;time-step&quot;: 2968}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0020032841712236404, &quot;time-step&quot;: 2969}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001989186042919755, &quot;time-step&quot;: 2970}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001999987056478858, &quot;time-step&quot;: 2971}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001985891256481409, &quot;time-step&quot;: 2972}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0019966892432421446, &quot;time-step&quot;: 2973}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0019825901836156845, &quot;time-step&quot;: 2974}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0019934228621423244, &quot;time-step&quot;: 2975}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0019793135579675436, &quot;time-step&quot;: 2976}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001990146469324827, &quot;time-step&quot;: 2977}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001976016676053405, &quot;time-step&quot;: 2978}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0019868542440235615, &quot;time-step&quot;: 2979}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001972716301679611, &quot;time-step&quot;: 2980}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001983576687052846, &quot;time-step&quot;: 2981}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0019694494549185038, &quot;time-step&quot;: 2982}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001980332424864173, &quot;time-step&quot;: 2983}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0019661770202219486, &quot;time-step&quot;: 2984}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001977081410586834, &quot;time-step&quot;: 2985}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001962966052815318, &quot;time-step&quot;: 2986}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0019738865084946156, &quot;time-step&quot;: 2987}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0019597476348280907, &quot;time-step&quot;: 2988}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0019706683233380318, &quot;time-step&quot;: 2989}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001956517808139324, &quot;time-step&quot;: 2990}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0019674510695040226, &quot;time-step&quot;: 2991}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0019533238373696804, &quot;time-step&quot;: 2992}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0019642645493149757, &quot;time-step&quot;: 2993}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0019500873750075698, &quot;time-step&quot;: 2994}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001961030066013336, &quot;time-step&quot;: 2995}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001946846372447908, &quot;time-step&quot;: 2996}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001957816071808338, &quot;time-step&quot;: 2997}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001943656476214528, &quot;time-step&quot;: 2998}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0019546635448932648, &quot;time-step&quot;: 2999}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001940506394021213, &quot;time-step&quot;: 3000}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001951513229869306, &quot;time-step&quot;: 3001}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0019373476970940828, &quot;time-step&quot;: 3002}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001948341727256775, &quot;time-step&quot;: 3003}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0019341728184372187, &quot;time-step&quot;: 3004}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0019452018896117806, &quot;time-step&quot;: 3005}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0019310098141431808, &quot;time-step&quot;: 3006}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0019420413300395012, &quot;time-step&quot;: 3007}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0019278540275990963, &quot;time-step&quot;: 3008}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0019388983491808176, &quot;time-step&quot;: 3009}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0019247194286435843, &quot;time-step&quot;: 3010}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0019357622368261218, &quot;time-step&quot;: 3011}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0019215734209865332, &quot;time-step&quot;: 3012}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001932644285261631, &quot;time-step&quot;: 3013}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0019184646662324667, &quot;time-step&quot;: 3014}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0019295370439067483, &quot;time-step&quot;: 3015}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0019153602188453078, &quot;time-step&quot;: 3016}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0019264596048742533, &quot;time-step&quot;: 3017}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0019122817320749164, &quot;time-step&quot;: 3018}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0019233962520956993, &quot;time-step&quot;: 3019}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001909213257022202, &quot;time-step&quot;: 3020}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0019203226547688246, &quot;time-step&quot;: 3021}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001906124409288168, &quot;time-step&quot;: 3022}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001917244982905686, &quot;time-step&quot;: 3023}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0019030440598726273, &quot;time-step&quot;: 3024}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00191417895257473, &quot;time-step&quot;: 3025}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018999666208401322, &quot;time-step&quot;: 3026}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0019111275905743241, &quot;time-step&quot;: 3027}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018969333032146096, &quot;time-step&quot;: 3028}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0019080714555457234, &quot;time-step&quot;: 3029}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018938424764201045, &quot;time-step&quot;: 3030}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001904980163089931, &quot;time-step&quot;: 3031}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00189075933303684, &quot;time-step&quot;: 3032}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0019019236788153648, &quot;time-step&quot;: 3033}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018877239199355245, &quot;time-step&quot;: 3034}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018989024683833122, &quot;time-step&quot;: 3035}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018846915336325765, &quot;time-step&quot;: 3036}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018958861473947763, &quot;time-step&quot;: 3037}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018817039672285318, &quot;time-step&quot;: 3038}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018929298967123032, &quot;time-step&quot;: 3039}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018787309527397156, &quot;time-step&quot;: 3040}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018899280112236738, &quot;time-step&quot;: 3041}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018757106736302376, &quot;time-step&quot;: 3042}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018869324121624231, &quot;time-step&quot;: 3043}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018727189162746072, &quot;time-step&quot;: 3044}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001883958000689745, &quot;time-step&quot;: 3045}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018697549821808934, &quot;time-step&quot;: 3046}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018809970933943987, &quot;time-step&quot;: 3047}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001866791513748467, &quot;time-step&quot;: 3048}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018780524842441082, &quot;time-step&quot;: 3049}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018638429464772344, &quot;time-step&quot;: 3050}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018751118332147598, &quot;time-step&quot;: 3051}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018609027611091733, &quot;time-step&quot;: 3052}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018721588421612978, &quot;time-step&quot;: 3053}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018579454626888037, &quot;time-step&quot;: 3054}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018692295998334885, &quot;time-step&quot;: 3055}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001855025882832706, &quot;time-step&quot;: 3056}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001866320613771677, &quot;time-step&quot;: 3057}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018521123565733433, &quot;time-step&quot;: 3058}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018633947474882007, &quot;time-step&quot;: 3059}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018491744995117188, &quot;time-step&quot;: 3060}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018604606157168746, &quot;time-step&quot;: 3061}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018462584121152759, &quot;time-step&quot;: 3062}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018575689755380154, &quot;time-step&quot;: 3063}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018433448858559132, &quot;time-step&quot;: 3064}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001854659290984273, &quot;time-step&quot;: 3065}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018404412548989058, &quot;time-step&quot;: 3066}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018517754506319761, &quot;time-step&quot;: 3067}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018375462386757135, &quot;time-step&quot;: 3068}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018488753121346235, &quot;time-step&quot;: 3069}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018346690339967608, &quot;time-step&quot;: 3070}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018460068386048079, &quot;time-step&quot;: 3071}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001831787172704935, &quot;time-step&quot;: 3072}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018431257922202349, &quot;time-step&quot;: 3073}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018289181170985103, &quot;time-step&quot;: 3074}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018402793211862445, &quot;time-step&quot;: 3075}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018260717624798417, &quot;time-step&quot;: 3076}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018374222563579679, &quot;time-step&quot;: 3077}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018232252914458513, &quot;time-step&quot;: 3078}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018345857970416546, &quot;time-step&quot;: 3079}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001820380799472332, &quot;time-step&quot;: 3080}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018317557405680418, &quot;time-step&quot;: 3081}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018175359582528472, &quot;time-step&quot;: 3082}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018289255676791072, &quot;time-step&quot;: 3083}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001814728369936347, &quot;time-step&quot;: 3084}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018261212389916182, &quot;time-step&quot;: 3085}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018119056476280093, &quot;time-step&quot;: 3086}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001823296071961522, &quot;time-step&quot;: 3087}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018090694211423397, &quot;time-step&quot;: 3088}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001820470904931426, &quot;time-step&quot;: 3089}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018062704475596547, &quot;time-step&quot;: 3090}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018176782177761197, &quot;time-step&quot;: 3091}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018034770619124174, &quot;time-step&quot;: 3092}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018148979870602489, &quot;time-step&quot;: 3093}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018006917089223862, &quot;time-step&quot;: 3094}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018121384782716632, &quot;time-step&quot;: 3095}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017979472177103162, &quot;time-step&quot;: 3096}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018093589460477233, &quot;time-step&quot;: 3097}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017951587215065956, &quot;time-step&quot;: 3098}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018066000193357468, &quot;time-step&quot;: 3099}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001792412018403411, &quot;time-step&quot;: 3100}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018038556445389986, &quot;time-step&quot;: 3101}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001789657399058342, &quot;time-step&quot;: 3102}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0018010989297181368, &quot;time-step&quot;: 3103}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017869058065116405, &quot;time-step&quot;: 3104}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017983746947720647, &quot;time-step&quot;: 3105}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017841788940131664, &quot;time-step&quot;: 3106}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017956416122615337, &quot;time-step&quot;: 3107}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017814477905631065, &quot;time-step&quot;: 3108}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017929170280694962, &quot;time-step&quot;: 3109}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017787161050364375, &quot;time-step&quot;: 3110}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001790158450603485, &quot;time-step&quot;: 3111}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017759562470018864, &quot;time-step&quot;: 3112}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017874276963993907, &quot;time-step&quot;: 3113}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001773237599991262, &quot;time-step&quot;: 3114}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001784721971489489, &quot;time-step&quot;: 3115}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017705349018797278, &quot;time-step&quot;: 3116}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017820082139223814, &quot;time-step&quot;: 3117}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001767834764905274, &quot;time-step&quot;: 3118}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017793355509638786, &quot;time-step&quot;: 3119}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017651605885475874, &quot;time-step&quot;: 3120}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017766542732715607, &quot;time-step&quot;: 3121}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017624814063310623, &quot;time-step&quot;: 3122}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001773985568434, &quot;time-step&quot;: 3123}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017598120030015707, &quot;time-step&quot;: 3124}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017713319975882769, &quot;time-step&quot;: 3125}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017571478383615613, &quot;time-step&quot;: 3126}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001768648624420166, &quot;time-step&quot;: 3127}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001754501718096435, &quot;time-step&quot;: 3128}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017660248558968306, &quot;time-step&quot;: 3129}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017518519889563322, &quot;time-step&quot;: 3130}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001763370237313211, &quot;time-step&quot;: 3131}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017492210026830435, &quot;time-step&quot;: 3132}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017607557820156217, &quot;time-step&quot;: 3133}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017466065473854542, &quot;time-step&quot;: 3134}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001758152968250215, &quot;time-step&quot;: 3135}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017440177034586668, &quot;time-step&quot;: 3136}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017555663362145424, &quot;time-step&quot;: 3137}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017413843888789415, &quot;time-step&quot;: 3138}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017529106698930264, &quot;time-step&quot;: 3139}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017387430416420102, &quot;time-step&quot;: 3140}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017502623377367854, &quot;time-step&quot;: 3141}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017361021600663662, &quot;time-step&quot;: 3142}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017476517241448164, &quot;time-step&quot;: 3143}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017335120355710387, &quot;time-step&quot;: 3144}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017450597370043397, &quot;time-step&quot;: 3145}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017309206305071712, &quot;time-step&quot;: 3146}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017424790421500802, &quot;time-step&quot;: 3147}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001728329691104591, &quot;time-step&quot;: 3148}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017398789059370756, &quot;time-step&quot;: 3149}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001725733745843172, &quot;time-step&quot;: 3150}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017372940201312304, &quot;time-step&quot;: 3151}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017231509555131197, &quot;time-step&quot;: 3152}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017347073880955577, &quot;time-step&quot;: 3153}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017205793410539627, &quot;time-step&quot;: 3154}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017321528866887093, &quot;time-step&quot;: 3155}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001718020997941494, &quot;time-step&quot;: 3156}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017295914003625512, &quot;time-step&quot;: 3157}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017154690576717257, &quot;time-step&quot;: 3158}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017270490061491728, &quot;time-step&quot;: 3159}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017129433108493686, &quot;time-step&quot;: 3160}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017245144117623568, &quot;time-step&quot;: 3161}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001710378797724843, &quot;time-step&quot;: 3162}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017219474539160728, &quot;time-step&quot;: 3163}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001707845600321889, &quot;time-step&quot;: 3164}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017194468528032303, &quot;time-step&quot;: 3165}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017053356859833002, &quot;time-step&quot;: 3166}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017169236671179533, &quot;time-step&quot;: 3167}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001702828798443079, &quot;time-step&quot;: 3168}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017144203884527087, &quot;time-step&quot;: 3169}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017003173707053065, &quot;time-step&quot;: 3170}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017119039548560977, &quot;time-step&quot;: 3171}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001697802566923201, &quot;time-step&quot;: 3172}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017093875212594867, &quot;time-step&quot;: 3173}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016953034792095423, &quot;time-step&quot;: 3174}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017069088062271476, &quot;time-step&quot;: 3175}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016928308177739382, &quot;time-step&quot;: 3176}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017044248525053263, &quot;time-step&quot;: 3177}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001690329983830452, &quot;time-step&quot;: 3178}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0017019378719851375, &quot;time-step&quot;: 3179}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016878669848665595, &quot;time-step&quot;: 3180}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001699472893960774, &quot;time-step&quot;: 3181}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016853867564350367, &quot;time-step&quot;: 3182}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016969854477792978, &quot;time-step&quot;: 3183}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016829001251608133, &quot;time-step&quot;: 3184}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016945145325735211, &quot;time-step&quot;: 3185}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001680442364886403, &quot;time-step&quot;: 3186}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001692045945674181, &quot;time-step&quot;: 3187}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016779731959104538, &quot;time-step&quot;: 3188}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016895981971174479, &quot;time-step&quot;: 3189}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016755362739786506, &quot;time-step&quot;: 3190}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016871476545929909, &quot;time-step&quot;: 3191}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016730895731598139, &quot;time-step&quot;: 3192}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016847171355038881, &quot;time-step&quot;: 3193}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016706684837117791, &quot;time-step&quot;: 3194}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001682277419604361, &quot;time-step&quot;: 3195}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016682419227436185, &quot;time-step&quot;: 3196}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001679871929809451, &quot;time-step&quot;: 3197}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001665838877670467, &quot;time-step&quot;: 3198}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016774543328210711, &quot;time-step&quot;: 3199}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001663410454057157, &quot;time-step&quot;: 3200}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016750465147197247, &quot;time-step&quot;: 3201}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001661032671108842, &quot;time-step&quot;: 3202}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001672673039138317, &quot;time-step&quot;: 3203}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016586571000516415, &quot;time-step&quot;: 3204}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016703014262020588, &quot;time-step&quot;: 3205}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016563000390306115, &quot;time-step&quot;: 3206}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001667927484959364, &quot;time-step&quot;: 3207}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016539107309654355, &quot;time-step&quot;: 3208}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016655372455716133, &quot;time-step&quot;: 3209}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016514970920979977, &quot;time-step&quot;: 3210}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001663128030486405, &quot;time-step&quot;: 3211}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016491229180246592, &quot;time-step&quot;: 3212}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016607724828645587, &quot;time-step&quot;: 3213}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016467706300318241, &quot;time-step&quot;: 3214}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016583926044404507, &quot;time-step&quot;: 3215}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001644402276724577, &quot;time-step&quot;: 3216}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016560449730604887, &quot;time-step&quot;: 3217}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001642050570808351, &quot;time-step&quot;: 3218}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016536952462047338, &quot;time-step&quot;: 3219}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016396953724324703, &quot;time-step&quot;: 3220}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016513268928974867, &quot;time-step&quot;: 3221}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016373435501009226, &quot;time-step&quot;: 3222}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016489820554852486, &quot;time-step&quot;: 3223}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001634997664950788, &quot;time-step&quot;: 3224}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00164663209579885, &quot;time-step&quot;: 3225}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001632652711123228, &quot;time-step&quot;: 3226}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016442773630842566, &quot;time-step&quot;: 3227}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001630304497666657, &quot;time-step&quot;: 3228}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001641943585127592, &quot;time-step&quot;: 3229}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016279809642583132, &quot;time-step&quot;: 3230}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016396190039813519, &quot;time-step&quot;: 3231}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016256714006885886, &quot;time-step&quot;: 3232}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016373267862945795, &quot;time-step&quot;: 3233}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016233902424573898, &quot;time-step&quot;: 3234}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016350381774827838, &quot;time-step&quot;: 3235}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016211025649681687, &quot;time-step&quot;: 3236}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016327459597960114, &quot;time-step&quot;: 3237}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016187970759347081, &quot;time-step&quot;: 3238}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016304306918755174, &quot;time-step&quot;: 3239}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001616498688235879, &quot;time-step&quot;: 3240}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016281392890959978, &quot;time-step&quot;: 3241}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001614206819795072, &quot;time-step&quot;: 3242}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016258669784292579, &quot;time-step&quot;: 3243}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016119652427732944, &quot;time-step&quot;: 3244}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016236042138189077, &quot;time-step&quot;: 3245}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016096754698082805, &quot;time-step&quot;: 3246}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016213160706683993, &quot;time-step&quot;: 3247}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016074135201051831, &quot;time-step&quot;: 3248}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016190569149330258, &quot;time-step&quot;: 3249}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001605127821676433, &quot;time-step&quot;: 3250}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016167645808309317, &quot;time-step&quot;: 3251}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016028638929128647, &quot;time-step&quot;: 3252}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016145282424986362, &quot;time-step&quot;: 3253}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016006318619474769, &quot;time-step&quot;: 3254}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001612273626960814, &quot;time-step&quot;: 3255}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015983885386958718, &quot;time-step&quot;: 3256}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016100520733743906, &quot;time-step&quot;: 3257}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015961696626618505, &quot;time-step&quot;: 3258}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016078143380582333, &quot;time-step&quot;: 3259}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001593938795849681, &quot;time-step&quot;: 3260}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016055733431130648, &quot;time-step&quot;: 3261}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015916904667392373, &quot;time-step&quot;: 3262}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016033366555348039, &quot;time-step&quot;: 3263}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001589477644301951, &quot;time-step&quot;: 3264}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0016011331463232636, &quot;time-step&quot;: 3265}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015872764633968472, &quot;time-step&quot;: 3266}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015989263774827123, &quot;time-step&quot;: 3267}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015850886702537537, &quot;time-step&quot;: 3268}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015967462677508593, &quot;time-step&quot;: 3269}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015829228796064854, &quot;time-step&quot;: 3270}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015945800114423037, &quot;time-step&quot;: 3271}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015807372983545065, &quot;time-step&quot;: 3272}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015923877945169806, &quot;time-step&quot;: 3273}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015785453142598271, &quot;time-step&quot;: 3274}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015901925507932901, &quot;time-step&quot;: 3275}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015763709088787436, &quot;time-step&quot;: 3276}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015880290884524584, &quot;time-step&quot;: 3277}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001574206748045981, &quot;time-step&quot;: 3278}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001585835823789239, &quot;time-step&quot;: 3279}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015720147639513016, &quot;time-step&quot;: 3280}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015836473321542144, &quot;time-step&quot;: 3281}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015698176575824618, &quot;time-step&quot;: 3282}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015814552316442132, &quot;time-step&quot;: 3283}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015676378970965743, &quot;time-step&quot;: 3284}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015792704652994871, &quot;time-step&quot;: 3285}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015654738526791334, &quot;time-step&quot;: 3286}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015771127073094249, &quot;time-step&quot;: 3287}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015633145812898874, &quot;time-step&quot;: 3288}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015749535523355007, &quot;time-step&quot;: 3289}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001561174402013421, &quot;time-step&quot;: 3290}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00157280417624861, &quot;time-step&quot;: 3291}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015590329421684146, &quot;time-step&quot;: 3292}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015706735430285335, &quot;time-step&quot;: 3293}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015569082461297512, &quot;time-step&quot;: 3294}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015685680555179715, &quot;time-step&quot;: 3295}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001554800313897431, &quot;time-step&quot;: 3296}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015664236852899194, &quot;time-step&quot;: 3297}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001552653149701655, &quot;time-step&quot;: 3298}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015642894431948662, &quot;time-step&quot;: 3299}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015505403280258179, &quot;time-step&quot;: 3300}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001562178716994822, &quot;time-step&quot;: 3301}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015484399627894163, &quot;time-step&quot;: 3302}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015600643819198012, &quot;time-step&quot;: 3303}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015463257441297174, &quot;time-step&quot;: 3304}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015579639002680779, &quot;time-step&quot;: 3305}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015442389994859695, &quot;time-step&quot;: 3306}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015558856539428234, &quot;time-step&quot;: 3307}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001542133861221373, &quot;time-step&quot;: 3308}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001553744776174426, &quot;time-step&quot;: 3309}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001540014985948801, &quot;time-step&quot;: 3310}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015516416169703007, &quot;time-step&quot;: 3311}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015379241667687893, &quot;time-step&quot;: 3312}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015495435800403357, &quot;time-step&quot;: 3313}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015358352102339268, &quot;time-step&quot;: 3314}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015474720858037472, &quot;time-step&quot;: 3315}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015337755903601646, &quot;time-step&quot;: 3316}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015454036183655262, &quot;time-step&quot;: 3317}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015317066572606564, &quot;time-step&quot;: 3318}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015433066291734576, &quot;time-step&quot;: 3319}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001529612229205668, &quot;time-step&quot;: 3320}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015412268694490194, &quot;time-step&quot;: 3321}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015275445766746998, &quot;time-step&quot;: 3322}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015391585184261203, &quot;time-step&quot;: 3323}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015254870522767305, &quot;time-step&quot;: 3324}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015371001791208982, &quot;time-step&quot;: 3325}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001523416955024004, &quot;time-step&quot;: 3326}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015350303146988153, &quot;time-step&quot;: 3327}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015213710721582174, &quot;time-step&quot;: 3328}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001532972790300846, &quot;time-step&quot;: 3329}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001519326469860971, &quot;time-step&quot;: 3330}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015309310983866453, &quot;time-step&quot;: 3331}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015172867570072412, &quot;time-step&quot;: 3332}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00152888847514987, &quot;time-step&quot;: 3333}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015152439009398222, &quot;time-step&quot;: 3334}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015268486458808184, &quot;time-step&quot;: 3335}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015132217667996883, &quot;time-step&quot;: 3336}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015248210402205586, &quot;time-step&quot;: 3337}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015112077817320824, &quot;time-step&quot;: 3338}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015228043776005507, &quot;time-step&quot;: 3339}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015091815730556846, &quot;time-step&quot;: 3340}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015207899268716574, &quot;time-step&quot;: 3341}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001507206354290247, &quot;time-step&quot;: 3342}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015188142424449325, &quot;time-step&quot;: 3343}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015052203088998795, &quot;time-step&quot;: 3344}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015168088721111417, &quot;time-step&quot;: 3345}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001503228791989386, &quot;time-step&quot;: 3346}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015148217789828777, &quot;time-step&quot;: 3347}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015012349467724562, &quot;time-step&quot;: 3348}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015128140803426504, &quot;time-step&quot;: 3349}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001499236561357975, &quot;time-step&quot;: 3350}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015108007937669754, &quot;time-step&quot;: 3351}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014972238568589091, &quot;time-step&quot;: 3352}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015088203363120556, &quot;time-step&quot;: 3353}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014952573692426085, &quot;time-step&quot;: 3354}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015068587381392717, &quot;time-step&quot;: 3355}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014932984486222267, &quot;time-step&quot;: 3356}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015048796776682138, &quot;time-step&quot;: 3357}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014913248596712947, &quot;time-step&quot;: 3358}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015028980560600758, &quot;time-step&quot;: 3359}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014893494080752134, &quot;time-step&quot;: 3360}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0015009242342785, &quot;time-step&quot;: 3361}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014874018961563706, &quot;time-step&quot;: 3362}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001498991739936173, &quot;time-step&quot;: 3363}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014854773180559278, &quot;time-step&quot;: 3364}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014970381744205952, &quot;time-step&quot;: 3365}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014835086185485125, &quot;time-step&quot;: 3366}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001495092874392867, &quot;time-step&quot;: 3367}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014815789181739092, &quot;time-step&quot;: 3368}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001493134768679738, &quot;time-step&quot;: 3369}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014796152245253325, &quot;time-step&quot;: 3370}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014911845792084932, &quot;time-step&quot;: 3371}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001477676210924983, &quot;time-step&quot;: 3372}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014892322942614555, &quot;time-step&quot;: 3373}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014757139142602682, &quot;time-step&quot;: 3374}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014872595202177763, &quot;time-step&quot;: 3375}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001473776064813137, &quot;time-step&quot;: 3376}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014853384345769882, &quot;time-step&quot;: 3377}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014718727907165885, &quot;time-step&quot;: 3378}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014834379544481635, &quot;time-step&quot;: 3379}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014699682360514998, &quot;time-step&quot;: 3380}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014815120957791805, &quot;time-step&quot;: 3381}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001468044356442988, &quot;time-step&quot;: 3382}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014796024188399315, &quot;time-step&quot;: 3383}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014661470195278525, &quot;time-step&quot;: 3384}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014776811003684998, &quot;time-step&quot;: 3385}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014642225578427315, &quot;time-step&quot;: 3386}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014757819008082151, &quot;time-step&quot;: 3387}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014623505994677544, &quot;time-step&quot;: 3388}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014738764148205519, &quot;time-step&quot;: 3389}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001460439758375287, &quot;time-step&quot;: 3390}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014719886239618063, &quot;time-step&quot;: 3391}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001458565704524517, &quot;time-step&quot;: 3392}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014701081672683358, &quot;time-step&quot;: 3393}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014566982863470912, &quot;time-step&quot;: 3394}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001468238653615117, &quot;time-step&quot;: 3395}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014548420440405607, &quot;time-step&quot;: 3396}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014663643669337034, &quot;time-step&quot;: 3397}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014529420295730233, &quot;time-step&quot;: 3398}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014644695911556482, &quot;time-step&quot;: 3399}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014510753098875284, &quot;time-step&quot;: 3400}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001462601707316935, &quot;time-step&quot;: 3401}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014491898473352194, &quot;time-step&quot;: 3402}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014606999466195703, &quot;time-step&quot;: 3403}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014473185874521732, &quot;time-step&quot;: 3404}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014588258927688003, &quot;time-step&quot;: 3405}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014454449992626905, &quot;time-step&quot;: 3406}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014569568447768688, &quot;time-step&quot;: 3407}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014436119236052036, &quot;time-step&quot;: 3408}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014551192289218307, &quot;time-step&quot;: 3409}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014417648781090975, &quot;time-step&quot;: 3410}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014532679924741387, &quot;time-step&quot;: 3411}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014399162027984858, &quot;time-step&quot;: 3412}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014514288632199168, &quot;time-step&quot;: 3413}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014380875509232283, &quot;time-step&quot;: 3414}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014495959039777517, &quot;time-step&quot;: 3415}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00143627414945513, &quot;time-step&quot;: 3416}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014477785443887115, &quot;time-step&quot;: 3417}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014344500377774239, &quot;time-step&quot;: 3418}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014459622325375676, &quot;time-step&quot;: 3419}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014326469972729683, &quot;time-step&quot;: 3420}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014441416133195162, &quot;time-step&quot;: 3421}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00143082020804286, &quot;time-step&quot;: 3422}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014423119137063622, &quot;time-step&quot;: 3423}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014289880637079477, &quot;time-step&quot;: 3424}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014404519461095333, &quot;time-step&quot;: 3425}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014271646505221725, &quot;time-step&quot;: 3426}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014386478578671813, &quot;time-step&quot;: 3427}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014253546250984073, &quot;time-step&quot;: 3428}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014368250267580152, &quot;time-step&quot;: 3429}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014235409907996655, &quot;time-step&quot;: 3430}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014350209385156631, &quot;time-step&quot;: 3431}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014217477291822433, &quot;time-step&quot;: 3432}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001433228375390172, &quot;time-step&quot;: 3433}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014199672732502222, &quot;time-step&quot;: 3434}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014314409345388412, &quot;time-step&quot;: 3435}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014181819278746843, &quot;time-step&quot;: 3436}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014296460431069136, &quot;time-step&quot;: 3437}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014163752784952521, &quot;time-step&quot;: 3438}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014278489397838712, &quot;time-step&quot;: 3439}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014146158937364817, &quot;time-step&quot;: 3440}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001426072558388114, &quot;time-step&quot;: 3441}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014128397451713681, &quot;time-step&quot;: 3442}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014243177138268948, &quot;time-step&quot;: 3443}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014110981719568372, &quot;time-step&quot;: 3444}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014225514605641365, &quot;time-step&quot;: 3445}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014093281934037805, &quot;time-step&quot;: 3446}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014207581989467144, &quot;time-step&quot;: 3447}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014075370272621512, &quot;time-step&quot;: 3448}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001418967847712338, &quot;time-step&quot;: 3449}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014057679800316691, &quot;time-step&quot;: 3450}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014172064838930964, &quot;time-step&quot;: 3451}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014040111564099789, &quot;time-step&quot;: 3452}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014154566451907158, &quot;time-step&quot;: 3453}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001402268884703517, &quot;time-step&quot;: 3454}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014136950485408306, &quot;time-step&quot;: 3455}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014005025150254369, &quot;time-step&quot;: 3456}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001411918899975717, &quot;time-step&quot;: 3457}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013987441780045629, &quot;time-step&quot;: 3458}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014101800043135881, &quot;time-step&quot;: 3459}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013970289146527648, &quot;time-step&quot;: 3460}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001408467418514192, &quot;time-step&quot;: 3461}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013953252928331494, &quot;time-step&quot;: 3462}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014067522715777159, &quot;time-step&quot;: 3463}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013935990864410996, &quot;time-step&quot;: 3464}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014049950987100601, &quot;time-step&quot;: 3465}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013918656622990966, &quot;time-step&quot;: 3466}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014032532926648855, &quot;time-step&quot;: 3467}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013901228085160255, &quot;time-step&quot;: 3468}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0014015438500791788, &quot;time-step&quot;: 3469}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013884170912206173, &quot;time-step&quot;: 3470}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013998195063322783, &quot;time-step&quot;: 3471}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013867092784494162, &quot;time-step&quot;: 3472}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001398110412992537, &quot;time-step&quot;: 3473}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013850087998434901, &quot;time-step&quot;: 3474}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013964009704068303, &quot;time-step&quot;: 3475}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001383304945193231, &quot;time-step&quot;: 3476}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001394694671034813, &quot;time-step&quot;: 3477}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013816035352647305, &quot;time-step&quot;: 3478}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001392989419400692, &quot;time-step&quot;: 3479}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001379899913445115, &quot;time-step&quot;: 3480}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001391278812661767, &quot;time-step&quot;: 3481}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013782150344923139, &quot;time-step&quot;: 3482}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013896033633500338, &quot;time-step&quot;: 3483}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013765289913862944, &quot;time-step&quot;: 3484}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013878940371796489, &quot;time-step&quot;: 3485}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001374833402223885, &quot;time-step&quot;: 3486}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001386200892738998, &quot;time-step&quot;: 3487}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013731600483879447, &quot;time-step&quot;: 3488}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013845362700521946, &quot;time-step&quot;: 3489}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013715021777898073, &quot;time-step&quot;: 3490}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013828511582687497, &quot;time-step&quot;: 3491}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013697999529540539, &quot;time-step&quot;: 3492}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013811506796628237, &quot;time-step&quot;: 3493}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013681091368198395, &quot;time-step&quot;: 3494}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013794514816254377, &quot;time-step&quot;: 3495}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013664327561855316, &quot;time-step&quot;: 3496}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001377797918394208, &quot;time-step&quot;: 3497}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001364795956760645, &quot;time-step&quot;: 3498}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013761401642113924, &quot;time-step&quot;: 3499}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013631407637149096, &quot;time-step&quot;: 3500}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013744793832302094, &quot;time-step&quot;: 3501}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001361468224786222, &quot;time-step&quot;: 3502}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013727971818298101, &quot;time-step&quot;: 3503}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013598211808130145, &quot;time-step&quot;: 3504}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013711692299693823, &quot;time-step&quot;: 3505}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013581773964688182, &quot;time-step&quot;: 3506}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013694815570488572, &quot;time-step&quot;: 3507}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013565149856731296, &quot;time-step&quot;: 3508}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013678499963134527, &quot;time-step&quot;: 3509}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013548913411796093, &quot;time-step&quot;: 3510}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013662023702636361, &quot;time-step&quot;: 3511}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013532472075894475, &quot;time-step&quot;: 3512}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013645566068589687, &quot;time-step&quot;: 3513}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013516235630959272, &quot;time-step&quot;: 3514}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013629476306959987, &quot;time-step&quot;: 3515}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013500119093805552, &quot;time-step&quot;: 3516}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013613090850412846, &quot;time-step&quot;: 3517}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013483890797942877, &quot;time-step&quot;: 3518}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013596899807453156, &quot;time-step&quot;: 3519}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00134676368907094, &quot;time-step&quot;: 3520}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013580742524936795, &quot;time-step&quot;: 3521}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013451738050207496, &quot;time-step&quot;: 3522}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013564556138589978, &quot;time-step&quot;: 3523}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013435587752610445, &quot;time-step&quot;: 3524}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013548440765589476, &quot;time-step&quot;: 3525}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013419545721262693, &quot;time-step&quot;: 3526}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001353218569420278, &quot;time-step&quot;: 3527}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013403117191046476, &quot;time-step&quot;: 3528}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013515852624550462, &quot;time-step&quot;: 3529}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001338711939752102, &quot;time-step&quot;: 3530}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013499828055500984, &quot;time-step&quot;: 3531}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013371057575568557, &quot;time-step&quot;: 3532}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013483846560120583, &quot;time-step&quot;: 3533}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001335512613877654, &quot;time-step&quot;: 3534}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013467730022966862, &quot;time-step&quot;: 3535}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013339078286662698, &quot;time-step&quot;: 3536}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013451724080368876, &quot;time-step&quot;: 3537}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013323166640475392, &quot;time-step&quot;: 3538}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013435757718980312, &quot;time-step&quot;: 3539}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013307308545336127, &quot;time-step&quot;: 3540}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013419734314084053, &quot;time-step&quot;: 3541}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013291207142174244, &quot;time-step&quot;: 3542}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013403664343059063, &quot;time-step&quot;: 3543}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013275350211188197, &quot;time-step&quot;: 3544}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013387692160904408, &quot;time-step&quot;: 3545}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001325942692346871, &quot;time-step&quot;: 3546}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013371846871450543, &quot;time-step&quot;: 3547}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013243877328932285, &quot;time-step&quot;: 3548}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013356130803003907, &quot;time-step&quot;: 3549}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013228198513388634, &quot;time-step&quot;: 3550}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013340465957298875, &quot;time-step&quot;: 3551}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001321268267929554, &quot;time-step&quot;: 3552}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013324939645826817, &quot;time-step&quot;: 3553}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013197122607380152, &quot;time-step&quot;: 3554}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013309404021129012, &quot;time-step&quot;: 3555}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001318173948675394, &quot;time-step&quot;: 3556}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013293843949213624, &quot;time-step&quot;: 3557}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013166344724595547, &quot;time-step&quot;: 3558}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013278574915602803, &quot;time-step&quot;: 3559}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013151053572073579, &quot;time-step&quot;: 3560}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013263099826872349, &quot;time-step&quot;: 3561}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013135606423020363, &quot;time-step&quot;: 3562}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013247665483504534, &quot;time-step&quot;: 3563}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013120385119691491, &quot;time-step&quot;: 3564}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013232443016022444, &quot;time-step&quot;: 3565}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001310508931055665, &quot;time-step&quot;: 3566}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001321702729910612, &quot;time-step&quot;: 3567}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013089888961985707, &quot;time-step&quot;: 3568}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001320180483162403, &quot;time-step&quot;: 3569}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013074371963739395, &quot;time-step&quot;: 3570}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013186169089749455, &quot;time-step&quot;: 3571}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013059088960289955, &quot;time-step&quot;: 3572}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00131707894615829, &quot;time-step&quot;: 3573}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013043757062405348, &quot;time-step&quot;: 3574}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013155498309060931, &quot;time-step&quot;: 3575}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013028663815930486, &quot;time-step&quot;: 3576}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013140320079401135, &quot;time-step&quot;: 3577}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013013456482440233, &quot;time-step&quot;: 3578}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013125346740707755, &quot;time-step&quot;: 3579}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012998601887375116, &quot;time-step&quot;: 3580}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001311019528657198, &quot;time-step&quot;: 3581}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012983487686142325, &quot;time-step&quot;: 3582}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013094936730340123, &quot;time-step&quot;: 3583}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00129684095736593, &quot;time-step&quot;: 3584}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013079952914267778, &quot;time-step&quot;: 3585}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012953504920005798, &quot;time-step&quot;: 3586}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013064935337752104, &quot;time-step&quot;: 3587}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012938342988491058, &quot;time-step&quot;: 3588}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013049656990915537, &quot;time-step&quot;: 3589}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012923481408506632, &quot;time-step&quot;: 3590}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013034716248512268, &quot;time-step&quot;: 3591}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012908498756587505, &quot;time-step&quot;: 3592}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013019756879657507, &quot;time-step&quot;: 3593}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012893578968942165, &quot;time-step&quot;: 3594}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0013004784705117345, &quot;time-step&quot;: 3595}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012878651032224298, &quot;time-step&quot;: 3596}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001298982067964971, &quot;time-step&quot;: 3597}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012863855808973312, &quot;time-step&quot;: 3598}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012974976561963558, &quot;time-step&quot;: 3599}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012849062914028764, &quot;time-step&quot;: 3600}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012960094027221203, &quot;time-step&quot;: 3601}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012834154767915606, &quot;time-step&quot;: 3602}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012945152120664716, &quot;time-step&quot;: 3603}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001281948760151863, &quot;time-step&quot;: 3604}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012930587399750948, &quot;time-step&quot;: 3605}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001280483789741993, &quot;time-step&quot;: 3606}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012915784027427435, &quot;time-step&quot;: 3607}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001279023359529674, &quot;time-step&quot;: 3608}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012901087757200003, &quot;time-step&quot;: 3609}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012775667710229754, &quot;time-step&quot;: 3610}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012886542826890945, &quot;time-step&quot;: 3611}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001276102033443749, &quot;time-step&quot;: 3612}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012871809303760529, &quot;time-step&quot;: 3613}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012746411375701427, &quot;time-step&quot;: 3614}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012857213150709867, &quot;time-step&quot;: 3615}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012731817550957203, &quot;time-step&quot;: 3616}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012842429568991065, &quot;time-step&quot;: 3617}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001271724933758378, &quot;time-step&quot;: 3618}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012827870668843389, &quot;time-step&quot;: 3619}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012702904641628265, &quot;time-step&quot;: 3620}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012813466601073742, &quot;time-step&quot;: 3621}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012688601855188608, &quot;time-step&quot;: 3622}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001279917429201305, &quot;time-step&quot;: 3623}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001267412444576621, &quot;time-step&quot;: 3624}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012784514110535383, &quot;time-step&quot;: 3625}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012659691274166107, &quot;time-step&quot;: 3626}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012770143803209066, &quot;time-step&quot;: 3627}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012645386159420013, &quot;time-step&quot;: 3628}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012755682691931725, &quot;time-step&quot;: 3629}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001263116137124598, &quot;time-step&quot;: 3630}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001274142530746758, &quot;time-step&quot;: 3631}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001261683413758874, &quot;time-step&quot;: 3632}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001272721216082573, &quot;time-step&quot;: 3633}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012602759525179863, &quot;time-step&quot;: 3634}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012713068863376975, &quot;time-step&quot;: 3635}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012588606914505363, &quot;time-step&quot;: 3636}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012698826612904668, &quot;time-step&quot;: 3637}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012574568390846252, &quot;time-step&quot;: 3638}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012684566900134087, &quot;time-step&quot;: 3639}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012560230679810047, &quot;time-step&quot;: 3640}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012670382857322693, &quot;time-step&quot;: 3641}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012546285288408399, &quot;time-step&quot;: 3642}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001265642698854208, &quot;time-step&quot;: 3643}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001253235386684537, &quot;time-step&quot;: 3644}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012642326764762402, &quot;time-step&quot;: 3645}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001251837587915361, &quot;time-step&quot;: 3646}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012628164840862155, &quot;time-step&quot;: 3647}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012504213955253363, &quot;time-step&quot;: 3648}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012614191509783268, &quot;time-step&quot;: 3649}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012490497902035713, &quot;time-step&quot;: 3650}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012600217014551163, &quot;time-step&quot;: 3651}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012476402334868908, &quot;time-step&quot;: 3652}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012586137745529413, &quot;time-step&quot;: 3653}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001246249070391059, &quot;time-step&quot;: 3654}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001257222960703075, &quot;time-step&quot;: 3655}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012448383495211601, &quot;time-step&quot;: 3656}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012557804584503174, &quot;time-step&quot;: 3657}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012434307718649507, &quot;time-step&quot;: 3658}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012543940683826804, &quot;time-step&quot;: 3659}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001242064405232668, &quot;time-step&quot;: 3660}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012530178064480424, &quot;time-step&quot;: 3661}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012406808091327548, &quot;time-step&quot;: 3662}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012516394490376115, &quot;time-step&quot;: 3663}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012393251527100801, &quot;time-step&quot;: 3664}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012502579484134912, &quot;time-step&quot;: 3665}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012379398103803396, &quot;time-step&quot;: 3666}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012488706270232797, &quot;time-step&quot;: 3667}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012365690199658275, &quot;time-step&quot;: 3668}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001247492851689458, &quot;time-step&quot;: 3669}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012351793702691793, &quot;time-step&quot;: 3670}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012461008736863732, &quot;time-step&quot;: 3671}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012338099768385291, &quot;time-step&quot;: 3672}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012447464978322387, &quot;time-step&quot;: 3673}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001232476788572967, &quot;time-step&quot;: 3674}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012434030650183558, &quot;time-step&quot;: 3675}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012311408063396811, &quot;time-step&quot;: 3676}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001242058933712542, &quot;time-step&quot;: 3677}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001229788176715374, &quot;time-step&quot;: 3678}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012407090980559587, &quot;time-step&quot;: 3679}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012284534750506282, &quot;time-step&quot;: 3680}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012393379583954811, &quot;time-step&quot;: 3681}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012270754668861628, &quot;time-step&quot;: 3682}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012379696127027273, &quot;time-step&quot;: 3683}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001225727959536016, &quot;time-step&quot;: 3684}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001236619078554213, &quot;time-step&quot;: 3685}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012243875535205007, &quot;time-step&quot;: 3686}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001235277857631445, &quot;time-step&quot;: 3687}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012230605352669954, &quot;time-step&quot;: 3688}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012339565437287092, &quot;time-step&quot;: 3689}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012217609910294414, &quot;time-step&quot;: 3690}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012326349969953299, &quot;time-step&quot;: 3691}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012204209342598915, &quot;time-step&quot;: 3692}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012312993640080094, &quot;time-step&quot;: 3693}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012190969428047538, &quot;time-step&quot;: 3694}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012299546506255865, &quot;time-step&quot;: 3695}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012177599128335714, &quot;time-step&quot;: 3696}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012286213459447026, &quot;time-step&quot;: 3697}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012164529180154204, &quot;time-step&quot;: 3698}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012272978201508522, &quot;time-step&quot;: 3699}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012151151895523071, &quot;time-step&quot;: 3700}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012259631184861064, &quot;time-step&quot;: 3701}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012137879384681582, &quot;time-step&quot;: 3702}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001224628183990717, &quot;time-step&quot;: 3703}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012124673230573535, &quot;time-step&quot;: 3704}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012233128072693944, &quot;time-step&quot;: 3705}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012111569521948695, &quot;time-step&quot;: 3706}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001221996615640819, &quot;time-step&quot;: 3707}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012098475126549602, &quot;time-step&quot;: 3708}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012206586543470621, &quot;time-step&quot;: 3709}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012085230555385351, &quot;time-step&quot;: 3710}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012193468865007162, &quot;time-step&quot;: 3711}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012072339886799455, &quot;time-step&quot;: 3712}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012180618941783905, &quot;time-step&quot;: 3713}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001205943524837494, &quot;time-step&quot;: 3714}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012167550157755613, &quot;time-step&quot;: 3715}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012046402553096414, &quot;time-step&quot;: 3716}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012154362630099058, &quot;time-step&quot;: 3717}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012033184757456183, &quot;time-step&quot;: 3718}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012141144834458828, &quot;time-step&quot;: 3719}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012020275462418795, &quot;time-step&quot;: 3720}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012128300732001662, &quot;time-step&quot;: 3721}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012007548939436674, &quot;time-step&quot;: 3722}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012115549761801958, &quot;time-step&quot;: 3723}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001199468970298767, &quot;time-step&quot;: 3724}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012102595064789057, &quot;time-step&quot;: 3725}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011981797870248556, &quot;time-step&quot;: 3726}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012089407537132502, &quot;time-step&quot;: 3727}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011968770995736122, &quot;time-step&quot;: 3728}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001207650639116764, &quot;time-step&quot;: 3729}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011955976951867342, &quot;time-step&quot;: 3730}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001206367276608944, &quot;time-step&quot;: 3731}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011943192221224308, &quot;time-step&quot;: 3732}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012050678487867117, &quot;time-step&quot;: 3733}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011930237524211407, &quot;time-step&quot;: 3734}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012037719134241343, &quot;time-step&quot;: 3735}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011917618103325367, &quot;time-step&quot;: 3736}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0012025136966258287, &quot;time-step&quot;: 3737}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011904877610504627, &quot;time-step&quot;: 3738}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001201235456392169, &quot;time-step&quot;: 3739}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011892335023730993, &quot;time-step&quot;: 3740}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011999758426100016, &quot;time-step&quot;: 3741}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011879756348207593, &quot;time-step&quot;: 3742}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011987084290012717, &quot;time-step&quot;: 3743}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011867065913975239, &quot;time-step&quot;: 3744}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001197436940856278, &quot;time-step&quot;: 3745}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00118544592987746, &quot;time-step&quot;: 3746}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011961753480136395, &quot;time-step&quot;: 3747}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011841865489259362, &quot;time-step&quot;: 3748}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011948986211791635, &quot;time-step&quot;: 3749}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011829145951196551, &quot;time-step&quot;: 3750}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011936293449252844, &quot;time-step&quot;: 3751}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011816793121397495, &quot;time-step&quot;: 3752}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011923944111913443, &quot;time-step&quot;: 3753}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001180423772893846, &quot;time-step&quot;: 3754}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011911168694496155, &quot;time-step&quot;: 3755}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011791775468736887, &quot;time-step&quot;: 3756}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001189871458336711, &quot;time-step&quot;: 3757}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011779408669099212, &quot;time-step&quot;: 3758}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011886267457157373, &quot;time-step&quot;: 3759}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001176708028651774, &quot;time-step&quot;: 3760}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011873948387801647, &quot;time-step&quot;: 3761}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011754712322726846, &quot;time-step&quot;: 3762}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011861404636874795, &quot;time-step&quot;: 3763}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001174238626845181, &quot;time-step&quot;: 3764}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011849054135382175, &quot;time-step&quot;: 3765}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001172987394966185, &quot;time-step&quot;: 3766}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011836579069495201, &quot;time-step&quot;: 3767}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011717858724296093, &quot;time-step&quot;: 3768}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011824506800621748, &quot;time-step&quot;: 3769}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001170561881735921, &quot;time-step&quot;: 3770}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001181233674287796, &quot;time-step&quot;: 3771}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011693463893607259, &quot;time-step&quot;: 3772}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011799907078966498, &quot;time-step&quot;: 3773}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011681168107315898, &quot;time-step&quot;: 3774}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001178769045509398, &quot;time-step&quot;: 3775}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011668989900499582, &quot;time-step&quot;: 3776}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011775349266827106, &quot;time-step&quot;: 3777}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001165671506896615, &quot;time-step&quot;: 3778}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011763126822188497, &quot;time-step&quot;: 3779}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011644621845334768, &quot;time-step&quot;: 3780}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011750903213396668, &quot;time-step&quot;: 3781}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011632535606622696, &quot;time-step&quot;: 3782}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011738752946257591, &quot;time-step&quot;: 3783}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001162045169621706, &quot;time-step&quot;: 3784}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011726785451173782, &quot;time-step&quot;: 3785}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011608580825850368, &quot;time-step&quot;: 3786}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011714613065123558, &quot;time-step&quot;: 3787}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011596448021009564, &quot;time-step&quot;: 3788}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011702608317136765, &quot;time-step&quot;: 3789}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011584698222577572, &quot;time-step&quot;: 3790}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011690856190398335, &quot;time-step&quot;: 3791}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011572870425879955, &quot;time-step&quot;: 3792}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011678820010274649, &quot;time-step&quot;: 3793}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011560774873942137, &quot;time-step&quot;: 3794}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011666633654385805, &quot;time-step&quot;: 3795}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001154894707724452, &quot;time-step&quot;: 3796}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011654846603050828, &quot;time-step&quot;: 3797}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001153705408796668, &quot;time-step&quot;: 3798}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001164268353022635, &quot;time-step&quot;: 3799}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011524813016876578, &quot;time-step&quot;: 3800}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011630426160991192, &quot;time-step&quot;: 3801}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011512869969010353, &quot;time-step&quot;: 3802}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001161860884167254, &quot;time-step&quot;: 3803}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011501050321385264, &quot;time-step&quot;: 3804}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011606616899371147, &quot;time-step&quot;: 3805}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011489191092550755, &quot;time-step&quot;: 3806}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001159466104581952, &quot;time-step&quot;: 3807}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011477310908958316, &quot;time-step&quot;: 3808}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001158277504146099, &quot;time-step&quot;: 3809}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011465479619801044, &quot;time-step&quot;: 3810}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001157101127319038, &quot;time-step&quot;: 3811}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011453843908384442, &quot;time-step&quot;: 3812}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001155932666733861, &quot;time-step&quot;: 3813}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001144222915172577, &quot;time-step&quot;: 3814}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011547496542334557, &quot;time-step&quot;: 3815}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011430549202486873, &quot;time-step&quot;: 3816}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011535902740433812, &quot;time-step&quot;: 3817}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011418894864618778, &quot;time-step&quot;: 3818}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011524296132847667, &quot;time-step&quot;: 3819}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011407592101022601, &quot;time-step&quot;: 3820}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011512779165059328, &quot;time-step&quot;: 3821}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011395964538678527, &quot;time-step&quot;: 3822}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011501064291223884, &quot;time-step&quot;: 3823}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001138451392762363, &quot;time-step&quot;: 3824}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011489666067063808, &quot;time-step&quot;: 3825}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011373121524229646, &quot;time-step&quot;: 3826}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011478116502985358, &quot;time-step&quot;: 3827}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001136145438067615, &quot;time-step&quot;: 3828}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001146635739132762, &quot;time-step&quot;: 3829}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011349947890266776, &quot;time-step&quot;: 3830}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011454909108579159, &quot;time-step&quot;: 3831}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011338551994413137, &quot;time-step&quot;: 3832}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011443361872807145, &quot;time-step&quot;: 3833}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011326931416988373, &quot;time-step&quot;: 3834}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011431784369051456, &quot;time-step&quot;: 3835}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011315522715449333, &quot;time-step&quot;: 3836}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001142020570114255, &quot;time-step&quot;: 3837}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011304094223305583, &quot;time-step&quot;: 3838}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011408915743231773, &quot;time-step&quot;: 3839}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011292772833257914, &quot;time-step&quot;: 3840}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011397331254556775, &quot;time-step&quot;: 3841}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001128125237300992, &quot;time-step&quot;: 3842}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011385788675397635, &quot;time-step&quot;: 3843}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011269788956269622, &quot;time-step&quot;: 3844}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011374294990673661, &quot;time-step&quot;: 3845}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011258538579568267, &quot;time-step&quot;: 3846}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011363006196916103, &quot;time-step&quot;: 3847}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011247254442423582, &quot;time-step&quot;: 3848}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011351672001183033, &quot;time-step&quot;: 3849}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011236123973503709, &quot;time-step&quot;: 3850}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011340465862303972, &quot;time-step&quot;: 3851}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011224832851439714, &quot;time-step&quot;: 3852}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001132905832491815, &quot;time-step&quot;: 3853}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001121356151998043, &quot;time-step&quot;: 3854}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011317864991724491, &quot;time-step&quot;: 3855}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011202406603842974, &quot;time-step&quot;: 3856}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011306595988571644, &quot;time-step&quot;: 3857}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011191286612302065, &quot;time-step&quot;: 3858}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011295363074168563, &quot;time-step&quot;: 3859}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011179971043020487, &quot;time-step&quot;: 3860}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011283972999081016, &quot;time-step&quot;: 3861}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011168764904141426, &quot;time-step&quot;: 3862}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011272678384557366, &quot;time-step&quot;: 3863}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001115742139518261, &quot;time-step&quot;: 3864}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011261404724791646, &quot;time-step&quot;: 3865}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011146381730213761, &quot;time-step&quot;: 3866}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011250392999500036, &quot;time-step&quot;: 3867}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001113545149564743, &quot;time-step&quot;: 3868}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011239199666306376, &quot;time-step&quot;: 3869}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011124369921162724, &quot;time-step&quot;: 3870}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011228214716538787, &quot;time-step&quot;: 3871}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011113315122202039, &quot;time-step&quot;: 3872}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011217083083465695, &quot;time-step&quot;: 3873}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011102394200861454, &quot;time-step&quot;: 3874}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011206127237528563, &quot;time-step&quot;: 3875}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011091453488916159, &quot;time-step&quot;: 3876}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001119515160098672, &quot;time-step&quot;: 3877}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011080654803663492, &quot;time-step&quot;: 3878}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011184345930814743, &quot;time-step&quot;: 3879}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011069822357967496, &quot;time-step&quot;: 3880}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011173362145200372, &quot;time-step&quot;: 3881}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011058907257393003, &quot;time-step&quot;: 3882}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011162463342770934, &quot;time-step&quot;: 3883}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011048077140003443, &quot;time-step&quot;: 3884}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011151506332680583, &quot;time-step&quot;: 3885}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011037272633984685, &quot;time-step&quot;: 3886}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011140585411339998, &quot;time-step&quot;: 3887}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011026400607079268, &quot;time-step&quot;: 3888}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011129634222015738, &quot;time-step&quot;: 3889}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011015440104529262, &quot;time-step&quot;: 3890}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011118509573861957, &quot;time-step&quot;: 3891}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011004350380972028, &quot;time-step&quot;: 3892}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011107519967481494, &quot;time-step&quot;: 3893}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010993746109306812, &quot;time-step&quot;: 3894}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001109687378630042, &quot;time-step&quot;: 3895}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010983101092278957, &quot;time-step&quot;: 3896}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001108622644096613, &quot;time-step&quot;: 3897}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010972355958074331, &quot;time-step&quot;: 3898}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011075240327045321, &quot;time-step&quot;: 3899}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001096147927455604, &quot;time-step&quot;: 3900}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001106439856812358, &quot;time-step&quot;: 3901}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010950780706480145, &quot;time-step&quot;: 3902}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011053754715248942, &quot;time-step&quot;: 3903}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010940332431346178, &quot;time-step&quot;: 3904}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011043368140235543, &quot;time-step&quot;: 3905}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010929771233350039, &quot;time-step&quot;: 3906}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011032670736312866, &quot;time-step&quot;: 3907}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010919250780716538, &quot;time-step&quot;: 3908}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001102218753658235, &quot;time-step&quot;: 3909}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010908913100138307, &quot;time-step&quot;: 3910}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0011011555325239897, &quot;time-step&quot;: 3911}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010898183099925518, &quot;time-step&quot;: 3912}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001100065652281046, &quot;time-step&quot;: 3913}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001088736462406814, &quot;time-step&quot;: 3914}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010989897418767214, &quot;time-step&quot;: 3915}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010876943124458194, &quot;time-step&quot;: 3916}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010979517828673124, &quot;time-step&quot;: 3917}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010866315569728613, &quot;time-step&quot;: 3918}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010968695860356092, &quot;time-step&quot;: 3919}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010855658911168575, &quot;time-step&quot;: 3920}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010958114871755242, &quot;time-step&quot;: 3921}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010845363140106201, &quot;time-step&quot;: 3922}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010947880800813437, &quot;time-step&quot;: 3923}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010834927670657635, &quot;time-step&quot;: 3924}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010937218321487308, &quot;time-step&quot;: 3925}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010824516648426652, &quot;time-step&quot;: 3926}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010926562827080488, &quot;time-step&quot;: 3927}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010813858825713396, &quot;time-step&quot;: 3928}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010916065657511353, &quot;time-step&quot;: 3929}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010803601471707225, &quot;time-step&quot;: 3930}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010905894450843334, &quot;time-step&quot;: 3931}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010793529218062758, &quot;time-step&quot;: 3932}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010895684827119112, &quot;time-step&quot;: 3933}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010783207835629582, &quot;time-step&quot;: 3934}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010885184165090322, &quot;time-step&quot;: 3935}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010772882960736752, &quot;time-step&quot;: 3936}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010874872095882893, &quot;time-step&quot;: 3937}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010762548772618175, &quot;time-step&quot;: 3938}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010864436626434326, &quot;time-step&quot;: 3939}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010752102825790644, &quot;time-step&quot;: 3940}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010853928979486227, &quot;time-step&quot;: 3941}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010741713922470808, &quot;time-step&quot;: 3942}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010843586642295122, &quot;time-step&quot;: 3943}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010731512447819114, &quot;time-step&quot;: 3944}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010833261767402291, &quot;time-step&quot;: 3945}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010721213184297085, &quot;time-step&quot;: 3946}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001082288334146142, &quot;time-step&quot;: 3947}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010710882488638163, &quot;time-step&quot;: 3948}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010812468826770782, &quot;time-step&quot;: 3949}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010700600687414408, &quot;time-step&quot;: 3950}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010802121832966805, &quot;time-step&quot;: 3951}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010690443450585008, &quot;time-step&quot;: 3952}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010791915701702237, &quot;time-step&quot;: 3953}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010680268751457334, &quot;time-step&quot;: 3954}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010781721211969852, &quot;time-step&quot;: 3955}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010670152259990573, &quot;time-step&quot;: 3956}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001077164663001895, &quot;time-step&quot;: 3957}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010660069528967142, &quot;time-step&quot;: 3958}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010761473095044494, &quot;time-step&quot;: 3959}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010650131152942777, &quot;time-step&quot;: 3960}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010751350782811642, &quot;time-step&quot;: 3961}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010640063555911183, &quot;time-step&quot;: 3962}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010741451987996697, &quot;time-step&quot;: 3963}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010630148462951183, &quot;time-step&quot;: 3964}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010731404181569815, &quot;time-step&quot;: 3965}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010620212415233254, &quot;time-step&quot;: 3966}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010721408762037754, &quot;time-step&quot;: 3967}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010610225144773722, &quot;time-step&quot;: 3968}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010711393551900983, &quot;time-step&quot;: 3969}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010600389214232564, &quot;time-step&quot;: 3970}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010701376013457775, &quot;time-step&quot;: 3971}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010590298334136605, &quot;time-step&quot;: 3972}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010691379429772496, &quot;time-step&quot;: 3973}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010580589296296239, &quot;time-step&quot;: 3974}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010681475978344679, &quot;time-step&quot;: 3975}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010570663725957274, &quot;time-step&quot;: 3976}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010671543423086405, &quot;time-step&quot;: 3977}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010560863884165883, &quot;time-step&quot;: 3978}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001066164462827146, &quot;time-step&quot;: 3979}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010550852166488767, &quot;time-step&quot;: 3980}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010651516495272517, &quot;time-step&quot;: 3981}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010540732182562351, &quot;time-step&quot;: 3982}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010641342960298061, &quot;time-step&quot;: 3983}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010530834551900625, &quot;time-step&quot;: 3984}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010631593177095056, &quot;time-step&quot;: 3985}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010521153453737497, &quot;time-step&quot;: 3986}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001062180264852941, &quot;time-step&quot;: 3987}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001051144558005035, &quot;time-step&quot;: 3988}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010612005135044456, &quot;time-step&quot;: 3989}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010501730721443892, &quot;time-step&quot;: 3990}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010602184338495135, &quot;time-step&quot;: 3991}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010491969296708703, &quot;time-step&quot;: 3992}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010592322796583176, &quot;time-step&quot;: 3993}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001048206933774054, &quot;time-step&quot;: 3994}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010582375107333064, &quot;time-step&quot;: 3995}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010472119320183992, &quot;time-step&quot;: 3996}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010572365717962384, &quot;time-step&quot;: 3997}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010462392820045352, &quot;time-step&quot;: 3998}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010562698589637876, &quot;time-step&quot;: 3999}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001045270822942257, &quot;time-step&quot;: 4000}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010552937164902687, &quot;time-step&quot;: 4001}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010443116771057248, &quot;time-step&quot;: 4002}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001054325490258634, &quot;time-step&quot;: 4003}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010433414718136191, &quot;time-step&quot;: 4004}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010533597087487578, &quot;time-step&quot;: 4005}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010423829080536962, &quot;time-step&quot;: 4006}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010523925302550197, &quot;time-step&quot;: 4007}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010414313292130828, &quot;time-step&quot;: 4008}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010514239547774196, &quot;time-step&quot;: 4009}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010404565837234259, &quot;time-step&quot;: 4010}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001050448976457119, &quot;time-step&quot;: 4011}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010395089630037546, &quot;time-step&quot;: 4012}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010494989110156894, &quot;time-step&quot;: 4013}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010385578498244286, &quot;time-step&quot;: 4014}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010485389502719045, &quot;time-step&quot;: 4015}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001037598354741931, &quot;time-step&quot;: 4016}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010475768940523267, &quot;time-step&quot;: 4017}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001036667381413281, &quot;time-step&quot;: 4018}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010466399835422635, &quot;time-step&quot;: 4019}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010357149876654148, &quot;time-step&quot;: 4020}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010456725722178817, &quot;time-step&quot;: 4021}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010347607312723994, &quot;time-step&quot;: 4022}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010447143577039242, &quot;time-step&quot;: 4023}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010337948333472013, &quot;time-step&quot;: 4024}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001043747179210186, &quot;time-step&quot;: 4025}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010328554781153798, &quot;time-step&quot;: 4026}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010428171372041106, &quot;time-step&quot;: 4027}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010319240391254425, &quot;time-step&quot;: 4028}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010418714955449104, &quot;time-step&quot;: 4029}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010309969075024128, &quot;time-step&quot;: 4030}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010409430833533406, &quot;time-step&quot;: 4031}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010300558060407639, &quot;time-step&quot;: 4032}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010399941820651293, &quot;time-step&quot;: 4033}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001029119361191988, &quot;time-step&quot;: 4034}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010390497045591474, &quot;time-step&quot;: 4035}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010281858267262578, &quot;time-step&quot;: 4036}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001038100104779005, &quot;time-step&quot;: 4037}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010272408835589886, &quot;time-step&quot;: 4038}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010371508542448282, &quot;time-step&quot;: 4039}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010263146832585335, &quot;time-step&quot;: 4040}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010362282628193498, &quot;time-step&quot;: 4041}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010253861546516418, &quot;time-step&quot;: 4042}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010352868121117353, &quot;time-step&quot;: 4043}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010244583245366812, &quot;time-step&quot;: 4044}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010343568865209818, &quot;time-step&quot;: 4045}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010235300287604332, &quot;time-step&quot;: 4046}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010334268445149064, &quot;time-step&quot;: 4047}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010226208250969648, &quot;time-step&quot;: 4048}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010325209004804492, &quot;time-step&quot;: 4049}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010216941591352224, &quot;time-step&quot;: 4050}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001031570602208376, &quot;time-step&quot;: 4051}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010207750601693988, &quot;time-step&quot;: 4052}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010306501062586904, &quot;time-step&quot;: 4053}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010198504896834493, &quot;time-step&quot;: 4054}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010297175031155348, &quot;time-step&quot;: 4055}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001018922426737845, &quot;time-step&quot;: 4056}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010287956101819873, &quot;time-step&quot;: 4057}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010180252138525248, &quot;time-step&quot;: 4058}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001027901889756322, &quot;time-step&quot;: 4059}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010171179892495275, &quot;time-step&quot;: 4060}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010269597405567765, &quot;time-step&quot;: 4061}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010161795653402805, &quot;time-step&quot;: 4062}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010260299313813448, &quot;time-step&quot;: 4063}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010152659378945827, &quot;time-step&quot;: 4064}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010251067578792572, &quot;time-step&quot;: 4065}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001014358946122229, &quot;time-step&quot;: 4066}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010241966228932142, &quot;time-step&quot;: 4067}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010134532349184155, &quot;time-step&quot;: 4068}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010232905624434352, &quot;time-step&quot;: 4069}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010125479893758893, &quot;time-step&quot;: 4070}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001022369833663106, &quot;time-step&quot;: 4071}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001011635409668088, &quot;time-step&quot;: 4072}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001021475763991475, &quot;time-step&quot;: 4073}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010107445996254683, &quot;time-step&quot;: 4074}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010205587605014443, &quot;time-step&quot;: 4075}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010098523925989866, &quot;time-step&quot;: 4076}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001019659568555653, &quot;time-step&quot;: 4077}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010089395800605416, &quot;time-step&quot;: 4078}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010187564184889197, &quot;time-step&quot;: 4079}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010080531938001513, &quot;time-step&quot;: 4080}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001017855480313301, &quot;time-step&quot;: 4081}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010071602882817388, &quot;time-step&quot;: 4082}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010169737506657839, &quot;time-step&quot;: 4083}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010062877554446459, &quot;time-step&quot;: 4084}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001016096444800496, &quot;time-step&quot;: 4085}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001005417201668024, &quot;time-step&quot;: 4086}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001015196554362774, &quot;time-step&quot;: 4087}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001004502410069108, &quot;time-step&quot;: 4088}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010142833925783634, &quot;time-step&quot;: 4089}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010036107851192355, &quot;time-step&quot;: 4090}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010134021285921335, &quot;time-step&quot;: 4091}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010027396492660046, &quot;time-step&quot;: 4092}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010125181870535016, &quot;time-step&quot;: 4093}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010018546599894762, &quot;time-step&quot;: 4094}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010116127086803317, &quot;time-step&quot;: 4095}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010009577963501215, &quot;time-step&quot;: 4096}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.001010722597129643, &quot;time-step&quot;: 4097}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010000705951824784, &quot;time-step&quot;: 4098}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010098177008330822, &quot;time-step&quot;: 4099}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009991828119382262, &quot;time-step&quot;: 4100}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010089363204315305, &quot;time-step&quot;: 4101}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009983174968510866, &quot;time-step&quot;: 4102}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010080579668283463, &quot;time-step&quot;: 4103}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009974315762519836, &quot;time-step&quot;: 4104}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010071861324831843, &quot;time-step&quot;: 4105}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009965626522898674, &quot;time-step&quot;: 4106}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010062837973237038, &quot;time-step&quot;: 4107}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009956805733963847, &quot;time-step&quot;: 4108}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010054062586277723, &quot;time-step&quot;: 4109}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009947980288416147, &quot;time-step&quot;: 4110}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010045134695246816, &quot;time-step&quot;: 4111}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009939305018633604, &quot;time-step&quot;: 4112}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010036519961431623, &quot;time-step&quot;: 4113}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009930754313245416, &quot;time-step&quot;: 4114}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00100279925391078, &quot;time-step&quot;: 4115}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000992211396805942, &quot;time-step&quot;: 4116}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010019252076745033, &quot;time-step&quot;: 4117}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009913541143760085, &quot;time-step&quot;: 4118}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010010587284341455, &quot;time-step&quot;: 4119}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009904926409944892, &quot;time-step&quot;: 4120}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0010002031922340393, &quot;time-step&quot;: 4121}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009896514238789678, &quot;time-step&quot;: 4122}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000999334966763854, &quot;time-step&quot;: 4123}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009887834312394261, &quot;time-step&quot;: 4124}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009984729113057256, &quot;time-step&quot;: 4125}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000987933250144124, &quot;time-step&quot;: 4126}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009976205183193088, &quot;time-step&quot;: 4127}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009870787616819143, &quot;time-step&quot;: 4128}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000996753922663629, &quot;time-step&quot;: 4129}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009862181032076478, &quot;time-step&quot;: 4130}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009958820883184671, &quot;time-step&quot;: 4131}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009853558149188757, &quot;time-step&quot;: 4132}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009950214298442006, &quot;time-step&quot;: 4133}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000984497251920402, &quot;time-step&quot;: 4134}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009941579774022102, &quot;time-step&quot;: 4135}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000983659760095179, &quot;time-step&quot;: 4136}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009933304972946644, &quot;time-step&quot;: 4137}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009828354232013226, &quot;time-step&quot;: 4138}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009924826445057988, &quot;time-step&quot;: 4139}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009819810511544347, &quot;time-step&quot;: 4140}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009916271083056927, &quot;time-step&quot;: 4141}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009811394847929478, &quot;time-step&quot;: 4142}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00099076796323061, &quot;time-step&quot;: 4143}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009802732383832335, &quot;time-step&quot;: 4144}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009899046272039413, &quot;time-step&quot;: 4145}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009794299257919192, &quot;time-step&quot;: 4146}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009890655055642128, &quot;time-step&quot;: 4147}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009785934817045927, &quot;time-step&quot;: 4148}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009882162557914853, &quot;time-step&quot;: 4149}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009777456289157271, &quot;time-step&quot;: 4150}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009873738745227456, &quot;time-step&quot;: 4151}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009769366588443518, &quot;time-step&quot;: 4152}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000986558268778026, &quot;time-step&quot;: 4153}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009761034161783755, &quot;time-step&quot;: 4154}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009856934193521738, &quot;time-step&quot;: 4155}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009752378682605922, &quot;time-step&quot;: 4156}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009848377667367458, &quot;time-step&quot;: 4157}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000974408583715558, &quot;time-step&quot;: 4158}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009840053971856833, &quot;time-step&quot;: 4159}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009735747589729726, &quot;time-step&quot;: 4160}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009831631323322654, &quot;time-step&quot;: 4161}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009727348806336522, &quot;time-step&quot;: 4162}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000982321216724813, &quot;time-step&quot;: 4163}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009719079826027155, &quot;time-step&quot;: 4164}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000981486402451992, &quot;time-step&quot;: 4165}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009710959857329726, &quot;time-step&quot;: 4166}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00098067382350564, &quot;time-step&quot;: 4167}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000970270368270576, &quot;time-step&quot;: 4168}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000979844480752945, &quot;time-step&quot;: 4169}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000969455111771822, &quot;time-step&quot;: 4170}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009790195617824793, &quot;time-step&quot;: 4171}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009686480043455958, &quot;time-step&quot;: 4172}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009782069828361273, &quot;time-step&quot;: 4173}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000967813772149384, &quot;time-step&quot;: 4174}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009773575002327561, &quot;time-step&quot;: 4175}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000966966210398823, &quot;time-step&quot;: 4176}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009765083668753505, &quot;time-step&quot;: 4177}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009661578224040568, &quot;time-step&quot;: 4178}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009757119114510715, &quot;time-step&quot;: 4179}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009653521701693535, &quot;time-step&quot;: 4180}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009748822776600718, &quot;time-step&quot;: 4181}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009645199170336127, &quot;time-step&quot;: 4182}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009740650421008468, &quot;time-step&quot;: 4183}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009637178736738861, &quot;time-step&quot;: 4184}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009732379112392664, &quot;time-step&quot;: 4185}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009628997649997473, &quot;time-step&quot;: 4186}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009724383126012981, &quot;time-step&quot;: 4187}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000962116988375783, &quot;time-step&quot;: 4188}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000971640576608479, &quot;time-step&quot;: 4189}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009613113943487406, &quot;time-step&quot;: 4190}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009708256693556905, &quot;time-step&quot;: 4191}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009605116210877895, &quot;time-step&quot;: 4192}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009700249647721648, &quot;time-step&quot;: 4193}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009597132448107004, &quot;time-step&quot;: 4194}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009692215244285762, &quot;time-step&quot;: 4195}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009589123073965311, &quot;time-step&quot;: 4196}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009684164542704821, &quot;time-step&quot;: 4197}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009581116610206664, &quot;time-step&quot;: 4198}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009676019544713199, &quot;time-step&quot;: 4199}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009573063580319285, &quot;time-step&quot;: 4200}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009667865233495831, &quot;time-step&quot;: 4201}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000956486095674336, &quot;time-step&quot;: 4202}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009659615461714566, &quot;time-step&quot;: 4203}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009556766017340124, &quot;time-step&quot;: 4204}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009651707368902862, &quot;time-step&quot;: 4205}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009548973757773638, &quot;time-step&quot;: 4206}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009643640369176865, &quot;time-step&quot;: 4207}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000954088696744293, &quot;time-step&quot;: 4208}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009635519236326218, &quot;time-step&quot;: 4209}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009532942203804851, &quot;time-step&quot;: 4210}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009627615800127387, &quot;time-step&quot;: 4211}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000952512607909739, &quot;time-step&quot;: 4212}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009619636693969369, &quot;time-step&quot;: 4213}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009517205762676895, &quot;time-step&quot;: 4214}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009611680870875716, &quot;time-step&quot;: 4215}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009509245865046978, &quot;time-step&quot;: 4216}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009603723883628845, &quot;time-step&quot;: 4217}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009501415188424289, &quot;time-step&quot;: 4218}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009595869923941791, &quot;time-step&quot;: 4219}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000949349720031023, &quot;time-step&quot;: 4220}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009587820386514068, &quot;time-step&quot;: 4221}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009485518676228821, &quot;time-step&quot;: 4222}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009579689940437675, &quot;time-step&quot;: 4223}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009477512794546783, &quot;time-step&quot;: 4224}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000957177544478327, &quot;time-step&quot;: 4225}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009469783981330693, &quot;time-step&quot;: 4226}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009564204956404865, &quot;time-step&quot;: 4227}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009462317684665322, &quot;time-step&quot;: 4228}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009556549484841526, &quot;time-step&quot;: 4229}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009454588871449232, &quot;time-step&quot;: 4230}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009548672242090106, &quot;time-step&quot;: 4231}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000944662606343627, &quot;time-step&quot;: 4232}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009540630271658301, &quot;time-step&quot;: 4233}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009438811102882028, &quot;time-step&quot;: 4234}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009532778640277684, &quot;time-step&quot;: 4235}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009430953650735319, &quot;time-step&quot;: 4236}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009524952038191259, &quot;time-step&quot;: 4237}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009423217852599919, &quot;time-step&quot;: 4238}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009517119615338743, &quot;time-step&quot;: 4239}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009415608365088701, &quot;time-step&quot;: 4240}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009509448427706957, &quot;time-step&quot;: 4241}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009407891193404794, &quot;time-step&quot;: 4242}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009501733584329486, &quot;time-step&quot;: 4243}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009400239796377718, &quot;time-step&quot;: 4244}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009493960533291101, &quot;time-step&quot;: 4245}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009392635547555983, &quot;time-step&quot;: 4246}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009486516937613487, &quot;time-step&quot;: 4247}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009385077282786369, &quot;time-step&quot;: 4248}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009478702559135854, &quot;time-step&quot;: 4249}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009377454989589751, &quot;time-step&quot;: 4250}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000947121880017221, &quot;time-step&quot;: 4251}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009370037587359548, &quot;time-step&quot;: 4252}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009463470196351409, &quot;time-step&quot;: 4253}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009362204000353813, &quot;time-step&quot;: 4254}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009455704130232334, &quot;time-step&quot;: 4255}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009354619542136788, &quot;time-step&quot;: 4256}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009448057971894741, &quot;time-step&quot;: 4257}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009346966398879886, &quot;time-step&quot;: 4258}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009440433350391686, &quot;time-step&quot;: 4259}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009339424432255328, &quot;time-step&quot;: 4260}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009432808146812022, &quot;time-step&quot;: 4261}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009331958135589957, &quot;time-step&quot;: 4262}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009425249882042408, &quot;time-step&quot;: 4263}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009324466809630394, &quot;time-step&quot;: 4264}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009417873225174844, &quot;time-step&quot;: 4265}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009317179210484028, &quot;time-step&quot;: 4266}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009410367347300053, &quot;time-step&quot;: 4267}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009309586021117866, &quot;time-step&quot;: 4268}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009402649593539536, &quot;time-step&quot;: 4269}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009301950922235847, &quot;time-step&quot;: 4270}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009395022061653435, &quot;time-step&quot;: 4271}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009294436895288527, &quot;time-step&quot;: 4272}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009387583704665303, &quot;time-step&quot;: 4273}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009287044522352517, &quot;time-step&quot;: 4274}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009380004485137761, &quot;time-step&quot;: 4275}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009279532823711634, &quot;time-step&quot;: 4276}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009372620261274278, &quot;time-step&quot;: 4277}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009272209135815501, &quot;time-step&quot;: 4278}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009365107398480177, &quot;time-step&quot;: 4279}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009264668915420771, &quot;time-step&quot;: 4280}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009357536910101771, &quot;time-step&quot;: 4281}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000925721600651741, &quot;time-step&quot;: 4282}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009349931497126818, &quot;time-step&quot;: 4283}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009249647264368832, &quot;time-step&quot;: 4284}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009342411649413407, &quot;time-step&quot;: 4285}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009242198430001736, &quot;time-step&quot;: 4286}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009334880742244422, &quot;time-step&quot;: 4287}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009234729805029929, &quot;time-step&quot;: 4288}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009327437728643417, &quot;time-step&quot;: 4289}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009227562695741653, &quot;time-step&quot;: 4290}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009320084936916828, &quot;time-step&quot;: 4291}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009220055071637034, &quot;time-step&quot;: 4292}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009312704205513, &quot;time-step&quot;: 4293}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009212850127369165, &quot;time-step&quot;: 4294}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009305333369411528, &quot;time-step&quot;: 4295}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009205455426126719, &quot;time-step&quot;: 4296}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009297898504883051, &quot;time-step&quot;: 4297}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009198174229823053, &quot;time-step&quot;: 4298}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009290652815252542, &quot;time-step&quot;: 4299}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009190825512632728, &quot;time-step&quot;: 4300}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009283167310059071, &quot;time-step&quot;: 4301}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009183399961329997, &quot;time-step&quot;: 4302}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009275597985833883, &quot;time-step&quot;: 4303}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009175911545753479, &quot;time-step&quot;: 4304}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009268190478906035, &quot;time-step&quot;: 4305}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009168776450678706, &quot;time-step&quot;: 4306}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000926105072721839, &quot;time-step&quot;: 4307}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009161586640402675, &quot;time-step&quot;: 4308}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000925378524698317, &quot;time-step&quot;: 4309}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009154443978331983, &quot;time-step&quot;: 4310}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009246690315194428, &quot;time-step&quot;: 4311}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009147304808720946, &quot;time-step&quot;: 4312}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009239264763891697, &quot;time-step&quot;: 4313}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009139913599938154, &quot;time-step&quot;: 4314}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009231861331500113, &quot;time-step&quot;: 4315}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009132615523412824, &quot;time-step&quot;: 4316}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009224583627656102, &quot;time-step&quot;: 4317}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009125572396442294, &quot;time-step&quot;: 4318}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009217523620463908, &quot;time-step&quot;: 4319}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009118432062678039, &quot;time-step&quot;: 4320}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009210267453454435, &quot;time-step&quot;: 4321}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009111107792705297, &quot;time-step&quot;: 4322}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009202876244671643, &quot;time-step&quot;: 4323}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009103792835958302, &quot;time-step&quot;: 4324}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009195560123771429, &quot;time-step&quot;: 4325}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009096745634451509, &quot;time-step&quot;: 4326}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009188415133394301, &quot;time-step&quot;: 4327}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009089552331715822, &quot;time-step&quot;: 4328}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000918135279789567, &quot;time-step&quot;: 4329}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009082644828595221, &quot;time-step&quot;: 4330}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00091742625227198, &quot;time-step&quot;: 4331}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009075470152311027, &quot;time-step&quot;: 4332}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009167017415165901, &quot;time-step&quot;: 4333}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009068435174413025, &quot;time-step&quot;: 4334}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009160107001662254, &quot;time-step&quot;: 4335}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009061516611836851, &quot;time-step&quot;: 4336}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009152917773462832, &quot;time-step&quot;: 4337}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009054335532709956, &quot;time-step&quot;: 4338}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000914566102437675, &quot;time-step&quot;: 4339}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000904716202057898, &quot;time-step&quot;: 4340}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000913848343770951, &quot;time-step&quot;: 4341}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009039970464073122, &quot;time-step&quot;: 4342}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009131233673542738, &quot;time-step&quot;: 4343}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009032884263433516, &quot;time-step&quot;: 4344}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009124246425926685, &quot;time-step&quot;: 4345}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009025913896039128, &quot;time-step&quot;: 4346}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000911719398573041, &quot;time-step&quot;: 4347}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009019023273140192, &quot;time-step&quot;: 4348}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009110050741583109, &quot;time-step&quot;: 4349}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009011728689074516, &quot;time-step&quot;: 4350}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009102858603000641, &quot;time-step&quot;: 4351}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009004819439724088, &quot;time-step&quot;: 4352}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009095821296796203, &quot;time-step&quot;: 4353}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008997613331303, &quot;time-step&quot;: 4354}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009088508086279035, &quot;time-step&quot;: 4355}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008990606293082237, &quot;time-step&quot;: 4356}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009081698954105377, &quot;time-step&quot;: 4357}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008983804727904499, &quot;time-step&quot;: 4358}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009074897388927639, &quot;time-step&quot;: 4359}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000897702295333147, &quot;time-step&quot;: 4360}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000906786706764251, &quot;time-step&quot;: 4361}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008970040362328291, &quot;time-step&quot;: 4362}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000906085129827261, &quot;time-step&quot;: 4363}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008963050204329193, &quot;time-step&quot;: 4364}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009053690009750426, &quot;time-step&quot;: 4365}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008955879602581263, &quot;time-step&quot;: 4366}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009046641644090414, &quot;time-step&quot;: 4367}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000894898665137589, &quot;time-step&quot;: 4368}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000903973588719964, &quot;time-step&quot;: 4369}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008942100103013217, &quot;time-step&quot;: 4370}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000903282780200243, &quot;time-step&quot;: 4371}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008935281075537205, &quot;time-step&quot;: 4372}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009025875478982925, &quot;time-step&quot;: 4373}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000892851734533906, &quot;time-step&quot;: 4374}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009019084391184151, &quot;time-step&quot;: 4375}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008921701228246093, &quot;time-step&quot;: 4376}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009012168738991022, &quot;time-step&quot;: 4377}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008914853679016232, &quot;time-step&quot;: 4378}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0009005463216453791, &quot;time-step&quot;: 4379}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008908312884159386, &quot;time-step&quot;: 4380}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008998660487122834, &quot;time-step&quot;: 4381}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008901404798962176, &quot;time-step&quot;: 4382}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008991853101179004, &quot;time-step&quot;: 4383}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008894777856767178, &quot;time-step&quot;: 4384}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008985226741060615, &quot;time-step&quot;: 4385}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008888199226930737, &quot;time-step&quot;: 4386}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008978443220257759, &quot;time-step&quot;: 4387}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008881303947418928, &quot;time-step&quot;: 4388}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008971535135060549, &quot;time-step&quot;: 4389}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008874682825990021, &quot;time-step&quot;: 4390}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008964883745647967, &quot;time-step&quot;: 4391}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008867866126820445, &quot;time-step&quot;: 4392}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008957790560089052, &quot;time-step&quot;: 4393}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008860797388479114, &quot;time-step&quot;: 4394}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008950804476626217, &quot;time-step&quot;: 4395}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008854022598825395, &quot;time-step&quot;: 4396}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008944175788201392, &quot;time-step&quot;: 4397}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008847502176649868, &quot;time-step&quot;: 4398}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008937540696933866, &quot;time-step&quot;: 4399}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008840932277962565, &quot;time-step&quot;: 4400}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008930881740525365, &quot;time-step&quot;: 4401}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008834125474095345, &quot;time-step&quot;: 4402}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008923909044824541, &quot;time-step&quot;: 4403}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00088272470748052, &quot;time-step&quot;: 4404}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008916975930333138, &quot;time-step&quot;: 4405}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000882050720974803, &quot;time-step&quot;: 4406}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008910453179851174, &quot;time-step&quot;: 4407}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008814187604002655, &quot;time-step&quot;: 4408}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000890398514457047, &quot;time-step&quot;: 4409}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008807433769106865, &quot;time-step&quot;: 4410}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000889707705937326, &quot;time-step&quot;: 4411}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000880085863173008, &quot;time-step&quot;: 4412}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000889045768417418, &quot;time-step&quot;: 4413}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008794106543064117, &quot;time-step&quot;: 4414}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008883799309842288, &quot;time-step&quot;: 4415}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008787724655121565, &quot;time-step&quot;: 4416}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008877222426235676, &quot;time-step&quot;: 4417}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008781144861131907, &quot;time-step&quot;: 4418}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008870680467225611, &quot;time-step&quot;: 4419}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008774591260589659, &quot;time-step&quot;: 4420}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008864160045050085, &quot;time-step&quot;: 4421}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008768195984885097, &quot;time-step&quot;: 4422}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008857722859829664, &quot;time-step&quot;: 4423}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008761726785451174, &quot;time-step&quot;: 4424}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008851016173139215, &quot;time-step&quot;: 4425}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008754939772188663, &quot;time-step&quot;: 4426}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008844303665682673, &quot;time-step&quot;: 4427}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008748503169044852, &quot;time-step&quot;: 4428}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008837825153023005, &quot;time-step&quot;: 4429}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008742095669731498, &quot;time-step&quot;: 4430}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008831327431835234, &quot;time-step&quot;: 4431}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008735531591810286, &quot;time-step&quot;: 4432}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008824790129438043, &quot;time-step&quot;: 4433}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008729081018827856, &quot;time-step&quot;: 4434}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008818241767585278, &quot;time-step&quot;: 4435}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000872271484695375, &quot;time-step&quot;: 4436}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008811839506961405, &quot;time-step&quot;: 4437}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008716199663467705, &quot;time-step&quot;: 4438}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008805149118416011, &quot;time-step&quot;: 4439}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008709533140063286, &quot;time-step&quot;: 4440}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008798505295999348, &quot;time-step&quot;: 4441}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008703077910467982, &quot;time-step&quot;: 4442}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00087919388897717, &quot;time-step&quot;: 4443}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008696273434907198, &quot;time-step&quot;: 4444}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008785159443505108, &quot;time-step&quot;: 4445}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000868993520271033, &quot;time-step&quot;: 4446}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008778921328485012, &quot;time-step&quot;: 4447}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008683633059263229, &quot;time-step&quot;: 4448}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008772372384555638, &quot;time-step&quot;: 4449}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008677156874909997, &quot;time-step&quot;: 4450}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008765948587097228, &quot;time-step&quot;: 4451}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008670897805131972, &quot;time-step&quot;: 4452}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008759687189012766, &quot;time-step&quot;: 4453}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008664510678499937, &quot;time-step&quot;: 4454}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008753171423450112, &quot;time-step&quot;: 4455}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008658156148158014, &quot;time-step&quot;: 4456}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008746766252443194, &quot;time-step&quot;: 4457}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008651806274428964, &quot;time-step&quot;: 4458}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008740589837543666, &quot;time-step&quot;: 4459}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008645702619105577, &quot;time-step&quot;: 4460}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008734248112887144, &quot;time-step&quot;: 4461}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000863931025378406, &quot;time-step&quot;: 4462}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000872795470058918, &quot;time-step&quot;: 4463}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008633060497231781, &quot;time-step&quot;: 4464}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008721580961719155, &quot;time-step&quot;: 4465}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008626726921647787, &quot;time-step&quot;: 4466}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000871507974807173, &quot;time-step&quot;: 4467}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008620311855338514, &quot;time-step&quot;: 4468}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008708664681762457, &quot;time-step&quot;: 4469}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008613920072093606, &quot;time-step&quot;: 4470}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008702331688255072, &quot;time-step&quot;: 4471}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008607734343968332, &quot;time-step&quot;: 4472}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008696023724041879, &quot;time-step&quot;: 4473}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000860140600707382, &quot;time-step&quot;: 4474}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008689601090736687, &quot;time-step&quot;: 4475}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008595070685259998, &quot;time-step&quot;: 4476}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008683206979185343, &quot;time-step&quot;: 4477}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008588631171733141, &quot;time-step&quot;: 4478}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008676659199409187, &quot;time-step&quot;: 4479}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008582206210121512, &quot;time-step&quot;: 4480}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000867040769662708, &quot;time-step&quot;: 4481}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008576078107580543, &quot;time-step&quot;: 4482}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008664300548844039, &quot;time-step&quot;: 4483}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008570037898607552, &quot;time-step&quot;: 4484}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008658011211082339, &quot;time-step&quot;: 4485}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008563750307075679, &quot;time-step&quot;: 4486}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008651823154650629, &quot;time-step&quot;: 4487}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008557676337659359, &quot;time-step&quot;: 4488}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008645495981909335, &quot;time-step&quot;: 4489}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008551342762075365, &quot;time-step&quot;: 4490}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000863923691213131, &quot;time-step&quot;: 4491}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008545169839635491, &quot;time-step&quot;: 4492}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008632965618744493, &quot;time-step&quot;: 4493}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008539026021026075, &quot;time-step&quot;: 4494}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008626780472695827, &quot;time-step&quot;: 4495}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008532756473869085, &quot;time-step&quot;: 4496}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008620606968179345, &quot;time-step&quot;: 4497}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008526740712113678, &quot;time-step&quot;: 4498}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008614464895799756, &quot;time-step&quot;: 4499}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008520514820702374, &quot;time-step&quot;: 4500}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008607974159531295, &quot;time-step&quot;: 4501}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008514116052538157, &quot;time-step&quot;: 4502}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008601770387031138, &quot;time-step&quot;: 4503}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008508003083989024, &quot;time-step&quot;: 4504}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008595670224167407, &quot;time-step&quot;: 4505}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008502034470438957, &quot;time-step&quot;: 4506}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008589477511122823, &quot;time-step&quot;: 4507}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00084958371007815, &quot;time-step&quot;: 4508}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008583224262110889, &quot;time-step&quot;: 4509}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008489691535942256, &quot;time-step&quot;: 4510}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008577101398259401, &quot;time-step&quot;: 4511}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008483532583341002, &quot;time-step&quot;: 4512}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008570852223783731, &quot;time-step&quot;: 4513}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008477485389448702, &quot;time-step&quot;: 4514}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008564950549043715, &quot;time-step&quot;: 4515}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008471650071442127, &quot;time-step&quot;: 4516}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008558992994949222, &quot;time-step&quot;: 4517}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000846564827952534, &quot;time-step&quot;: 4518}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008552903309464455, &quot;time-step&quot;: 4519}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008459521923214197, &quot;time-step&quot;: 4520}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008546652970835567, &quot;time-step&quot;: 4521}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008453510818071663, &quot;time-step&quot;: 4522}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008540679118596017, &quot;time-step&quot;: 4523}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008447431027889252, &quot;time-step&quot;: 4524}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008534665103070438, &quot;time-step&quot;: 4525}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008441524696536362, &quot;time-step&quot;: 4526}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008528555044904351, &quot;time-step&quot;: 4527}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008435453055426478, &quot;time-step&quot;: 4528}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008522426942363381, &quot;time-step&quot;: 4529}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008429407025687397, &quot;time-step&quot;: 4530}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008516497910022736, &quot;time-step&quot;: 4531}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000842363340780139, &quot;time-step&quot;: 4532}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008510589832440019, &quot;time-step&quot;: 4533}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008417592034675181, &quot;time-step&quot;: 4534}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008504593861289322, &quot;time-step&quot;: 4535}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008411813178099692, &quot;time-step&quot;: 4536}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008498742827214301, &quot;time-step&quot;: 4537}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008405852131545544, &quot;time-step&quot;: 4538}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000849263567943126, &quot;time-step&quot;: 4539}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008399906218983233, &quot;time-step&quot;: 4540}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008486623992212117, &quot;time-step&quot;: 4541}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008393876487389207, &quot;time-step&quot;: 4542}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008480542455799878, &quot;time-step&quot;: 4543}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008387928828597069, &quot;time-step&quot;: 4544}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000847457442432642, &quot;time-step&quot;: 4545}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008382006781175733, &quot;time-step&quot;: 4546}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008468727464787662, &quot;time-step&quot;: 4547}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008376094046980143, &quot;time-step&quot;: 4548}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008462603436782956, &quot;time-step&quot;: 4549}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008370220893993974, &quot;time-step&quot;: 4550}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008456825744360685, &quot;time-step&quot;: 4551}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008364475797861814, &quot;time-step&quot;: 4552}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008450956083834171, &quot;time-step&quot;: 4553}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008358572376891971, &quot;time-step&quot;: 4554}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008445002604275942, &quot;time-step&quot;: 4555}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008352588047273457, &quot;time-step&quot;: 4556}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008438960649073124, &quot;time-step&quot;: 4557}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008346660761162639, &quot;time-step&quot;: 4558}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008433029870502651, &quot;time-step&quot;: 4559}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008340826025232673, &quot;time-step&quot;: 4560}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008427105494774878, &quot;time-step&quot;: 4561}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008334862068295479, &quot;time-step&quot;: 4562}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008421142119914293, &quot;time-step&quot;: 4563}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008328973781317472, &quot;time-step&quot;: 4564}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008415285847149789, &quot;time-step&quot;: 4565}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008323254296556115, &quot;time-step&quot;: 4566}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008409471483901143, &quot;time-step&quot;: 4567}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008317477768287063, &quot;time-step&quot;: 4568}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008403559913858771, &quot;time-step&quot;: 4569}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008311660494655371, &quot;time-step&quot;: 4570}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008397795609198511, &quot;time-step&quot;: 4571}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008305845549330115, &quot;time-step&quot;: 4572}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008391889277845621, &quot;time-step&quot;: 4573}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008300010813400149, &quot;time-step&quot;: 4574}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008386042900383472, &quot;time-step&quot;: 4575}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00082944001769647, &quot;time-step&quot;: 4576}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008380459039472044, &quot;time-step&quot;: 4577}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008288688841275871, &quot;time-step&quot;: 4578}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008374606259167194, &quot;time-step&quot;: 4579}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008282907074317336, &quot;time-step&quot;: 4580}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008368752896785736, &quot;time-step&quot;: 4581}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000827704556286335, &quot;time-step&quot;: 4582}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008362854132428765, &quot;time-step&quot;: 4583}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000827122014015913, &quot;time-step&quot;: 4584}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000835695187561214, &quot;time-step&quot;: 4585}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008265498909167945, &quot;time-step&quot;: 4586}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000835130806080997, &quot;time-step&quot;: 4587}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008259870810434222, &quot;time-step&quot;: 4588}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008345509995706379, &quot;time-step&quot;: 4589}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008253940031863749, &quot;time-step&quot;: 4590}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008339622290804982, &quot;time-step&quot;: 4591}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008248113445006311, &quot;time-step&quot;: 4592}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000833389931358397, &quot;time-step&quot;: 4593}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008242673939093947, &quot;time-step&quot;: 4594}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008328226977027953, &quot;time-step&quot;: 4595}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008236901485361159, &quot;time-step&quot;: 4596}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008322346839122474, &quot;time-step&quot;: 4597}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008231059182435274, &quot;time-step&quot;: 4598}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008316482417285442, &quot;time-step&quot;: 4599}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008225249475799501, &quot;time-step&quot;: 4600}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008310600533150136, &quot;time-step&quot;: 4601}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000821957248263061, &quot;time-step&quot;: 4602}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008305070223286748, &quot;time-step&quot;: 4603}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008214185363613069, &quot;time-step&quot;: 4604}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008299561450257897, &quot;time-step&quot;: 4605}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008208481594920158, &quot;time-step&quot;: 4606}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008293733699247241, &quot;time-step&quot;: 4607}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008202592143788934, &quot;time-step&quot;: 4608}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008287790114991367, &quot;time-step&quot;: 4609}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008196987328119576, &quot;time-step&quot;: 4610}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000828234595246613, &quot;time-step&quot;: 4611}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000819141510874033, &quot;time-step&quot;: 4612}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000827659503556788, &quot;time-step&quot;: 4613}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008185853948816657, &quot;time-step&quot;: 4614}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000827107927761972, &quot;time-step&quot;: 4615}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008180219447240233, &quot;time-step&quot;: 4616}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008265237556770444, &quot;time-step&quot;: 4617}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008174499962478876, &quot;time-step&quot;: 4618}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00082595698768273, &quot;time-step&quot;: 4619}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008169024367816746, &quot;time-step&quot;: 4620}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008254183921962976, &quot;time-step&quot;: 4621}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008163698948919773, &quot;time-step&quot;: 4622}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008248690865002573, &quot;time-step&quot;: 4623}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008158180862665176, &quot;time-step&quot;: 4624}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008243228658102453, &quot;time-step&quot;: 4625}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008152733789756894, &quot;time-step&quot;: 4626}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008237621514126658, &quot;time-step&quot;: 4627}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008147097541950643, &quot;time-step&quot;: 4628}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008232034160755575, &quot;time-step&quot;: 4629}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008141545695252717, &quot;time-step&quot;: 4630}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008226248901337385, &quot;time-step&quot;: 4631}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008135936222970486, &quot;time-step&quot;: 4632}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008220779127441347, &quot;time-step&quot;: 4633}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000813043094240129, &quot;time-step&quot;: 4634}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008215120760723948, &quot;time-step&quot;: 4635}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008124860469251871, &quot;time-step&quot;: 4636}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000820955669041723, &quot;time-step&quot;: 4637}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008119448903016746, &quot;time-step&quot;: 4638}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008204197511076927, &quot;time-step&quot;: 4639}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008114215452224016, &quot;time-step&quot;: 4640}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008198840077966452, &quot;time-step&quot;: 4641}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008108728216029704, &quot;time-step&quot;: 4642}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008193308021873236, &quot;time-step&quot;: 4643}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008103234576992691, &quot;time-step&quot;: 4644}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008187835337594151, &quot;time-step&quot;: 4645}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008097919053398073, &quot;time-step&quot;: 4646}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008182514575310051, &quot;time-step&quot;: 4647}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008092586067505181, &quot;time-step&quot;: 4648}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008177008712664247, &quot;time-step&quot;: 4649}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000808709766715765, &quot;time-step&quot;: 4650}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008171480149030685, &quot;time-step&quot;: 4651}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008081553387455642, &quot;time-step&quot;: 4652}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008165886392816901, &quot;time-step&quot;: 4653}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00080761534627527, &quot;time-step&quot;: 4654}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008160392171703279, &quot;time-step&quot;: 4655}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008070527110248804, &quot;time-step&quot;: 4656}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008154729148373008, &quot;time-step&quot;: 4657}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008064954308792949, &quot;time-step&quot;: 4658}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008149194763973355, &quot;time-step&quot;: 4659}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008059628307819366, &quot;time-step&quot;: 4660}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008143901941366494, &quot;time-step&quot;: 4661}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008054433856159449, &quot;time-step&quot;: 4662}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008138720877468586, &quot;time-step&quot;: 4663}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008049173629842699, &quot;time-step&quot;: 4664}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008133308147080243, &quot;time-step&quot;: 4665}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008043849375098944, &quot;time-step&quot;: 4666}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008128010085783899, &quot;time-step&quot;: 4667}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008038586238399148, &quot;time-step&quot;: 4668}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00081225874600932, &quot;time-step&quot;: 4669}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008033282938413322, &quot;time-step&quot;: 4670}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008117378456518054, &quot;time-step&quot;: 4671}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008027921430766582, &quot;time-step&quot;: 4672}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008111890638247132, &quot;time-step&quot;: 4673}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008022417314350605, &quot;time-step&quot;: 4674}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008106192690320313, &quot;time-step&quot;: 4675}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008016913780011237, &quot;time-step&quot;: 4676}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008100966806523502, &quot;time-step&quot;: 4677}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008011921308934689, &quot;time-step&quot;: 4678}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008095767116174102, &quot;time-step&quot;: 4679}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008006551652215421, &quot;time-step&quot;: 4680}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008090410847216845, &quot;time-step&quot;: 4681}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008001375244930387, &quot;time-step&quot;: 4682}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008085161680355668, &quot;time-step&quot;: 4683}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007996167987585068, &quot;time-step&quot;: 4684}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008079935214482248, &quot;time-step&quot;: 4685}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007990947924554348, &quot;time-step&quot;: 4686}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008074746001511812, &quot;time-step&quot;: 4687}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007985737174749374, &quot;time-step&quot;: 4688}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008069322793744504, &quot;time-step&quot;: 4689}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007980427471920848, &quot;time-step&quot;: 4690}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008064116118475795, &quot;time-step&quot;: 4691}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007975196349434555, &quot;time-step&quot;: 4692}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008058623643592, &quot;time-step&quot;: 4693}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007969806902110577, &quot;time-step&quot;: 4694}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008053327910602093, &quot;time-step&quot;: 4695}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007964551914483309, &quot;time-step&quot;: 4696}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008047979790717363, &quot;time-step&quot;: 4697}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007959232898429036, &quot;time-step&quot;: 4698}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008042629924602807, &quot;time-step&quot;: 4699}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007953981985338032, &quot;time-step&quot;: 4700}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008037355146370828, &quot;time-step&quot;: 4701}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007948695565573871, &quot;time-step&quot;: 4702}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008031927281990647, &quot;time-step&quot;: 4703}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007943274686113, &quot;time-step&quot;: 4704}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008026599534787238, &quot;time-step&quot;: 4705}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007937968475744128, &quot;time-step&quot;: 4706}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008021132671274245, &quot;time-step&quot;: 4707}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007932598236948252, &quot;time-step&quot;: 4708}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000801589572802186, &quot;time-step&quot;: 4709}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007927524857223034, &quot;time-step&quot;: 4710}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008010717574506998, &quot;time-step&quot;: 4711}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007922282093204558, &quot;time-step&quot;: 4712}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008005436393432319, &quot;time-step&quot;: 4713}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007917191251181066, &quot;time-step&quot;: 4714}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0008000324014574289, &quot;time-step&quot;: 4715}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000791208993177861, &quot;time-step&quot;: 4716}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007995246560312808, &quot;time-step&quot;: 4717}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007907052058726549, &quot;time-step&quot;: 4718}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007990105077624321, &quot;time-step&quot;: 4719}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007901937933638692, &quot;time-step&quot;: 4720}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007984781404957175, &quot;time-step&quot;: 4721}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007896493189036846, &quot;time-step&quot;: 4722}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007979337824508548, &quot;time-step&quot;: 4723}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007891322602517903, &quot;time-step&quot;: 4724}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007974356412887573, &quot;time-step&quot;: 4725}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007886465173214674, &quot;time-step&quot;: 4726}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007969369180500507, &quot;time-step&quot;: 4727}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007881352212280035, &quot;time-step&quot;: 4728}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007964188698679209, &quot;time-step&quot;: 4729}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007876300951465964, &quot;time-step&quot;: 4730}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007959171780385077, &quot;time-step&quot;: 4731}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007871207199059427, &quot;time-step&quot;: 4732}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007954033790156245, &quot;time-step&quot;: 4733}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007866197265684605, &quot;time-step&quot;: 4734}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007948902202770114, &quot;time-step&quot;: 4735}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007861016201786697, &quot;time-step&quot;: 4736}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007943626260384917, &quot;time-step&quot;: 4737}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007855850271880627, &quot;time-step&quot;: 4738}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007938542403280735, &quot;time-step&quot;: 4739}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007850878755562007, &quot;time-step&quot;: 4740}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007933452725410461, &quot;time-step&quot;: 4741}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007845815271139145, &quot;time-step&quot;: 4742}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007928290287964046, &quot;time-step&quot;: 4743}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007840597536414862, &quot;time-step&quot;: 4744}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007923216326162219, &quot;time-step&quot;: 4745}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007835774449631572, &quot;time-step&quot;: 4746}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007918291375972331, &quot;time-step&quot;: 4747}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000783064984716475, &quot;time-step&quot;: 4748}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000791306491009891, &quot;time-step&quot;: 4749}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007825759239494801, &quot;time-step&quot;: 4750}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007908237748779356, &quot;time-step&quot;: 4751}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007820812752470374, &quot;time-step&quot;: 4752}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007903081132099032, &quot;time-step&quot;: 4753}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007815748103894293, &quot;time-step&quot;: 4754}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007898075273260474, &quot;time-step&quot;: 4755}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007810722454451025, &quot;time-step&quot;: 4756}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007892947178333998, &quot;time-step&quot;: 4757}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007805604836903512, &quot;time-step&quot;: 4758}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007887778920121491, &quot;time-step&quot;: 4759}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007800634484738111, &quot;time-step&quot;: 4760}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007882918580435216, &quot;time-step&quot;: 4761}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007795743294991553, &quot;time-step&quot;: 4762}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007877874304540455, &quot;time-step&quot;: 4763}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007790817180648446, &quot;time-step&quot;: 4764}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007872923160903156, &quot;time-step&quot;: 4765}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007785821217112243, &quot;time-step&quot;: 4766}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007867883541621268, &quot;time-step&quot;: 4767}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007780803134664893, &quot;time-step&quot;: 4768}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007862747879698873, &quot;time-step&quot;: 4769}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007775841513648629, &quot;time-step&quot;: 4770}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007857849122956395, &quot;time-step&quot;: 4771}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007770883967168629, &quot;time-step&quot;: 4772}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007852871785871685, &quot;time-step&quot;: 4773}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007766091148369014, &quot;time-step&quot;: 4774}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007848022505640984, &quot;time-step&quot;: 4775}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007761165034025908, &quot;time-step&quot;: 4776}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000784300034865737, &quot;time-step&quot;: 4777}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00077561050420627, &quot;time-step&quot;: 4778}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007837919401936233, &quot;time-step&quot;: 4779}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007751210359856486, &quot;time-step&quot;: 4780}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007833020063117146, &quot;time-step&quot;: 4781}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007746372139081359, &quot;time-step&quot;: 4782}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007828164962120354, &quot;time-step&quot;: 4783}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007741522858850658, &quot;time-step&quot;: 4784}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007823198684491217, &quot;time-step&quot;: 4785}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007736592669971287, &quot;time-step&quot;: 4786}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007818301673978567, &quot;time-step&quot;: 4787}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007731615914963186, &quot;time-step&quot;: 4788}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007813215488567948, &quot;time-step&quot;: 4789}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00077268440509215, &quot;time-step&quot;: 4790}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007808574009686708, &quot;time-step&quot;: 4791}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007722063455730677, &quot;time-step&quot;: 4792}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007803666521795094, &quot;time-step&quot;: 4793}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007717180415056646, &quot;time-step&quot;: 4794}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007798693259246647, &quot;time-step&quot;: 4795}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000771211227402091, &quot;time-step&quot;: 4796}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007793609402142465, &quot;time-step&quot;: 4797}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007707175682298839, &quot;time-step&quot;: 4798}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007788445800542831, &quot;time-step&quot;: 4799}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007702187867835164, &quot;time-step&quot;: 4800}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007783723413012922, &quot;time-step&quot;: 4801}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007697506225667894, &quot;time-step&quot;: 4802}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007778815925121307, &quot;time-step&quot;: 4803}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007692514918744564, &quot;time-step&quot;: 4804}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000777382985688746, &quot;time-step&quot;: 4805}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007687804754823446, &quot;time-step&quot;: 4806}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000776914821472019, &quot;time-step&quot;: 4807}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007683010771870613, &quot;time-step&quot;: 4808}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007764353649690747, &quot;time-step&quot;: 4809}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007678295369260013, &quot;time-step&quot;: 4810}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007759418222121894, &quot;time-step&quot;: 4811}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007673450745642185, &quot;time-step&quot;: 4812}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007754600374028087, &quot;time-step&quot;: 4813}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007668546168133616, &quot;time-step&quot;: 4814}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007749656797386706, &quot;time-step&quot;: 4815}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007663664873689413, &quot;time-step&quot;: 4816}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007744813337922096, &quot;time-step&quot;: 4817}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007658945396542549, &quot;time-step&quot;: 4818}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007740113651379943, &quot;time-step&quot;: 4819}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000765447854064405, &quot;time-step&quot;: 4820}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007735516992397606, &quot;time-step&quot;: 4821}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007649724138900638, &quot;time-step&quot;: 4822}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007730750367045403, &quot;time-step&quot;: 4823}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007645026198588312, &quot;time-step&quot;: 4824}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007726006442680955, &quot;time-step&quot;: 4825}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007640231633558869, &quot;time-step&quot;: 4826}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007721157744526863, &quot;time-step&quot;: 4827}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007635597721673548, &quot;time-step&quot;: 4828}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007716512191109359, &quot;time-step&quot;: 4829}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007630996988154948, &quot;time-step&quot;: 4830}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007711839862167835, &quot;time-step&quot;: 4831}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000762623967602849, &quot;time-step&quot;: 4832}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007707125041633844, &quot;time-step&quot;: 4833}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007621538243256509, &quot;time-step&quot;: 4834}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007702328730374575, &quot;time-step&quot;: 4835}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007616904331371188, &quot;time-step&quot;: 4836}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007697669207118452, &quot;time-step&quot;: 4837}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007612091139890254, &quot;time-step&quot;: 4838}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007692675571888685, &quot;time-step&quot;: 4839}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007607194129377604, &quot;time-step&quot;: 4840}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007687801844440401, &quot;time-step&quot;: 4841}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007602465921081603, &quot;time-step&quot;: 4842}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007683246512897313, &quot;time-step&quot;: 4843}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007598118390887976, &quot;time-step&quot;: 4844}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007678712718188763, &quot;time-step&quot;: 4845}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007593365153297782, &quot;time-step&quot;: 4846}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007673825602978468, &quot;time-step&quot;: 4847}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007588626467622817, &quot;time-step&quot;: 4848}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007669327314943075, &quot;time-step&quot;: 4849}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007584048435091972, &quot;time-step&quot;: 4850}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007664423901587725, &quot;time-step&quot;: 4851}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007579165976494551, &quot;time-step&quot;: 4852}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007659581606276333, &quot;time-step&quot;: 4853}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007574498886242509, &quot;time-step&quot;: 4854}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007654859800823033, &quot;time-step&quot;: 4855}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007569909212179482, &quot;time-step&quot;: 4856}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007650345796719193, &quot;time-step&quot;: 4857}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007565345149487257, &quot;time-step&quot;: 4858}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007645651348866522, &quot;time-step&quot;: 4859}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007560645462945104, &quot;time-step&quot;: 4860}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007641009869985282, &quot;time-step&quot;: 4861}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007556099444627762, &quot;time-step&quot;: 4862}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007636356167495251, &quot;time-step&quot;: 4863}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007551483577117324, &quot;time-step&quot;: 4864}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007631750777363777, &quot;time-step&quot;: 4865}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007547066197730601, &quot;time-step&quot;: 4866}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007627221057191491, &quot;time-step&quot;: 4867}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007542424718849361, &quot;time-step&quot;: 4868}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007622671546414495, &quot;time-step&quot;: 4869}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007537866476923227, &quot;time-step&quot;: 4870}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007617967785336077, &quot;time-step&quot;: 4871}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007533289026468992, &quot;time-step&quot;: 4872}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000761350616812706, &quot;time-step&quot;: 4873}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007528803544119, &quot;time-step&quot;: 4874}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007608843734487891, &quot;time-step&quot;: 4875}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007524261600337923, &quot;time-step&quot;: 4876}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007604373968206346, &quot;time-step&quot;: 4877}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007519920472986996, &quot;time-step&quot;: 4878}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007599805830977857, &quot;time-step&quot;: 4879}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007515218458138406, &quot;time-step&quot;: 4880}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007595013012178242, &quot;time-step&quot;: 4881}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000751051411498338, &quot;time-step&quot;: 4882}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00075903395190835, &quot;time-step&quot;: 4883}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007505850517190993, &quot;time-step&quot;: 4884}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007585734711028636, &quot;time-step&quot;: 4885}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007501379586756229, &quot;time-step&quot;: 4886}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000758123816922307, &quot;time-step&quot;: 4887}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007496791076846421, &quot;time-step&quot;: 4888}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007576552452519536, &quot;time-step&quot;: 4889}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007492292206734419, &quot;time-step&quot;: 4890}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007571985479444265, &quot;time-step&quot;: 4891}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007487796829082072, &quot;time-step&quot;: 4892}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007567543652839959, &quot;time-step&quot;: 4893}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007483430672436953, &quot;time-step&quot;: 4894}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007563324761576951, &quot;time-step&quot;: 4895}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007479228661395609, &quot;time-step&quot;: 4896}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007558915531262755, &quot;time-step&quot;: 4897}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007474572630599141, &quot;time-step&quot;: 4898}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007554118055850267, &quot;time-step&quot;: 4899}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007470048731192946, &quot;time-step&quot;: 4900}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007549664587713778, &quot;time-step&quot;: 4901}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000746561330743134, &quot;time-step&quot;: 4902}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007545056287199259, &quot;time-step&quot;: 4903}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007461037021130323, &quot;time-step&quot;: 4904}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007540576625615358, &quot;time-step&quot;: 4905}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007456616731360555, &quot;time-step&quot;: 4906}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007536046323366463, &quot;time-step&quot;: 4907}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007452134741470218, &quot;time-step&quot;: 4908}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007531589944846928, &quot;time-step&quot;: 4909}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007447696989402175, &quot;time-step&quot;: 4910}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007527071284130216, &quot;time-step&quot;: 4911}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000744324701372534, &quot;time-step&quot;: 4912}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007522576488554478, &quot;time-step&quot;: 4913}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007438714383170009, &quot;time-step&quot;: 4914}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007518030470237136, &quot;time-step&quot;: 4915}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007434346480295062, &quot;time-step&quot;: 4916}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000751360144931823, &quot;time-step&quot;: 4917}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007429852848872542, &quot;time-step&quot;: 4918}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007509088027291, &quot;time-step&quot;: 4919}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007425468065775931, &quot;time-step&quot;: 4920}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007504658424295485, &quot;time-step&quot;: 4921}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007420990732498467, &quot;time-step&quot;: 4922}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000750011473428458, &quot;time-step&quot;: 4923}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000741650233976543, &quot;time-step&quot;: 4924}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.00074956682510674, &quot;time-step&quot;: 4925}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000741202908102423, &quot;time-step&quot;: 4926}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007491104770451784, &quot;time-step&quot;: 4927}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007407702505588531, &quot;time-step&quot;: 4928}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007486816612072289, &quot;time-step&quot;: 4929}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007403435301966965, &quot;time-step&quot;: 4930}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007482513901777565, &quot;time-step&quot;: 4931}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007399073801934719, &quot;time-step&quot;: 4932}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007478039478883147, &quot;time-step&quot;: 4933}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007394657586701214, &quot;time-step&quot;: 4934}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007473570294678211, &quot;time-step&quot;: 4935}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007390093524008989, &quot;time-step&quot;: 4936}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007468937546946108, &quot;time-step&quot;: 4937}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007385764038190246, &quot;time-step&quot;: 4938}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007464721566066146, &quot;time-step&quot;: 4939}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007381467148661613, &quot;time-step&quot;: 4940}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007460307679139078, &quot;time-step&quot;: 4941}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000737725174985826, &quot;time-step&quot;: 4942}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007456065504811704, &quot;time-step&quot;: 4943}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007372871623374522, &quot;time-step&quot;: 4944}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007451591081917286, &quot;time-step&quot;: 4945}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007368552614934742, &quot;time-step&quot;: 4946}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007447367534041405, &quot;time-step&quot;: 4947}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007364326738752425, &quot;time-step&quot;: 4948}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007442984497174621, &quot;time-step&quot;: 4949}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007359882583841681, &quot;time-step&quot;: 4950}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007438493194058537, &quot;time-step&quot;: 4951}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007355520501732826, &quot;time-step&quot;: 4952}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007434304570779204, &quot;time-step&quot;: 4953}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007351257372647524, &quot;time-step&quot;: 4954}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007429818506352603, &quot;time-step&quot;: 4955}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007346898200921714, &quot;time-step&quot;: 4956}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007425556541420519, &quot;time-step&quot;: 4957}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007342715398408473, &quot;time-step&quot;: 4958}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000742129166610539, &quot;time-step&quot;: 4959}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007338436553254724, &quot;time-step&quot;: 4960}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007416838780045509, &quot;time-step&quot;: 4961}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007334044203162193, &quot;time-step&quot;: 4962}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.000741249998100102, &quot;time-step&quot;: 4963}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007329797372221947, &quot;time-step&quot;: 4964}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007408193196170032, &quot;time-step&quot;: 4965}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007325475453399122, &quot;time-step&quot;: 4966}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007403820636682212, &quot;time-step&quot;: 4967}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007321140728890896, &quot;time-step&quot;: 4968}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007399487076327205, &quot;time-step&quot;: 4969}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007316757109947503, &quot;time-step&quot;: 4970}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007394984713755548, &quot;time-step&quot;: 4971}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007312421803362668, &quot;time-step&quot;: 4972}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007390666869468987, &quot;time-step&quot;: 4973}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007308197673410177, &quot;time-step&quot;: 4974}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007386321085505188, &quot;time-step&quot;: 4975}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007303828606382012, &quot;time-step&quot;: 4976}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007382150506600738, &quot;time-step&quot;: 4977}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007299843127839267, &quot;time-step&quot;: 4978}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007378238369710743, &quot;time-step&quot;: 4979}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007295871619135141, &quot;time-step&quot;: 4980}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007373992702923715, &quot;time-step&quot;: 4981}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007291564252227545, &quot;time-step&quot;: 4982}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007369618397206068, &quot;time-step&quot;: 4983}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007287305779755116, &quot;time-step&quot;: 4984}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007365294732153416, &quot;time-step&quot;: 4985}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007283018785528839, &quot;time-step&quot;: 4986}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007361037423834205, &quot;time-step&quot;: 4987}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007278826087713242, &quot;time-step&quot;: 4988}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007356888381764293, &quot;time-step&quot;: 4989}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007274654344655573, &quot;time-step&quot;: 4990}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007352633401751518, &quot;time-step&quot;: 4991}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007270402275025845, &quot;time-step&quot;: 4992}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007348343497142196, &quot;time-step&quot;: 4993}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007266225875355303, &quot;time-step&quot;: 4994}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007344129262492061, &quot;time-step&quot;: 4995}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007262114086188376, &quot;time-step&quot;: 4996}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007339917938224971, &quot;time-step&quot;: 4997}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007257911493070424, &quot;time-step&quot;: 4998}, {&quot;accuracy&quot;: 1.0, &quot;loss&quot;: 0.0007335797417908907, &quot;time-step&quot;: 4999}]}}, {&quot;mode&quot;: &quot;vega-lite&quot;});
&lt;/script&gt;

&lt;p&gt;We see that accuracy goes to 100% in around 1,000 epochs (note that different runs may slightly change the results).&lt;/p&gt;

&lt;h3 id=&quot;lstm&quot;&gt;LSTM&lt;/h3&gt;

&lt;p&gt;In a strict sense, LSTM is a type of layer instead of a type of network. What I’ve calling LSTM networks is basically any RNN composed of LSTM layers. Most RNNs you’ll find in the wild (i.e., the internet) use either LSTMs or &lt;a href=&quot;https://en.wikipedia.org/wiki/Gated_recurrent_unit&quot;&gt;Gated Recurrent Units (GRU)&lt;/a&gt;. We don’t cover GRU here since they are very similar to LSTMs and this blogpost is dense enough as it is. If you want to learn more about GRU see &lt;a href=&quot;https://arxiv.org/abs/1406.1078&quot;&gt;Cho et al (2014)&lt;/a&gt; and &lt;a href=&quot;https://d2l.ai/chapter_recurrent-modern/gru.html&quot;&gt;Chapter 9.1 from Zhang (2020)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For this section, I’ll base the code in the example provided by Chollet (2017) in chapter 6. As a side note, if you are interested in learning Keras in-depth, &lt;a href=&quot;https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438&quot;&gt;Chollet’s book&lt;/a&gt; is probably the best source since he is the creator of Keras library.&lt;/p&gt;

&lt;h3 id=&quot;reading-and-preprocessing-data-from-keras&quot;&gt;Reading and preprocessing data from Keras&lt;/h3&gt;

&lt;p&gt;If you are like me,  you like to check the &lt;a href=&quot;https://www.imdb.com/&quot;&gt;IMDB&lt;/a&gt; reviews before watching a movie. For this example, we will make use of the &lt;a href=&quot;https://www.imdb.com/interfaces/&quot;&gt;IMDB dataset&lt;/a&gt;, and Lucky us, Keras comes pre-packaged with it. The IMDB dataset comprises 50,000 movie reviews, 50% positive and 50% negative. Keras give access to a numerically encoded version of the dataset where each word is mapped to sequences of integers. We can download the dataset by running the following:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Libraries for this section
&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensorflow.keras.layers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LSTM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Embedding&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensorflow.keras.models&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keras.datasets&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;imdb&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keras.preprocessing.sequence&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad_sequences&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;altair&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: This time I also imported Tensorflow, and from there Keras layers and models. Keras happens to be integrated with Tensorflow, as a high-level interface, so nothing important changes when doing this. Yet, there are some implementation issues with the optimizer that require importing from Tensorflow to work.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;maxlen&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5000&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;imdb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;load_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;num_words&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maxlen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The parameter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_words=5000&lt;/code&gt; restrict the dataset to the top 5,000 most frequent words. We do this to avoid highly infrequent words. Often, infrequent words are either typos or words for which we don’t have enough statistical information to learn useful representations.&lt;/p&gt;

&lt;p&gt;Data is downloaded as a (25000,) tuples of integers.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;train-data shape: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;, train-labels shape: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;test-data shape: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;, test-labels shape: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;train-data shape: (25000,), train-labels shape: (25000,)
test-data shape: (25000,), test-labels shape: (25000,) 

[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32] 1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you are curious about the review contents, the code snippet below decodes the first review into words.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;word_index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;imdb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;get_word_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;reverse_word_index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;for &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;decoded_review&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reverse_word_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;?&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;decoded_review&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&quot;? this film was just brilliant casting location scenery story direction everyone&apos;s really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly ? was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little ? that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big ? for the whole film but these children are amazing and should be ? for what they have done don&apos;t you think the whole story was so lovely because it was true and was someone&apos;s life after all that was ? with us all&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, we need to &lt;strong&gt;“pad” each sequence with zeros&lt;/strong&gt; such that all sequences are of the same length. We do this because Keras layers expect same-length vectors as input sequences. Given that we are considering only the 5,000 more frequent words, we have max length of any sequence is 5,000. Hence, we have to pad every sequence to have length 5,000.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;pad_sequences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;maxlen&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maxlen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;pad_sequences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;maxlen&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maxlen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As a result, we go from a list of list (samples= 25000,), to a matrix of shape (samples=25000, maxleng=5000)&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;train-data shape: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;, train-labels shape: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;train-data shape: (25000, 5000), train-labels shape: (25000, 5000) 

[  0   0   0 ...  19 178  32] [  0   0   0 ...  14   6 717]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally, we will take only the first 5,000 training and testing examples. We do this because training RNNs is computationally expensive, and we don’t have access to enough hardware resources to train a large model here.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;training_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5000&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;training_sentences&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;testing_sentences&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;training_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;testing_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;train-data shape: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_sentences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;, train-labels shape: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;test-data shape: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;testing_sentences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;, test-labels shape: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;testing_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;train-data shape: (5000, 5000), train-labels shape: (5000,)
test-data shape: (5000, 5000), test-labels shape: (5000,)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Let’s compute the percentage of positive reviews samples on training and testing as a sanity check. We want this to be close to 50% so the sample is balanced.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;percentage of positive reviews in training: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_size&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;percentage of positive reviews in testing: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;testing_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_size&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;percentage of positive reviews in training: 0.5092
percentage of positive reviews in testing: 0.4858
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;word-embeddings-with-keras&quot;&gt;Word embeddings with Keras&lt;/h3&gt;

&lt;p&gt;We will use word embeddings instead of one-hot encodings this time. Again, Keras provides convenience functions (or layer) to learn word embeddings along with RNNs training. An embedding in Keras is a layer that takes two inputs as a minimum: the &lt;strong&gt;max length of a sequence&lt;/strong&gt; (i.e., the max number of tokens), and the &lt;strong&gt;desired dimensionality of the embedding&lt;/strong&gt; (i.e., in how many vectors you want to represent the tokens). For instance, for an embedding with 5,000 tokens and 32 embedding vectors we just define &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model.add(Embedding(5,000, 32))&lt;/code&gt;. We will do this when defining the network architecture.&lt;/p&gt;

&lt;h3 id=&quot;lstm-architecture-in-keras&quot;&gt;LSTM architecture in Keras&lt;/h3&gt;

&lt;p&gt;Defining RNN with LSTM layers is remarkably simple with Keras (considering how complex LSTMs are as mathematical objects). I’ll define a relatively “shallow” network with just 1 hidden LSTM layer.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Define a network as a linear stack of layers
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Add embedding layer with:
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - Max number of tokens: 10,000
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - Number of embeddings vectors: 32 
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maxlen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Add LSTM layer with 32 units (sequence length)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;LSTM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Add output layer with sigmoid activation unit
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;lstm-application-imdb-review-prediction&quot;&gt;LSTM Application: IMDB review prediction&lt;/h2&gt;

&lt;p&gt;It’s time to train and test our RNN. I’ll run just five epochs, again, because we don’t have enough computational resources and for a demo is more than enough. If you run this, it may take around 5-15 minutes in a CPU.  For instance, my Intel i7-8550U took ~10 min to run five epochs.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;RMSprop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;losses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;BinaryCrossentropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;acc&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;history&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_sentences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;training_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# update gradients every 128 sequences
&lt;/span&gt;                    &lt;span class=&quot;n&quot;&gt;validation_split&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# validation subsample
&lt;/span&gt;                    &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: a “validation split” is different from the testing set: It’s a sub-sample from the training set. For instance, with a training sample of 5,000, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;validation_split = 0.2&lt;/code&gt; will split the data in a 4,000 effective training set and a 1,000 validation set. The network is trained only in the training set, whereas the validation set is used as a real-time(ish) way to help with hyper-parameter tunning, by synchronously evaluating the network in such a sub-sample. To learn more about this see the &lt;a href=&quot;https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets&quot;&gt;Wikipedia article on the topic&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;final training accuracy:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;history&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;history&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;acc&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;final validation accuracy:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;history&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;history&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;val_acc&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;final training accuracy:0.8777499794960022
final validation accuracy:0.8149999976158142
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We obtained a &lt;strong&gt;training accuracy of ~88%&lt;/strong&gt; and &lt;strong&gt;validation accuracy of ~81%&lt;/strong&gt; (note that different runs may slightly change the results). The left-pane in &lt;strong&gt;Chart 3&lt;/strong&gt; shows the training and validation curves for accuracy, whereas the right-pane shows the same for the loss. It is clear that the network overfitting the data by the 3rd epoch. This is expected as our architecture is shallow, the training set relatively small, and no regularization method was used.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;history&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;history&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;val_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;history&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;history&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;val_loss&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;accuracy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;history&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;history&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;acc&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;val_acc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;history&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;history&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;val_acc&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;accuracy&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                   &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;val_accuracy&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val_acc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                   &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                   &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;val_loss&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                   &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;time-step&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)})&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;accu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mark_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;#0202d6&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;time-step&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;accuracy&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;val_accu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mark_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;#7272a1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;time-step&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;val_accuracy&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mark_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;#d60202&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;time-step&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;val_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mark_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;#cc6e6e&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;time-step&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;val_loss&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accu&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val_accu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;properties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Chart 3&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div id=&quot;altair-viz-07863c1c6e494449b9c2b8b78a81926c&quot;&gt;&lt;/div&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
  (function(spec, embedOpt){
    const outputDiv = document.getElementById(&quot;altair-viz-07863c1c6e494449b9c2b8b78a81926c&quot;);
    const paths = {
      &quot;vega&quot;: &quot;https://cdn.jsdelivr.net/npm//vega@5?noext&quot;,
      &quot;vega-lib&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-lib?noext&quot;,
      &quot;vega-lite&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-lite@4.0.2?noext&quot;,
      &quot;vega-embed&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-embed@6?noext&quot;,
    };

    function loadScript(lib) {
      return new Promise(function(resolve, reject) {
        var s = document.createElement(&apos;script&apos;);
        s.src = paths[lib];
        s.async = true;
        s.onload = () =&gt; resolve(paths[lib]);
        s.onerror = () =&gt; reject(`Error loading script: ${paths[lib]}`);
        document.getElementsByTagName(&quot;head&quot;)[0].appendChild(s);
      });
    }

    function showError(err) {
      outputDiv.innerHTML = `&lt;div class=&quot;error&quot; style=&quot;color:red;&quot;&gt;${err}&lt;/div&gt;`;
      throw err;
    }

    function displayChart(vegaEmbed) {
      vegaEmbed(outputDiv, spec, embedOpt)
        .catch(err =&gt; showError(`Javascript Error: ${err.message}&lt;br&gt;This usually means there&apos;s a typo in your chart specification. See the javascript console for the full traceback.`));
    }

    if(typeof define === &quot;function&quot; &amp;&amp; define.amd) {
      requirejs.config({paths});
      require([&quot;vega-embed&quot;], displayChart, err =&gt; showError(`Error loading script: ${err.message}`));
    } else if (typeof vegaEmbed === &quot;function&quot;) {
      displayChart(vegaEmbed);
    } else {
      loadScript(&quot;vega&quot;)
        .then(() =&gt; loadScript(&quot;vega-lite&quot;))
        .then(() =&gt; loadScript(&quot;vega-embed&quot;))
        .catch(showError)
        .then(() =&gt; displayChart(vegaEmbed));
    }
  })({&quot;config&quot;: {&quot;view&quot;: {&quot;continuousWidth&quot;: 400, &quot;continuousHeight&quot;: 300}}, &quot;hconcat&quot;: [{&quot;layer&quot;: [{&quot;mark&quot;: {&quot;type&quot;: &quot;line&quot;, &quot;color&quot;: &quot;#0202d6&quot;}, &quot;encoding&quot;: {&quot;x&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;time-step&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;accuracy&quot;}}}, {&quot;mark&quot;: {&quot;type&quot;: &quot;line&quot;, &quot;color&quot;: &quot;#7272a1&quot;}, &quot;encoding&quot;: {&quot;x&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;time-step&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;val_accuracy&quot;}}}]}, {&quot;layer&quot;: [{&quot;mark&quot;: {&quot;type&quot;: &quot;line&quot;, &quot;color&quot;: &quot;#d60202&quot;}, &quot;encoding&quot;: {&quot;x&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;time-step&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;loss&quot;}}}, {&quot;mark&quot;: {&quot;type&quot;: &quot;line&quot;, &quot;color&quot;: &quot;#cc6e6e&quot;}, &quot;encoding&quot;: {&quot;x&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;time-step&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;val_loss&quot;}}}]}], &quot;data&quot;: {&quot;name&quot;: &quot;data-9587a6adfd99e4fe616912ad0f7d25cb&quot;}, &quot;title&quot;: &quot;Chart 3&quot;, &quot;$schema&quot;: &quot;https://vega.github.io/schema/vega-lite/v4.0.2.json&quot;, &quot;datasets&quot;: {&quot;data-9587a6adfd99e4fe616912ad0f7d25cb&quot;: [{&quot;accuracy&quot;: 0.5809999704360962, &quot;val_accuracy&quot;: 0.7310000061988831, &quot;loss&quot;: 0.687211995601654, &quot;val_loss&quot;: 0.6642515668869019, &quot;time-step&quot;: 1}, {&quot;accuracy&quot;: 0.7577499747276306, &quot;val_accuracy&quot;: 0.777999997138977, &quot;loss&quot;: 0.6248226439952851, &quot;val_loss&quot;: 0.590116578578949, &quot;time-step&quot;: 2}, {&quot;accuracy&quot;: 0.8230000138282776, &quot;val_accuracy&quot;: 0.6859999895095825, &quot;loss&quot;: 0.5040617761611939, &quot;val_loss&quot;: 0.5789643344879151, &quot;time-step&quot;: 3}, {&quot;accuracy&quot;: 0.846750020980835, &quot;val_accuracy&quot;: 0.8209999799728394, &quot;loss&quot;: 0.4134952628612518, &quot;val_loss&quot;: 0.4255082859992981, &quot;time-step&quot;: 4}, {&quot;accuracy&quot;: 0.8777499794960022, &quot;val_accuracy&quot;: 0.8149999976158142, &quot;loss&quot;: 0.3233668565750122, &quot;val_loss&quot;: 0.4243640975952148, &quot;time-step&quot;: 5}]}}, {&quot;mode&quot;: &quot;vega-lite&quot;});
&lt;/script&gt;

&lt;p&gt;Finally, the model obtains a &lt;strong&gt;test set accuracy of ~80%&lt;/strong&gt; echoing the results from the validation set. All things considered, this is a very respectable result!&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;evaluate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;testing_sentences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;testing_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Test loss score: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Test accuracy score:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Test loss score: 0.4296374634742737
Test accuracy score:0.8098000288009644
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;limitations&quot;&gt;Limitations&lt;/h2&gt;

&lt;h3 id=&quot;training-rnns-is-hard-and-costly&quot;&gt;Training RNNs is hard and costly&lt;/h3&gt;

&lt;p&gt;As I mentioned in previous sections, there are three well-known issues that make training RNNs really hard: (1) vanishing gradients, (2) exploding gradients, (3) and its sequential nature, which make them computationally expensive as parallelization is difficult. I won’t discuss again these issues. Many techniques have been developed to address all these issues, from architectures like LSTM,  GRU, and ResNets, to techniques like gradient clipping and regularization (Pascanu et al (2012); for an up to date (i.e., 2020) review of this issues see &lt;a href=&quot;https://d2l.ai/chapter_recurrent-modern/index.html&quot;&gt;Chapter 9 of Zhang et al book&lt;/a&gt;.).&lt;/p&gt;

&lt;p&gt;The quest for solutions to RNNs deficiencies has prompt the development of new architectures like Encoder-Decoder networks with “attention” mechanisms (Bahdanau et al, 2014; Vaswani et al, 2017). This new type of architecture seems to be outperforming RNNs in tasks like machine translation and text generation, in addition to overcoming some RNN deficiencies.&lt;/p&gt;

&lt;h3 id=&quot;do-rnns-really-understandanything&quot;&gt;Do RNNs “really” understand…anything?&lt;/h3&gt;

&lt;p&gt;Critics like Gary Marcus have pointed out the apparent inability of neural-networks based models to “really” understand their outputs (Marcus, 2018). This is prominent for RNNs since they have been used profusely used in the context of language generation and understanding. For instance, even state-of-the-art models like &lt;a href=&quot;https://openai.com/blog/better-language-models/&quot;&gt;OpenAI GPT-2&lt;/a&gt; sometimes produce incoherent sentences. Marcus gives the &lt;a href=&quot;https://thegradient.pub/gpt2-and-the-nature-of-intelligence/&quot;&gt;following example&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;(&lt;strong&gt;Marcus&lt;/strong&gt;) Suppose for example that I ask the system what happens when I put two trophies a table and another: &lt;em&gt;I put two trophies on a table, and then add another, the total number is…&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(&lt;strong&gt;GPT-2 answer&lt;/strong&gt;) &lt;em&gt;…is five trophies and I’m like, ‘Well, I can live with that, right?&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;From Marcus’ perspective, this lack of coherence is an exemplar of GPT-2 incapacity to understand language.&lt;/p&gt;

&lt;p&gt;Yet, I’ll argue two things. First, this is an unfairly underspecified question: &lt;strong&gt;What do we mean by understanding?&lt;/strong&gt; From a cognitive science perspective, this is a fundamental yet strikingly hard question to answer. If you ask five cognitive science what does it “really” mean to understand something you are likely to get five different answers. What do we need is a falsifiable way to decide when a system “really” understands language. Is lack of coherence enough? I produce incoherent phrases all the time, and I know lots of people that do the same. In any case, it is important to question whether human-level understanding of language (however you want to define it) is necessary to show that a computational model of any cognitive process is a good model or not. We have several great models of many natural phenomena, yet not a single one gets all the aspects of the phenomena perfectly. For instance, Marcus has said that the fact that GPT-2 sometimes produces incoherent sentences is somehow a proof that human “thoughts” (i.e., internal representations) can’t possibly be represented as vectors (like neural nets do), which I believe is non-sequitur.&lt;/p&gt;

&lt;p&gt;Second, &lt;strong&gt;Why should we expect that a network trained for a narrow task like language production should understand what language “really” is?&lt;/strong&gt; The exercise of comparing computational models of “cognitive processes” with “full-blown” human cognition, makes as much sense as comparing a model of bipedal locomotion with the entire motor control system of an animal. A model of bipedal locomotion is just that: &lt;strong&gt;a model of a sub-system or sub-process within a larger system, not a reproduction of the entire system&lt;/strong&gt;. The fact that a model of bipedal locomotion does not capture well the mechanics of “jumping”, does not undermine it’s veracity or utility, in the same manner, that the inability of a model of language production to “understand” all aspects of language does not undermine its plausibility as a model of…languague production.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;Recurrent neural networks have been prolific models in cognitive science (Munakata et al, 1997; St. John, 1992; Plaut et al., 1996; Christiansen &amp;amp; Chater, 1999; Botvinick &amp;amp; Plaut, 2004; Muñoz-Organero et al., 2019), bringing together intuitions about how cognitive systems work in time-dependent domains, and how neural networks may accommodate such processes.&lt;/p&gt;

&lt;p&gt;Nevertheless, problems like vanishing gradients, exploding gradients, and computational inefficiency (i.e., lack of parallelization) have difficulted RNN use in many domains. Although new architectures (without recursive structures) have been developed to improve RNN results and overcome its limitations, they remain relevant from a cognitive science perspective.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Bahdanau, D., Cho, K., &amp;amp; Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. ArXiv Preprint ArXiv:1409.0473.&lt;/li&gt;
  &lt;li&gt;Bengio, Y., Simard, P., &amp;amp; Frasconi, P. (1994). Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks, 5(2), 157–166.&lt;/li&gt;
  &lt;li&gt;Botvinick, M., &amp;amp; Plaut, D. C. (2004). Doing without schema hierarchies: A recurrent connectionist approach to normal and impaired routine sequential action. Psychological Review, 111(2), 395.&lt;/li&gt;
  &lt;li&gt;Barak, O. (2017). Recurrent neural networks as versatile tools of neuroscience research. Current Opinion in Neurobiology, 46, 1–6. https://doi.org/10.1016/j.conb.2017.06.003&lt;/li&gt;
  &lt;li&gt;Chen, G. (2016). A gentle tutorial of recurrent neural network with error backpropagation. arXiv preprint arXiv:1610.02583.&lt;/li&gt;
  &lt;li&gt;Elman, J. L. (1990). Finding Structure in Time. Cognitive Science, 14(2), 179–211. https://doi.org/10.1207/s15516709cog1402_1&lt;/li&gt;
  &lt;li&gt;François, C. (2017). 6. Deep Learning for text and sequences. Deep learning with Python. Manning.&lt;/li&gt;
  &lt;li&gt;Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., &amp;amp; Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078.&lt;/li&gt;
  &lt;li&gt;Christiansen, M. H., &amp;amp; Chater, N. (1999). Toward a connectionist model of recursion in human linguistic performance. Cognitive Science, 23(2), 157–205.&lt;/li&gt;
  &lt;li&gt;Goodfellow, I., Bengio, Y., &amp;amp; Courville, A. (2016). 10. Sequence Modeling: Recurrent and Recursive Nets. In Deep Learning. MIT Press. https://www.deeplearningbook.org/contents/mlp.html&lt;/li&gt;
  &lt;li&gt;Hebb, D. O. (1949). The organization of behavior: A neuropsychological theory. Psychology Press.&lt;/li&gt;
  &lt;li&gt;Hochreiter, S., &amp;amp; Schmidhuber, J. (1997). Long short-term memory. Neural Computation, 9(8), 1735–1780.&lt;/li&gt;
  &lt;li&gt;Güçlü, U., &amp;amp; van Gerven, M. A. (2017). Modeling the dynamics of human brain activity with recurrent neural networks. Frontiers in Computational Neuroscience, 11, 7.&lt;/li&gt;
  &lt;li&gt;Graves, A. (2012). Supervised sequence labelling. In Supervised sequence labelling with recurrent neural networks (pp. 5-13). Springer, Berlin, Heidelberg.&lt;/li&gt;
  &lt;li&gt;Jarne, C., &amp;amp; Laje, R. (2019). A detailed study of recurrent neural networks used to model tasks in the cerebral cortex. ArXiv Preprint ArXiv:1906.01094.&lt;/li&gt;
  &lt;li&gt;John, M. F. (1992). The story gestalt: A model of knowledge-intensive processes in text comprehension. Cognitive Science, 16(2), 271–306.&lt;/li&gt;
  &lt;li&gt;Marcus, G. (2018). Deep learning: A critical appraisal. ArXiv Preprint ArXiv:1801.00631.&lt;/li&gt;
  &lt;li&gt;Munakata, Y., McClelland, J. L., Johnson, M. H., &amp;amp; Siegler, R. S. (1997). Rethinking infant knowledge: Toward an adaptive process account of successes and failures in object permanence tasks. Psychological Review, 104(4), 686.&lt;/li&gt;
  &lt;li&gt;Muñoz-Organero, M., Powell, L., Heller, B., Harpin, V., &amp;amp; Parker, J. (2019). Using Recurrent Neural Networks to Compare Movement Patterns in ADHD and Normally Developing Children Based on Acceleration Signals from the Wrist and Ankle. Sensors (Basel, Switzerland), 19(13). https://doi.org/10.3390/s19132935&lt;/li&gt;
  &lt;li&gt;K. J. Lang, A. H. Waibel, and G. E. Hinton. A Time-delay Neural Network Architecture for Isolated Word Recognition. Neural Networks, 3(1):23-43, 1990&lt;/li&gt;
  &lt;li&gt;Pascanu, R., Mikolov, T., &amp;amp; Bengio, Y. (2013). On the difficulty of training recurrent neural networks. International Conference on Machine Learning, 1310–1318.&lt;/li&gt;
  &lt;li&gt;Philipp, G., Song, D., &amp;amp; Carbonell, J. G. (2017). The exploding gradient problem demystified-definition, prevalence, impact, origin, tradeoffs, and solutions. ArXiv Preprint ArXiv:1712.05577.&lt;/li&gt;
  &lt;li&gt;Plaut, D. C., McClelland, J. L., Seidenberg, M. S., &amp;amp; Patterson, K. (1996). Understanding normal and impaired word reading: Computational principles in quasi-regular domains. Psychological Review, 103(1), 56.&lt;/li&gt;
  &lt;li&gt;Raj, B. (2020, Spring). Neural Networks: Hopfield Nets and Auto Associators [Lecture]. http://deeplearning.cs.cmu.edu/document/slides/lec17.hopfield.pdf&lt;/li&gt;
  &lt;li&gt;Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \Lukasz, &amp;amp; Polosukhin, I. (2017). Attention is all you need. Advances in Neural Information Processing Systems, 5998–6008.&lt;/li&gt;
  &lt;li&gt;Zhang, A., Lipton, Z. C., Li, M., &amp;amp; Smola, A. J. (2020). 8. Recurrent Neural Networks. In Dive into Deep Learning. https://d2l.ai/chapter_convolutional-neural-networks/index.html&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;useful-online-resources&quot;&gt;Useful online resources&lt;/h2&gt;

&lt;p&gt;Here a list of my favorite online resources to learn more about Recurrent Neural Networks:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Stanford Lectures: Natural Language Processing with Deep Learning, Winter 2020. &lt;a href=&quot;http://web.stanford.edu/class/cs224n/index.html#schedule&quot;&gt;Coruse webpage&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Bhiksha Raj’s Deep Learning Lectures 13, 14, and 15 at CMU. &lt;a href=&quot;http://deeplearning.cs.cmu.edu/&quot;&gt;Course webpage&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Geoffrey Hinton’s Neural Network Lectures 7 and 8. &lt;a href=&quot;https://www.youtube.com/watch?v=2fRnHVVLf1Y&amp;amp;list=PLiPvV5TNogxKKwvKb1RKwkq2hm7ZvpHz0&quot;&gt;YouTube Videos&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 16 Apr 2020 00:00:00 +0800</pubDate>
        <link>//the-recurrent-net</link>
        <link href="/the-recurrent-net"/>
        <guid isPermaLink="true">/the-recurrent-net</guid>
      </item>
    
      <item>
        <title>The Convolutional Neural Network - Theory and Implementation of LeNet-5 and AlexNet</title>
        <description>&lt;iframe src=&quot;https://github.com/sponsors/pabloinsente/card&quot; title=&quot;Sponsor pabloinsente&quot; height=&quot;225&quot; width=&quot;600&quot; style=&quot;border: 0;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;learning-objectives&quot;&gt;Learning objectives&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Understand the principles behind the creation of the convolutional network&lt;/li&gt;
  &lt;li&gt;Gain an intuitive understanding of the convolution (feature map) and pooling (subsampling) operations&lt;/li&gt;
  &lt;li&gt;Develop a basic code implementation of the LeNet-5 and AlexNet networks in Python&lt;/li&gt;
  &lt;li&gt;Identify the similarities and differences between human vision and convolutional networks&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;historical-and-theoretical-background&quot;&gt;Historical and theoretical background&lt;/h2&gt;

&lt;h3 id=&quot;hubel-and-wiesel&quot;&gt;Hubel and Wiesel&lt;/h3&gt;

&lt;p&gt;Rosenblatt’s photo-perceptron (1958) was the first neural network model attempting to emulate human visual and perceptual capacities. Unfortunately, little was known at the time about the mammalian visual cortex that could inform Rosenblatt’s work. Consequently, the photo-perceptron architecture was inspired by a very coarse idea of how the information flows from the retina to be processed by the brain. This changed fast in the years following the introduction of the perceptron.&lt;/p&gt;

&lt;p&gt;In 1962, &lt;a href=&quot;https://en.wikipedia.org/wiki/David_H._Hubel&quot;&gt;David H. Hubel&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Torsten_Wiesel&quot;&gt;Torsten Wiesel&lt;/a&gt; published one the major breakthroughs in the neurophysiology of the visual cortex: &lt;strong&gt;the existence of orientation selectivity and columnar organization&lt;/strong&gt;. This is what they did: they placed tiny microelectrode in a single neuron in the primary visual cortex (V1) of an anesthetized cat and projected light and dark dots into the cat’s eye. It did not work at all, they could not get a response from the neuron. But, they had a lucky accident. Since they were using a slide projector to show the dots, the &lt;em&gt;margin of the slide&lt;/em&gt; with the dot also was projected into the cat’s eyes and bam! the neuron fired. From there, they experimented with light and dark bars in different orientations, which led them to propose the existence of &lt;strong&gt;two types of cells in the visual cortex&lt;/strong&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;simple cells&lt;/strong&gt;, that fire at a higher (or lower) rate depending on the bar orientation. Sometimes called “line detectors”.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;complex cells&lt;/strong&gt; that fire in response to a wider variety of orientations, yet, they still show a preference (higher firing rate) to certain orientations. Sometimes are called “motion detectors”. Importantly, these cells receive input from several &lt;em&gt;simple cells&lt;/em&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Importantly, these cells are &lt;strong&gt;hierarchically organized&lt;/strong&gt;. Keeps this in mind as it’ll become important later. Altogether, these discoveries were the basis of the work that granted them the Nobel Prize in Physiology in 1981. &lt;a href=&quot;http://www.youtube.com/watch?v=jw6nBWo21Zk&quot;&gt;Here&lt;/a&gt; is a short video from their experiments.&lt;/p&gt;

&lt;h3 id=&quot;fukushimas-neocognitron&quot;&gt;Fukushima’s Neocognitron&lt;/h3&gt;

&lt;p&gt;The work of Hubel and Wiesel served as the basis for the precursor of modern convolutional neural networks: &lt;strong&gt;Fukushima’s Neocognitron&lt;/strong&gt; (1980). &lt;a href=&quot;https://en.wikipedia.org/wiki/Kunihiko_Fukushima&quot;&gt;Kunihiko Fukushima&lt;/a&gt;, a Japanese computer scientist, developed the Neocognitron idea while working at the &lt;a href=&quot;https://en.wikipedia.org/wiki/NHK_Science_%26_Technology_Research_Laboratories&quot;&gt;NHK Science &amp;amp; Technology Research Laboratories&lt;/a&gt;. He did this by implementing the simple-cells and complex-cells discovered by Hubel and Wiesel in a multilayer neural network architecture. &lt;strong&gt;Figure 1&lt;/strong&gt; shows a simplified diagram of the Neocognitron with 3 layers (4 if you count the inputs).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 1: Simplified Neocognitrone&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-8/neocognitron.png&quot; width=&quot;70%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The general idea behind the Neocognitron is the following: the &lt;strong&gt;input layer $L_0$ works as the retina&lt;/strong&gt;, reading the raw input pattern. Then, each cell in a $S_1$ patch “reads” a sub-section of the input image based on a “preference” for a certain type of pattern. Any given layer $L_n$ will have several of these $S_j$ patches as a collection of &lt;strong&gt;feature “filters”&lt;/strong&gt;. Some may detect a diagonal line, while other a small triangle, or a corner, or something else. Each $S_j$ patch connects to a $C_k$ cell, and such a cell fires if it gets any positive input from its corresponding patch. This process is also known as &lt;strong&gt;“pooling”&lt;/strong&gt;. This cycle of “feature” detection and “pooling” is repeated as many times as intermediate layers in the network. The last layer corresponds to the output, where some neurons will fire depending on the input pattern. Mathematically, “feature detection” is accomplished by multiplying the input by a fixed matrix of weights, whereas “pooling” corresponding to taking an average of the connected patch of S-cells.&lt;/p&gt;

&lt;p&gt;You may have noticed that the behavior of the S-cells and C-cells replicate (to some extent) what Hubel and Wiesel found in their experiments. The great thing about this architecture is that it is &lt;strong&gt;robust to shifts in the input image&lt;/strong&gt;: you can move the image around and the combination of “feature detection” and “pooling” will detect the presence of each part of the image regardless of its position. &lt;strong&gt;Figure 2&lt;/strong&gt; exemplifies this characteristic.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-8/neocognitron-cells.png&quot; width=&quot;60%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The Neocognitron is also &lt;strong&gt;robust to deformation&lt;/strong&gt;: it will detect the object even if it’s enlarged, reduced in size, or blurred, by virtue of the same mechanism that allows robustness to positional shifting. It is also important to notice that the pooling operation will “blur” the input image, and the fact that C-cells take the average of its corresponding S-cells makes the pooling more robust to random noise added to the image. &lt;a href=&quot;http://www.youtube.com/watch?v=Qil4kmvm2Sw&quot;&gt;Here&lt;/a&gt; you can find a short video (from the 80s!) explaining the basics of the Neocognitron.&lt;/p&gt;

&lt;p&gt;In sum, the Neocognitron established the following principles:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;S-cells extract simple features&lt;/li&gt;
  &lt;li&gt;C-cells combine and subsample S-cells extracted features&lt;/li&gt;
  &lt;li&gt;Image features are learned and combined to produce more complex representations&lt;/li&gt;
  &lt;li&gt;The image recognition process is hierarchically organized&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you are familiar with convolutional neural networks, you may be wondering what is the difference between the Neocognitron and later models like Yann LeCun’s LeNet (1989), since they look remarkably similar. The main (but not only) difference is the training algorithm: &lt;strong&gt;the Neocognitron does not use backpropagation&lt;/strong&gt;. At the time, backpropagation was not widely known as a training method for multilayer neural networks, reason why Fukushima never use it. Instead, he trained his model by using an unsupervised learning approach. Regardless, the Neocognitron laid the groundwork of modern neural network models of vision and computer vision more generally.&lt;/p&gt;

&lt;h3 id=&quot;lecuns-lenet&quot;&gt;LeCun’s LeNet&lt;/h3&gt;

&lt;p&gt;The architecture today known as the convolutional neural network was introduced by &lt;a href=&quot;http://yann.lecun.com/&quot;&gt;Yann LeCun&lt;/a&gt; in 1989. Although LeCun was trained as an Electrical Engineer, he got interested in the idea of building intelligent machines from early on in his undergraduate education by reading a book about the &lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/0010027794900345&quot;&gt;Piaget vs Chomsky debate on language acquisition&lt;/a&gt;. In that book, several researchers argued in favor of or against each author’s view. Among those contributors was &lt;a href=&quot;https://en.wikipedia.org/wiki/Seymour_Papert&quot;&gt;Seymour Papert&lt;/a&gt; who mentioned Rosenblatt’s perceptron in his article, which inspired LeCun to learn about neural networks for the first time. Ironically, this was the same Seymour Papert that published &lt;a href=&quot;https://en.wikipedia.org/wiki/Perceptrons_(book)&quot;&gt;the book&lt;/a&gt; (along with Marvin Minsky) that brought the demise on the interest on neural networks in the late ’60s. I don’t believe in karma, but this certainly looks like it.&lt;/p&gt;

&lt;p&gt;Eventually, LeCun became a postdoc at the University of Toronto with Geoffrey Hinton and started to prototype the first convolutional network. By the late ’80s, LeCun was working at &lt;a href=&quot;https://en.wikipedia.org/wiki/Bell_Labs&quot;&gt;Bell Labs&lt;/a&gt; in New Jersey, the place where he and his colleagues developed at published the &lt;strong&gt;first convolutional neural network trained with backpropagation&lt;/strong&gt;, the &lt;strong&gt;“LeNet”&lt;/strong&gt;, that could effectively recognize handwritten zip codes from US post office. This early convolutional network went through several rounds of modifications and improvements (LeNet-1, LeNet-2, etc.) until in 1998 the &lt;a href=&quot;http://yann.lecun.com/exdb/lenet/&quot;&gt;LeNet-5&lt;/a&gt; reached test error rates of 0.95% (99.05 of classification accuracy) in the &lt;a href=&quot;http://yann.lecun.com/exdb/mnist/&quot;&gt;MNIST dataset of handwritten digits&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;the-convolution-opertion-feature-detection&quot;&gt;The convolution opertion: feature detection&lt;/h3&gt;

&lt;p&gt;I’ll begin by schematically describing the LeNet-5 model and leave the mathematics for the next section. This conceptual explanation should be enough to have a higher-level understanding of the model but not necessarily to implement a convolutional network.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 3: LeNet-5 Architecture&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-8/LeNet.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The general architecture of the LeNet-5 is shown in &lt;strong&gt;Figure 3&lt;/strong&gt;. The input layer $L-0$ acts like the retina receiving images of characters that are centered and size-normalized (otherwise, some images may not fit in the input layer). The next layer $L-1$ is composed of several &lt;strong&gt;features maps&lt;/strong&gt;, which have the same role that the Neocognitron simple-cells: to extract simple features as oriented edges, corners, end-points, etc. In practice, a feature map is a squared matrix of &lt;strong&gt;identical weights&lt;/strong&gt;.  Weights &lt;em&gt;within&lt;/em&gt; a feature map need to be identical so they can detect &lt;em&gt;the same&lt;/em&gt; local feature in the input image. Weights &lt;em&gt;between&lt;/em&gt; feature maps are different so they can detect &lt;em&gt;different&lt;/em&gt; local features. Each unit in a feature map has a &lt;strong&gt;receptive field&lt;/strong&gt;. This is, a small $n \times n$ sub-area or “neighborhood” of the input image that can be “perceived” by a unit in the feature map.&lt;/p&gt;

&lt;p&gt;Feature maps and receptive fields sound complicated. Here is a metaphor that may be helpful: imagine that you have 6 flashlights with a &lt;em&gt;square&lt;/em&gt; beam of light. Each flashlight has the special quality of revealing certain “features” of images drawn with invisible ink, like corners or oriented edges. Also, imagine that you have a set of images that were drawn with invisible ink. Now, you need your special flashlights to reveal the hidden character in the image. What you need to do is to carefully illuminate each section of the invisible image, from &lt;em&gt;right to left and top to bottom&lt;/em&gt;, with each of your 6 flashlights. Once you finish the process, you should be able to put together all the little “features” revealed by each flashlight to compose the full image shape. Here, the square beam of light sliding over each pixel represents the aforementioned &lt;em&gt;receptive field&lt;/em&gt;, and each flashlight represents a &lt;em&gt;feature map&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 4&lt;/strong&gt; shows a simplified representation of the feature detection process (assuming that each time a pixel in the input image &lt;em&gt;match&lt;/em&gt; a pixel in the feature detector we add a value of 1, although in practice it can be any real-valued scalar). In this example we use a &lt;strong&gt;stride&lt;/strong&gt; of 1, meaning that we shift the receptive field by 1 pixel (to the right or down) for each cell in the feature map.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 4: Feature detection (convolution)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-8/convolution.png&quot; width=&quot;60%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The process of sliding over the image with the receptive field (sometimes called &lt;em&gt;kernels&lt;/em&gt;) of feature maps equals to a mathematical operation called &lt;strong&gt;convolution&lt;/strong&gt; (technically, equals to &lt;em&gt;cross-correlation&lt;/em&gt;, more about this later), hence the name &lt;strong&gt;convolutional network&lt;/strong&gt;. The full convolution operation involves repeating the process in &lt;strong&gt;Figure 4&lt;/strong&gt; for each feature map. If you are wondering how do you come up with appropriated features detectors, the answer is that you don’t need to: the &lt;strong&gt;feature maps weights are learned in the training process&lt;/strong&gt;. More on the mathematics of this later.&lt;/p&gt;

&lt;h3 id=&quot;the-pooling-operation-subsampling&quot;&gt;The pooling operation: subsampling&lt;/h3&gt;

&lt;p&gt;Once the convolution operation is done, what we have learned is whether a feature is present in the image or not. Now, knowing that a collection of features is present in an image won’t tell us, by itself, which image they correspond to. What we need to know is their &lt;strong&gt;approximate position relative to each other&lt;/strong&gt;. For instance, if we know that we have a “curvy horizontal line” at the center-bottom, a “curvy vertical line” at the middle-right, a “straight vertical line” at upper-left, and a “straight horizontal line” at the center-top, we should be able to tell we have a “5”. This is even more important considering that real-life images like handwritten numbers have considerable variability in their shape. No two individuals write numbers in the exact same manner. Hence, we want our network to be as &lt;em&gt;insensitive as possible&lt;/em&gt; to the absolute position of a feature, and as &lt;em&gt;sensitive as possible&lt;/em&gt; to its relative position: handwritten 5s may look different, but the curvy part is almost always at the bottom and the straight part at the top. This is sometimes referred to as &lt;strong&gt;invariance to local translation&lt;/strong&gt;. One way to accomplish this is by &lt;strong&gt;reducing the spatial resolution of the image&lt;/strong&gt;. This is what &lt;strong&gt;sub-sampling&lt;/strong&gt; or &lt;strong&gt;pooling&lt;/strong&gt; does.&lt;/p&gt;

&lt;p&gt;There are many ways to sub-sample an image. In the LeNet-5, this operation performs a &lt;strong&gt;local averaging&lt;/strong&gt; of a section of the feature map, effectively &lt;em&gt;reducing the resolution&lt;/em&gt; of the feature map as a whole, and the sensitivity of the network to shifts and distortions in the input image. A colloquial example is what happens when you “pixelate” an image like in &lt;strong&gt;Figure 5&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 5: sub-sampling effect&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-8/pixelated.png&quot; width=&quot;70%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A sub-sampling layer will have as many “pixelated” feature maps as “normal” feature maps in the convolutional layer. The &lt;strong&gt;mechanics of sub-sampling&lt;/strong&gt; are as follows: again, we have $n \times n$ receptive field that “perceives” a section of the “normal” feature map and connect to a unit in the “pixelated” feature map. This time, there is no overlap between each “stride” of the receptive field: each unit is connected to a &lt;em&gt;non-overlapping section&lt;/em&gt; of the original feature map. You can think about this as taking “strides” of a size equal to $n$, e.g., for a $3 \times 3$ feature map, we take a stride of $3$. Then, we take a weighted average of each pixel in the receptive field and pass the resulting sum through a sigmoid function (or any other non-linear function). The &lt;em&gt;weights&lt;/em&gt; in the weighted average are also parameters that the network learns with training. &lt;strong&gt;Figure 6&lt;/strong&gt; shows this process for a &lt;em&gt;single&lt;/em&gt; sub-sampled feature map.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 6: Sub-sampling (pooling)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-8/pooling.png&quot; width=&quot;70%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The result of sub-sampling is another grid of numbers (note that the numbers in &lt;strong&gt;Figure 6&lt;/strong&gt; are made up). We went from a $12 \times 12$ input image, to a $3 \times 3$ feature map after convolution and pooling (keep in mind that I intentionally reduced LeNet-5 original dimensions to simplify the examples). Since in our original example we had 6 features map, we need to repeat the process in &lt;strong&gt;Figure 6&lt;/strong&gt; 6 times, one of each feature map.&lt;/p&gt;

&lt;p&gt;The next convolution hidden layer $S_2$ increases the number of feature maps compared to $S_1$. If you were to add more sets of $S_n$ and $C_n$ hidden layers, you will repeat this alternating pattern again: &lt;em&gt;as the spatial resolution is reduced (by pooling), the number of feature maps in the next layer is increased&lt;/em&gt;. The idea here is to &lt;strong&gt;compensate for the reduction in spatial resolution by increasing the richness of the learned representations&lt;/strong&gt; (i.e., more feature maps).&lt;/p&gt;

&lt;p&gt;Once we are done with the sequence of convolution and pooling, the network implements a traditional fully-connected layer as in the &lt;a href=&quot;https://pabloinsente.github.io/the-multilayer-perceptron&quot;&gt;multi-layer perceptron&lt;/a&gt;. The first fully-connected $F_1$ layer has the role of &lt;strong&gt;“flattening”&lt;/strong&gt; the $C_2$ pooling layer. Remember that fully-connected layers take an input vector, and the dimensions of the LeNet-5 $C_2$ layer are a $5 \times 5 \times 16$ tensor, this is, sixteen 5 by 5 feature maps. The dimensionality of the first fully-connected layer is $120$, which is the result of another convolution. The next hidden layer $F_2$ “compress” the output even further into a vector of size $84$. Finally, we have the &lt;strong&gt;output-layer&lt;/strong&gt; implementing a &lt;strong&gt;euclidean radial basal function&lt;/strong&gt; (RBD) with 10 neurons to perform the classification of numbers (0-9).&lt;/p&gt;

&lt;h3 id=&quot;alexnet&quot;&gt;AlexNet&lt;/h3&gt;

&lt;p&gt;The LeNet-5 performance in the MNIST dataset was impressive but not out of the ordinary. Other methods like the Support Vector Machines could reach &lt;a href=&quot;http://yann.lecun.com/exdb/mnist/&quot;&gt;similar or better performance at the time&lt;/a&gt;. Training neural networks were still costly and complicated compared to other machine learning techniques, hence the interest in neural nets faded in the late ’90s again. However, several research groups continued to work in neural networks. The next big breakthrough in computer vision came in 2012  when Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton introduced the &lt;a href=&quot;https://en.wikipedia.org/wiki/AlexNet&quot;&gt;“AlexNet”&lt;/a&gt;, a convolutional neural network that won the &lt;a href=&quot;https://en.wikipedia.org/wiki/ImageNet#ImageNet_Challenge&quot;&gt;“ImageNet Large Scale Visual Recognition Challenge”&lt;/a&gt; for a wide margin, surprising the entire computer vision community.&lt;/p&gt;

&lt;p&gt;The main innovation introduced by AlexNet compared to the LeNet-5 was &lt;strong&gt;its sheer size&lt;/strong&gt;. AlexNet main elements are the same: a sequence of convolutional and pooling layers followed by a couple of fully-connected layers. The LeNet-5 has two sets of convolutional and pooling layers, two fully-connected layers, and an RBD classifier as an output layer. AlexNet has five convolutional layers, three pooling layers, two fully-connected layers, and a softmax classifier output layer. The training time and dataset were larger as well. All of this was possible thanks to the availability of more computational processing power (particularly &lt;a href=&quot;https://en.wikipedia.org/wiki/Graphics_processing_unit&quot;&gt;Graphics Processing Units (GPUs)&lt;/a&gt;), and larger datasets (because of the internet). There a few additional innovations introduced with AlexNet:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Rectifier Linear Units (ReLU)&lt;/strong&gt;: instead of the hyperbolic tangent (tanh) and sigmoid units. ReLUs train several times faster than tanh or sigmoid units.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Normalization layers&lt;/strong&gt;: aimed to reduce overfitting. More on this latter.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Dropout layers&lt;/strong&gt;: dropout consists of setting to zero the output of a hidden neuron with some probability, in this case, 0.5. Also aimed to help with overfitting.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data augmentation&lt;/strong&gt;: images were artificially translated, reflected, and distorted to increase the dataset size. The more variation in training examples, the more information available for the model to learn.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Diagramming AlexNet is complicated because the architecture is large and, at the time, they had to split the network into two GPUs, which is not necessary today. I’ll use simplified notation to describe AlexNet and compare it with LeNet-5 as shown in &lt;strong&gt;Figure 7&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 7: AlexNet and LeNet architectures&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-8/alexnet.png&quot; width=&quot;60%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Each layer in AlexNet is three dimensional because it was designed to classify 1000 &lt;em&gt;color images&lt;/em&gt; (LeNet-5 classified 10 grey-scale digits). The dimensions represent &lt;em&gt;width x height x RGB&lt;/em&gt; (red, green, and blue) color values. This type of 3-D arrays of numbers is often referred to as &lt;a href=&quot;https://en.wikipedia.org/wiki/Tensor&quot;&gt;mathematical tensors&lt;/a&gt;. The pooling operation is done by taking the maximum value in the receptive field instead of the average of all units, which is known as &lt;strong&gt;max pooling&lt;/strong&gt;. The pattern of connectivity between convolutional and pooling layers is different from the one in LeNet-5 too. Other than that, AlexNet utilizes the same building blocks and operations as LeNet-5.&lt;/p&gt;

&lt;h2 id=&quot;neural-network-models-of-vision-and-computer-vision-drifting-apart&quot;&gt;Neural network models of vision and computer vision drifting apart&lt;/h2&gt;

&lt;p&gt;If you ask a random researcher in computer vision about the correspondence between the human visual/perceptual system and convolutional nets, the most likely answer would be something like: “&lt;em&gt;Well, CNN’s are roughly inspired in the brain but aren’t actual models of the brain. I care about solving the problem artificial vision by any means necessary, regardless of the biological correspondence to human vision, more or less in the same manner we solved flying without having to imitate birds flapping&lt;/em&gt;”. Or some version of that. This talks to how &lt;strong&gt;computer vision has become an independent area of research with its own goals&lt;/strong&gt;. Most researchers are fully aware that many of the design properties of modern convolutional nets are not biologically realistic. Beyond the parallels with human vision, strictly speaking, LeNet-5 and AlexNet are designed to maximize object-recognition performance, not biological-realism. And that’s is perfectly fine. For instance, the LeNet 5 paper (1998) was published in the context of the debate between traditional pattern recognition with handcrafted features vs the automated learning-based approach of neural nets. Nothing was said about human perception. However, from our perspective, the issue of &lt;strong&gt;whether convolutional nets are a useful model of human perception and vision&lt;/strong&gt; is critical. This is an open debate. Many researchers do believe that convolutional nets are useful models for human vision and perception, and there is a long list of &lt;a href=&quot;https://www.mitpressjournals.org/doi/abs/10.1162/jocn_a_01544&quot;&gt;scientific articles trying to show this point&lt;/a&gt;. I won’t review those arguments now. My point is to highlight the fact that what I’ll describe next are “coarse” models attempting to approximate human abilities in narrow settings, not full-blown models of human vision.&lt;/p&gt;

&lt;h2 id=&quot;mathematical-formalization&quot;&gt;Mathematical formalization&lt;/h2&gt;

&lt;p&gt;I’ll describe the mathematics for the LeNet-5, with a few references to some AlexNet operations. LeNet-5 architecture can be described by the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;convolution function (for hidden layers)&lt;/li&gt;
  &lt;li&gt;non-linear transformation function (for hidden layers)&lt;/li&gt;
  &lt;li&gt;pooling function (for hidden layers)&lt;/li&gt;
  &lt;li&gt;linear function (for fully-connected layers)&lt;/li&gt;
  &lt;li&gt;euclidean radial basis (RBF) function (for the output)&lt;/li&gt;
  &lt;li&gt;cost function (to compute overall error)&lt;/li&gt;
  &lt;li&gt;learning procedure (i.e., backpropagation)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;convolution-function&quot;&gt;Convolution function&lt;/h3&gt;

&lt;p&gt;The convolution operation &lt;em&gt;convolutes&lt;/em&gt; pairs of functions. Here I’m using the plain meaning of “convoluted”: to intertwine or twist things together. In the neural network context, the functions we convolute together are the &lt;strong&gt;input function&lt;/strong&gt; $P$ and the &lt;strong&gt;kernel function&lt;/strong&gt; $K$ (remember that &lt;em&gt;kernel&lt;/em&gt; is another way to call the &lt;em&gt;receptive field&lt;/em&gt; of a feature map). For the 2-dimensional inputs as in LeNet-5, the $P_{ij}$ function contains the 2-dimensional values for the input image, which in our case are grayscale values between 0 (white) and 255 (black). The $K_{mn}$ function contains the 2-dimensional values for the kernel, this is the matrix of weights $W_{mn}$ to be learned by the network. The &lt;strong&gt;output&lt;/strong&gt; of the convolution is the feature map $F_{mn}$ in the next layer. In practice, the convolution operation is a &lt;strong&gt;linear operation&lt;/strong&gt;, i.e., a weighted sum.&lt;/p&gt;

&lt;p&gt;The convolution formula has different forms depending on the context. In the neural network context, we will compute a &lt;strong&gt;discrete convolution&lt;/strong&gt;. The &lt;strong&gt;convolution operator&lt;/strong&gt; is conventionally represented by the &lt;strong&gt;$\bf{*}$ symbol&lt;/strong&gt;. Hence, we define the convolution between $P$ and $K$ as:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-8/convolution-math.png&quot; width=&quot;70%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Where ${ij}$ are the width and length of the input image, and ${mn}$ are the width and length of the kernel.&lt;/p&gt;

&lt;p&gt;Technically, LeNet-5 also adds a “bias” $b$ to each convolution, so the full expression becomes:&lt;/p&gt;

\[F_{mn}= S(i,j) = b + (P*K)_{ij}  = b + \sum_m\sum_nP_{i-m, j-n} * K_{m,n}\]

&lt;p&gt;I’ll ignore the $b$ term since it’s not part of a convolution operation and it’s not relevant for its explanation.&lt;/p&gt;

&lt;p&gt;To apply a convolution the most important part is &lt;strong&gt;to get the indices right&lt;/strong&gt;. We will work with a $3 \times 3$ input image example. There are multiple index conventions floating around the internet. I will use the following for convolution:&lt;/p&gt;

\[P=
\begin{bmatrix}
p_{-1,1} &amp;amp; p_{0,1} &amp;amp; p_{1,1} \\
p_{-1,0} &amp;amp; p_{0,0} &amp;amp; p_{1,0} \\
p_{-1,-1} &amp;amp; p_{0,-1} &amp;amp; p_{1,-1}
\end{bmatrix}\]

&lt;p&gt;For a $2 \times 2$ kernel that can’t be centered at $0$, we will fix the bottom-left entry at $(0,0)$ as:&lt;/p&gt;

\[K=
\begin{bmatrix}
k_{0,1} &amp;amp; k_{1,1}\\
k_{0,0} &amp;amp; k_{1,0}
\end{bmatrix}\]

&lt;p&gt;I’ll use a trick to make the convolution operation clearer. I’ll replace the entries with actual numbers and overlay the matrices in a cartesian plane as in &lt;strong&gt;Figure 8&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 8&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-8/cartesian-matrix.png&quot; width=&quot;70%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now, remember that we want to compute a feature map $F$ with dimensions equal to $K$. Consequently, we need to compute 4 convolutions:&lt;/p&gt;

\[F_{00} = S(i=0,j=0) \\
F_{01} = S(i=0,j=1) \\
F_{10} = S(i=1,j=0) \\
F_{11} = S(i=1,j=1)\]

&lt;p&gt;To obtain:&lt;/p&gt;

\[F=
\begin{bmatrix}
f_{0,1} &amp;amp; f_{1,1}\\
f_{0,0} &amp;amp; f_{1,0}
\end{bmatrix}\]

&lt;p&gt;Let’s compute $F_{00} = S(i=0,j=0)$. Table 1 shows the entries to be multiplied and added together when we follow the double summtion $\sum_m\sum_nP_{i-m, j-n} * K_{m,n}$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Table 1&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;i&lt;/th&gt;
      &lt;th&gt;j&lt;/th&gt;
      &lt;th&gt;m&lt;/th&gt;
      &lt;th&gt;n&lt;/th&gt;
      &lt;th&gt;i-m&lt;/th&gt;
      &lt;th&gt;j-n&lt;/th&gt;
      &lt;th&gt;P&lt;sub&gt;i-m,j-n&lt;/sub&gt;&lt;/th&gt;
      &lt;th&gt;K&lt;sub&gt;m,n&lt;/sub&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0-0=0&lt;/td&gt;
      &lt;td&gt;0-0=0&lt;/td&gt;
      &lt;td&gt;P&lt;sub&gt;0,0&lt;/sub&gt;&lt;/td&gt;
      &lt;td&gt;K&lt;sub&gt;0,0&lt;/sub&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0-0=0&lt;/td&gt;
      &lt;td&gt;0-1=-1&lt;/td&gt;
      &lt;td&gt;P&lt;sub&gt;0,-1&lt;/sub&gt;&lt;/td&gt;
      &lt;td&gt;K&lt;sub&gt;0,1&lt;/sub&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0-1=-1&lt;/td&gt;
      &lt;td&gt;0-0=0&lt;/td&gt;
      &lt;td&gt;P&lt;sub&gt;-1,0&lt;/sub&gt;&lt;/td&gt;
      &lt;td&gt;K&lt;sub&gt;1,0&lt;/sub&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0-1=-1&lt;/td&gt;
      &lt;td&gt;0-1=-1&lt;/td&gt;
      &lt;td&gt;P&lt;sub&gt;-1,-1&lt;/sub&gt;&lt;/td&gt;
      &lt;td&gt;K&lt;sub&gt;1,1&lt;/sub&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We will color-code each entry in the matrices and compute the value for $F_{00}$ as in &lt;strong&gt;Figure 9&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 9&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-8/cartesian-matrix-1.png&quot; width=&quot;70%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can fill in with the first $F_{00}$ entry (here is where you could add $b$ to the summation result):&lt;/p&gt;

\[F=
\begin{bmatrix}
f_{0,1} &amp;amp; f_{1,1}\\
142.5 &amp;amp; f_{1,0}
\end{bmatrix}\]

&lt;p&gt;If you observe the indices carefully, you’ll notice the $P$ indices are the $K$ indices “flipped”. Taking $-K$ (i.e., $-m,-n$) &lt;strong&gt;reflects&lt;/strong&gt; the indices on the horizontal and vertical axes, whereas $j,i$ &lt;strong&gt;offset&lt;/strong&gt; the indices on their corresponding axes. In the last example, there was no offset because both $j,i$ equal $0$. &lt;strong&gt;Figure 10&lt;/strong&gt; shows the effects of reflecting by taking $-k$ and offsetting vertically and horizontally by different values of $j,i$:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 10&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-8/cartesian-matrix-rotations.png&quot; width=&quot;70%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s see what happens when we compute the next feature map entry $F_{01}$:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;i&lt;/th&gt;
      &lt;th&gt;j&lt;/th&gt;
      &lt;th&gt;m&lt;/th&gt;
      &lt;th&gt;n&lt;/th&gt;
      &lt;th&gt;i-m&lt;/th&gt;
      &lt;th&gt;j-n&lt;/th&gt;
      &lt;th&gt;P&lt;sub&gt;i-m,j-n&lt;/sub&gt;&lt;/th&gt;
      &lt;th&gt;K&lt;sub&gt;m,n&lt;/sub&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0-0=0&lt;/td&gt;
      &lt;td&gt;1-0=1&lt;/td&gt;
      &lt;td&gt;P&lt;sub&gt;0,1&lt;/sub&gt;&lt;/td&gt;
      &lt;td&gt;K&lt;sub&gt;0,0&lt;/sub&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0-0=0&lt;/td&gt;
      &lt;td&gt;1-1=0&lt;/td&gt;
      &lt;td&gt;P&lt;sub&gt;0,0&lt;/sub&gt;&lt;/td&gt;
      &lt;td&gt;K&lt;sub&gt;0,1&lt;/sub&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0-1=-1&lt;/td&gt;
      &lt;td&gt;1-0=1&lt;/td&gt;
      &lt;td&gt;P&lt;sub&gt;-1,1&lt;/sub&gt;&lt;/td&gt;
      &lt;td&gt;K&lt;sub&gt;1,0&lt;/sub&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0-1=-1&lt;/td&gt;
      &lt;td&gt;1-1=0&lt;/td&gt;
      &lt;td&gt;P&lt;sub&gt;-1,0&lt;/sub&gt;&lt;/td&gt;
      &lt;td&gt;K&lt;sub&gt;1,1&lt;/sub&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Graphically, this looks like:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 11&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-8/cartesian-matrix-2.png&quot; width=&quot;70%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can fill in with the second entry on $F_{01}$:&lt;/p&gt;

\[F=
\begin{bmatrix}
195 &amp;amp; f_{1,1}\\
142.5 &amp;amp; f_{1,0}
\end{bmatrix}\]

&lt;p&gt;The pattern from here is always the same: flipp, offset, overlay, multiply, and add. If we follow the pattern for $F_{11}$ and $F_{10}$, $F$ results in:&lt;/p&gt;

\[F=
\begin{bmatrix}
195 &amp;amp; 241\\
142.5 &amp;amp; 222
\end{bmatrix}\]

&lt;h3 id=&quot;convolution-in-practice-cross-correlation&quot;&gt;Convolution in practice: cross-correlation&lt;/h3&gt;

&lt;p&gt;This may come as a surprise to you but in practice, several deep learning libraries like &lt;a href=&quot;https://beta.mxnet.io/api/ndarray/_autogen/mxnet.ndarray.Convolution.html&quot;&gt;MXNet&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#convolution-layers&quot;&gt;Pytorch&lt;/a&gt; &lt;strong&gt;DO NOT implement convolutions&lt;/strong&gt; but a closely related operation called &lt;strong&gt;cross-correlation&lt;/strong&gt; (although the authors insist on calling it convolution). The cross-correlation operation is defined as:&lt;/p&gt;

\[F_{mn}= S(i,j) = (P \star K)_{ij} =\sum_m\sum_nP_{i+m, j+n} \star K_{m,n}\]

&lt;p&gt;If you pay close attention to $K_{i+m, j+n}$ you’ll see that the only difference is we are replacing the $-$ symbol with a $+$ symbol.&lt;/p&gt;

&lt;p&gt;Now, If we keep the same convention of centering the input image at zero, we will get into trouble. For instance, &lt;strong&gt;Table 2&lt;/strong&gt; shows the values for $i=0, j=1$:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Table 2&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;i&lt;/th&gt;
      &lt;th&gt;j&lt;/th&gt;
      &lt;th&gt;m&lt;/th&gt;
      &lt;th&gt;n&lt;/th&gt;
      &lt;th&gt;i+m&lt;/th&gt;
      &lt;th&gt;j+n&lt;/th&gt;
      &lt;th&gt;P&lt;sub&gt;i+m,j+n&lt;/sub&gt;&lt;/th&gt;
      &lt;th&gt;K&lt;sub&gt;m,n&lt;/sub&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0+0=0&lt;/td&gt;
      &lt;td&gt;1+0=1&lt;/td&gt;
      &lt;td&gt;P&lt;sub&gt;0,1&lt;/sub&gt;&lt;/td&gt;
      &lt;td&gt;K&lt;sub&gt;0,0&lt;/sub&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0+0=0&lt;/td&gt;
      &lt;td&gt;1+1=2&lt;/td&gt;
      &lt;td&gt;P&lt;sub&gt;0,2&lt;/sub&gt;&lt;/td&gt;
      &lt;td&gt;K&lt;sub&gt;0,1&lt;/sub&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0+1=1&lt;/td&gt;
      &lt;td&gt;1+0=1&lt;/td&gt;
      &lt;td&gt;P&lt;sub&gt;1,1&lt;/sub&gt;&lt;/td&gt;
      &lt;td&gt;K&lt;sub&gt;1,0&lt;/sub&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0+1=1&lt;/td&gt;
      &lt;td&gt;1+1=0&lt;/td&gt;
      &lt;td&gt;P&lt;sub&gt;2,0&lt;/sub&gt;&lt;/td&gt;
      &lt;td&gt;K&lt;sub&gt;1,1&lt;/sub&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Now we get $P_{02} \star K_{01}$, which does not make sense since we don’t have values at $P_{02}$. One way to address this is by &lt;strong&gt;padding&lt;/strong&gt; the input image with zeros like:&lt;/p&gt;

\[P=
\begin{bmatrix}
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; p_{-1,1} &amp;amp; p_{0,1} &amp;amp; p_{1,1} &amp;amp; 0 \\
0 &amp;amp; p_{-1,0} &amp;amp; p_{0,0} &amp;amp; p_{1,0} &amp;amp; 0\\
0 &amp;amp; p_{-1,-1} &amp;amp; p_{0,-1} &amp;amp; p_{1,-1} &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0
\end{bmatrix}\]

&lt;p&gt;Now we have values at $P_{02}$. I personally find such solution mathematically sketchy. A better aproach is to change the indices of $P$ to be:&lt;/p&gt;

\[P=
\begin{bmatrix}
p_{0,2} &amp;amp; p_{1,2} &amp;amp; p_{2,2} \\
p_{0,1} &amp;amp; p_{1,1} &amp;amp; p_{2,1} \\
p_{0,0} &amp;amp; p_{1,0} &amp;amp; p_{2,0}
\end{bmatrix}\]

&lt;p&gt;Now we have values at $P_{02}$ and no padding is nedded. If you iterate over $\sum_m\sum_nP_{i+m, j+n} \star K_{m,n}$ the indices will work just fine to obtain $F$.&lt;/p&gt;

&lt;p&gt;Notice that in cross-correlation there aren’t reflections just offsets. This is how the offsets look now:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 12&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-8/cartesian-matrix-cross.png&quot; width=&quot;70%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I’ll not compute the cross-correlation values. The computation is as simple as overlaying the kernel matrix $K$ on top of the $P$ input matrix and take a weighted sum. This is the reason why you’ll see most textbooks in deep learning explain convolutions as “sliding” the kernel over the image taking a stride of X. Essentially, cross-correlation is a &lt;strong&gt;measure of similarity&lt;/strong&gt; between the kernel and the input image: &lt;em&gt;the better the alignment, the higher the cross-correlation value&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;We addressed convolution and cross-correlation in 2 dimensions. Yet, keep in mind that you can use such techniques for problems in 1, 3, or N dimensions, and the mathematics extends naturally to such dimensions as well.&lt;/p&gt;

&lt;h3 id=&quot;a-note-about-convolution-and-cross-correlation&quot;&gt;A note about convolution and cross-correlation&lt;/h3&gt;

&lt;p&gt;Before moving to the next section I want to address two questions: &lt;strong&gt;Why to bother with convolutions (or cross-correlation) at all?&lt;/strong&gt;, and &lt;strong&gt;Does it matter if I use convolution or cross-correlation?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Regarding the first question, technically, we could “flatten” the image into a long vector &lt;em&gt;from the beginning&lt;/em&gt;, and then use a traditional multi-layer perceptron to classify the images. Just imagine aligning each pixel $[p_{00}, p_{01}, …, p_{ji}]$ one after the other in a long list, such that each pixel becomes a feature. This is way simpler than bothering with convolution or cross-correlation. The short answer is: because the &lt;strong&gt;topological information matters&lt;/strong&gt;. Flattening the image lose such topological information. In other words, the relative position of each pixel in the grid matters, and we want to &lt;strong&gt;exploit such space-dependence&lt;/strong&gt; when learning the network weights. If you train a model to recognize faces, the relative distance between &lt;em&gt;your eyes&lt;/em&gt; and your &lt;em&gt;mouth&lt;/em&gt; matters. Knowing that the images have a pair of eyes and a mouth is not enough. Therefore, we want to keep such a grid-structure when training the network.&lt;/p&gt;

&lt;p&gt;Regarding the second question, the answer is: &lt;strong&gt;no, it does not matter&lt;/strong&gt;. Well, it does matter in terms of the implementation, computational complexity, etc. But it does not matter in terms of finding a solution. The training algorithm will &lt;strong&gt;ensure that the appropriate weights are learned&lt;/strong&gt; regardless. In short, when you use &lt;em&gt;convolution&lt;/em&gt; the network will learn a matrix of weights $W$. If you use &lt;em&gt;cross-correlation&lt;/em&gt; instead, the network will learn a set of weights $-W$, i.e., the weights “flipped”.&lt;/p&gt;

&lt;h3 id=&quot;pooling-function&quot;&gt;Pooling function&lt;/h3&gt;

&lt;p&gt;After convolution, we “pool” the feature map output. “Pooling” essentially means to compute a summary statistic of a selected region of pixels. There are several pooling modalities: max pooling, average pooling, $L^2$ norm pooling, and others.&lt;/p&gt;

&lt;p&gt;LeNet-5 implements &lt;strong&gt;average pooling&lt;/strong&gt;, which is simply the average of the selected region in the feature map. For instance, if we take the feature map $F$:&lt;/p&gt;

\[F=
\begin{bmatrix}
195 &amp;amp; 241\\
142.5 &amp;amp; 222
\end{bmatrix}\]

&lt;p&gt;And apply average pooling, we get:&lt;/p&gt;

\[M_{mn} = \frac{1}{m \times n} \sum_m\sum_nf_{mn} = \frac{1}{2*2} (142.5 + 222 + 195 + 241) = \frac{1}{4} (800.5) = 200.125\]

&lt;p&gt;AlexNet implements &lt;strong&gt;max-pooling&lt;/strong&gt;, which is simply the largest value in the selected region of the feature map:&lt;/p&gt;

\[M_{mn} = max(F) = 222\]

&lt;p&gt;There is a lot to say about pooling. For are thorough evaluation in the field of object recognition see &lt;a href=&quot;http://ais.uni-bonn.de/papers/icann2010_maxpool.pdf&quot;&gt;Scherer, Muller, and Behnke (2011)&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;non-linear-transformation-function&quot;&gt;Non-linear transformation function&lt;/h3&gt;

&lt;p&gt;LeNet-5 implements two non-linear functions: a &lt;strong&gt;sigmoid&lt;/strong&gt; after the convolution layer and a &lt;strong&gt;scaled hyperbolic tangent&lt;/strong&gt; after the pooling layer. LeCun et all call the latter “squashing” function.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;sigmoid&lt;/strong&gt; function is defined as:&lt;/p&gt;

\[a = \sigma(f_{mn}) = \frac{1}{1+e^{-f_{mn}}}\]

&lt;p&gt;The sigmoid has an $S$ shape like in &lt;strong&gt;Chart 1&lt;/strong&gt; (the red line indicates the inflection point or threshold):&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scipy.special&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;expit&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;altair&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;5.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;5.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;expit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;z1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;a1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mark_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;threshold&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mark_rule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;red&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;z1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;a1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;threshold&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;properties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Chart 1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div id=&quot;altair-viz-a80f5bfcb35343cf93af9fa6a3604e1c&quot;&gt;&lt;/div&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
  (function(spec, embedOpt){
    const outputDiv = document.getElementById(&quot;altair-viz-a80f5bfcb35343cf93af9fa6a3604e1c&quot;);
    const paths = {
      &quot;vega&quot;: &quot;https://cdn.jsdelivr.net/npm//vega@5?noext&quot;,
      &quot;vega-lib&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-lib?noext&quot;,
      &quot;vega-lite&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-lite@4.0.2?noext&quot;,
      &quot;vega-embed&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-embed@6?noext&quot;,
    };

    function loadScript(lib) {
      return new Promise(function(resolve, reject) {
        var s = document.createElement(&apos;script&apos;);
        s.src = paths[lib];
        s.async = true;
        s.onload = () =&gt; resolve(paths[lib]);
        s.onerror = () =&gt; reject(`Error loading script: ${paths[lib]}`);
        document.getElementsByTagName(&quot;head&quot;)[0].appendChild(s);
      });
    }

    function showError(err) {
      outputDiv.innerHTML = `&lt;div class=&quot;error&quot; style=&quot;color:red;&quot;&gt;${err}&lt;/div&gt;`;
      throw err;
    }

    function displayChart(vegaEmbed) {
      vegaEmbed(outputDiv, spec, embedOpt)
        .catch(err =&gt; showError(`Javascript Error: ${err.message}&lt;br&gt;This usually means there&apos;s a typo in your chart specification. See the javascript console for the full traceback.`));
    }

    if(typeof define === &quot;function&quot; &amp;&amp; define.amd) {
      requirejs.config({paths});
      require([&quot;vega-embed&quot;], displayChart, err =&gt; showError(`Error loading script: ${err.message}`));
    } else if (typeof vegaEmbed === &quot;function&quot;) {
      displayChart(vegaEmbed);
    } else {
      loadScript(&quot;vega&quot;)
        .then(() =&gt; loadScript(&quot;vega-lite&quot;))
        .then(() =&gt; loadScript(&quot;vega-embed&quot;))
        .catch(showError)
        .then(() =&gt; displayChart(vegaEmbed));
    }
  })({&quot;config&quot;: {&quot;view&quot;: {&quot;continuousWidth&quot;: 400, &quot;continuousHeight&quot;: 300}}, &quot;layer&quot;: [{&quot;mark&quot;: &quot;line&quot;, &quot;encoding&quot;: {&quot;x&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;z&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;a&quot;}}}, {&quot;mark&quot;: {&quot;type&quot;: &quot;rule&quot;, &quot;color&quot;: &quot;red&quot;}, &quot;encoding&quot;: {&quot;x&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;z1&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;a1&quot;}}}], &quot;data&quot;: {&quot;name&quot;: &quot;data-4ce8df9580f59e633b8d399e9190ac6d&quot;}, &quot;title&quot;: &quot;Chart 1&quot;, &quot;$schema&quot;: &quot;https://vega.github.io/schema/vega-lite/v4.0.2.json&quot;, &quot;datasets&quot;: {&quot;data-4ce8df9580f59e633b8d399e9190ac6d&quot;: [{&quot;a&quot;: 0.0066928509242848554, &quot;z&quot;: -5.0, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.007391541344281971, &quot;z&quot;: -4.9, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.00816257115315989, &quot;z&quot;: -4.800000000000001, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.009013298652847815, &quot;z&quot;: -4.700000000000001, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.009951801866904308, &quot;z&quot;: -4.600000000000001, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.010986942630593162, &quot;z&quot;: -4.500000000000002, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.012128434984274213, &quot;z&quot;: -4.400000000000002, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.013386917827664744, &quot;z&quot;: -4.3000000000000025, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.014774031693273017, &quot;z&quot;: -4.200000000000003, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.01630249937144089, &quot;z&quot;: -4.100000000000003, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.017986209962091496, &quot;z&quot;: -4.0000000000000036, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.01984030573407743, &quot;z&quot;: -3.900000000000004, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.021881270936130383, &quot;z&quot;: -3.8000000000000043, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.024127021417669092, &quot;z&quot;: -3.7000000000000046, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.026596993576865725, &quot;z&quot;: -3.600000000000005, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.02931223075135617, &quot;z&quot;: -3.5000000000000053, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.03229546469845033, &quot;z&quot;: -3.4000000000000057, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.035571189272635965, &quot;z&quot;: -3.300000000000006, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.03916572279676412, &quot;z&quot;: -3.2000000000000064, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.043107254941085846, &quot;z&quot;: -3.1000000000000068, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.04742587317756646, &quot;z&quot;: -3.000000000000007, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.05215356307841737, &quot;z&quot;: -2.9000000000000075, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.05732417589886832, &quot;z&quot;: -2.800000000000008, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.06297335605699601, &quot;z&quot;: -2.700000000000008, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.06913842034334627, &quot;z&quot;: -2.6000000000000085, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.07585818002124294, &quot;z&quot;: -2.500000000000009, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.08317269649392166, &quot;z&quot;: -2.4000000000000092, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.09112296101485534, &quot;z&quot;: -2.3000000000000096, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.09975048911968425, &quot;z&quot;: -2.20000000000001, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.10909682119561194, &quot;z&quot;: -2.1000000000000103, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.11920292202211644, &quot;z&quot;: -2.0000000000000107, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.1301084743629966, &quot;z&quot;: -1.900000000000011, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.1418510649004864, &quot;z&quot;: -1.8000000000000114, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.15446526508353317, &quot;z&quot;: -1.7000000000000117, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.16798161486607383, &quot;z&quot;: -1.600000000000012, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.1824255238063545, &quot;z&quot;: -1.5000000000000124, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.19781611144141623, &quot;z&quot;: -1.4000000000000128, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.21416501695743917, &quot;z&quot;: -1.3000000000000131, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.23147521650098, &quot;z&quot;: -1.2000000000000135, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.24973989440487981, &quot;z&quot;: -1.1000000000000139, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.26894142136999233, &quot;z&quot;: -1.0000000000000142, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.28905049737499305, &quot;z&quot;: -0.9000000000000146, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.3100255188723844, &quot;z&quot;: -0.8000000000000149, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.3318122278318305, &quot;z&quot;: -0.7000000000000153, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.35434369377420094, &quot;z&quot;: -0.6000000000000156, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.3775406687981417, &quot;z&quot;: -0.500000000000016, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.40131233988754406, &quot;z&quot;: -0.40000000000001634, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.4255574831883369, &quot;z&quot;: -0.3000000000000167, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.45016600268751783, &quot;z&quot;: -0.20000000000001705, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.47502081252105566, &quot;z&quot;: -0.10000000000001741, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.49999999999999556, &quot;z&quot;: -1.7763568394002505e-14, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.5249791874789355, &quot;z&quot;: 0.09999999999998188, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.5498339973124733, &quot;z&quot;: 0.19999999999998153, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.5744425168116544, &quot;z&quot;: 0.29999999999998117, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.5986876601124473, &quot;z&quot;: 0.3999999999999808, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.62245933120185, &quot;z&quot;: 0.49999999999998046, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.6456563062257908, &quot;z&quot;: 0.5999999999999801, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.6681877721681616, &quot;z&quot;: 0.6999999999999797, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.6899744811276081, &quot;z&quot;: 0.7999999999999794, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.7109495026249997, &quot;z&quot;: 0.899999999999979, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.7310585786300007, &quot;z&quot;: 0.9999999999999787, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.7502601055951135, &quot;z&quot;: 1.0999999999999783, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.7685247834990137, &quot;z&quot;: 1.199999999999978, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.7858349830425548, &quot;z&quot;: 1.2999999999999776, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.8021838885585781, &quot;z&quot;: 1.3999999999999773, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.8175744761936402, &quot;z&quot;: 1.499999999999977, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.8320183851339212, &quot;z&quot;: 1.5999999999999766, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.8455347349164622, &quot;z&quot;: 1.6999999999999762, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.8581489350995093, &quot;z&quot;: 1.7999999999999758, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.8698915256369995, &quot;z&quot;: 1.8999999999999755, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.8807970779778798, &quot;z&quot;: 1.9999999999999751, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.8909031788043846, &quot;z&quot;: 2.0999999999999748, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.9002495108803125, &quot;z&quot;: 2.1999999999999744, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.9088770389851418, &quot;z&quot;: 2.299999999999974, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.9168273035060757, &quot;z&quot;: 2.3999999999999737, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.9241418199787546, &quot;z&quot;: 2.4999999999999734, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.9308615796566515, &quot;z&quot;: 2.599999999999973, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.937026643943002, &quot;z&quot;: 2.6999999999999726, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.9426758241011297, &quot;z&quot;: 2.7999999999999723, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.947846436921581, &quot;z&quot;: 2.899999999999972, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.9525741268224319, &quot;z&quot;: 2.9999999999999716, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.9568927450589126, &quot;z&quot;: 3.0999999999999712, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.9608342772032344, &quot;z&quot;: 3.199999999999971, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.9644288107273629, &quot;z&quot;: 3.2999999999999705, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.9677045353015485, &quot;z&quot;: 3.39999999999997, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.9706877692486428, &quot;z&quot;: 3.49999999999997, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.9734030064231335, &quot;z&quot;: 3.5999999999999694, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.97587297858233, &quot;z&quot;: 3.699999999999969, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.9781187290638689, &quot;z&quot;: 3.7999999999999687, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.9801596942659219, &quot;z&quot;: 3.8999999999999684, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.982013790037908, &quot;z&quot;: 3.999999999999968, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.9836975006285584, &quot;z&quot;: 4.099999999999968, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.9852259683067265, &quot;z&quot;: 4.199999999999967, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.9866130821723347, &quot;z&quot;: 4.299999999999967, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.9878715650157253, &quot;z&quot;: 4.399999999999967, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.9890130573694065, &quot;z&quot;: 4.499999999999966, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.9900481981330953, &quot;z&quot;: 4.599999999999966, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.9909867013471519, &quot;z&quot;: 4.6999999999999655, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.9918374288468399, &quot;z&quot;: 4.799999999999965, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}, {&quot;a&quot;: 0.9926084586557179, &quot;z&quot;: 4.899999999999965, &quot;z1&quot;: 0, &quot;a1&quot;: 0.5}]}}, {&quot;mode&quot;: &quot;vega-lite&quot;});
&lt;/script&gt;

&lt;p&gt;The &lt;strong&gt;scaled hyperbolic tangent function&lt;/strong&gt; is defined as:&lt;/p&gt;

\[\tau(f_{mn}) = A \times \frac{(e^{f_{mn}} - e^{f_{mn}})}{(e^{f_{mn}} + e^{f_{mn}})}\]

&lt;p&gt;With:
\(A = 1.7159\)&lt;/p&gt;

&lt;p&gt;In practice, the scaling factor can be omitted with no relevant side effects, as it’s done today.&lt;/p&gt;

&lt;p&gt;The shape of the scaled tanh function is similar to the sigmoid but the tanh can take negative values and the threshold point is at $0,0$:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;5.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;5.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;z1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;t1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tanh&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mark_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;threshold&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mark_rule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;red&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;z1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;t1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tanh&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;threshold&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;properties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Chart 2&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div id=&quot;altair-viz-b7c0b2ab45f14ab1bcdefdd21b4b0e7b&quot;&gt;&lt;/div&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
  (function(spec, embedOpt){
    const outputDiv = document.getElementById(&quot;altair-viz-b7c0b2ab45f14ab1bcdefdd21b4b0e7b&quot;);
    const paths = {
      &quot;vega&quot;: &quot;https://cdn.jsdelivr.net/npm//vega@5?noext&quot;,
      &quot;vega-lib&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-lib?noext&quot;,
      &quot;vega-lite&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-lite@4.0.2?noext&quot;,
      &quot;vega-embed&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-embed@6?noext&quot;,
    };

    function loadScript(lib) {
      return new Promise(function(resolve, reject) {
        var s = document.createElement(&apos;script&apos;);
        s.src = paths[lib];
        s.async = true;
        s.onload = () =&gt; resolve(paths[lib]);
        s.onerror = () =&gt; reject(`Error loading script: ${paths[lib]}`);
        document.getElementsByTagName(&quot;head&quot;)[0].appendChild(s);
      });
    }

    function showError(err) {
      outputDiv.innerHTML = `&lt;div class=&quot;error&quot; style=&quot;color:red;&quot;&gt;${err}&lt;/div&gt;`;
      throw err;
    }

    function displayChart(vegaEmbed) {
      vegaEmbed(outputDiv, spec, embedOpt)
        .catch(err =&gt; showError(`Javascript Error: ${err.message}&lt;br&gt;This usually means there&apos;s a typo in your chart specification. See the javascript console for the full traceback.`));
    }

    if(typeof define === &quot;function&quot; &amp;&amp; define.amd) {
      requirejs.config({paths});
      require([&quot;vega-embed&quot;], displayChart, err =&gt; showError(`Error loading script: ${err.message}`));
    } else if (typeof vegaEmbed === &quot;function&quot;) {
      displayChart(vegaEmbed);
    } else {
      loadScript(&quot;vega&quot;)
        .then(() =&gt; loadScript(&quot;vega-lite&quot;))
        .then(() =&gt; loadScript(&quot;vega-embed&quot;))
        .catch(showError)
        .then(() =&gt; displayChart(vegaEmbed));
    }
  })({&quot;config&quot;: {&quot;view&quot;: {&quot;continuousWidth&quot;: 400, &quot;continuousHeight&quot;: 300}}, &quot;layer&quot;: [{&quot;mark&quot;: &quot;line&quot;, &quot;encoding&quot;: {&quot;x&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;z&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;t&quot;}}}, {&quot;mark&quot;: {&quot;type&quot;: &quot;rule&quot;, &quot;color&quot;: &quot;red&quot;}, &quot;encoding&quot;: {&quot;x&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;z1&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;t1&quot;}}}], &quot;data&quot;: {&quot;name&quot;: &quot;data-26785d68b27a022afca9cca44793e3af&quot;}, &quot;title&quot;: &quot;Chart 2&quot;, &quot;$schema&quot;: &quot;https://vega.github.io/schema/vega-lite/v4.0.2.json&quot;, &quot;datasets&quot;: {&quot;data-26785d68b27a022afca9cca44793e3af&quot;: [{&quot;t&quot;: -0.9999092042625951, &quot;z&quot;: -5.0, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.9998891029505544, &quot;z&quot;: -4.9, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.9998645517007605, &quot;z&quot;: -4.800000000000001, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.9998345655542966, &quot;z&quot;: -4.700000000000001, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.9997979416121845, &quot;z&quot;: -4.600000000000001, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.9997532108480275, &quot;z&quot;: -4.500000000000002, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.9996985792838805, &quot;z&quot;: -4.400000000000002, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.9996318561900731, &quot;z&quot;: -4.3000000000000025, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.9995503664595334, &quot;z&quot;: -4.200000000000003, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.9994508436877974, &quot;z&quot;: -4.100000000000003, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.999329299739067, &quot;z&quot;: -4.0000000000000036, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.999180865670028, &quot;z&quot;: -3.900000000000004, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.9989995977858409, &quot;z&quot;: -3.8000000000000043, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.9987782412811312, &quot;z&quot;: -3.7000000000000046, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.9985079423323266, &quot;z&quot;: -3.600000000000005, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.9981778976111987, &quot;z&quot;: -3.5000000000000053, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.9977749279342794, &quot;z&quot;: -3.4000000000000057, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.9972829600991421, &quot;z&quot;: -3.300000000000006, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.9966823978396512, &quot;z&quot;: -3.2000000000000064, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.9959493592219003, &quot;z&quot;: -3.1000000000000068, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.9950547536867306, &quot;z&quot;: -3.000000000000007, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.9939631673505832, &quot;z&quot;: -2.9000000000000075, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.9926315202011281, &quot;z&quot;: -2.800000000000008, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.9910074536781178, &quot;z&quot;: -2.700000000000008, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.9890274022010994, &quot;z&quot;: -2.6000000000000085, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.9866142981514305, &quot;z&quot;: -2.500000000000009, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.9836748576936805, &quot;z&quot;: -2.4000000000000092, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.9800963962661917, &quot;z&quot;: -2.3000000000000096, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.975743130031452, &quot;z&quot;: -2.20000000000001, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.9704519366134545, &quot;z&quot;: -2.1000000000000103, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.9640275800758177, &quot;z&quot;: -2.0000000000000107, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.95623745812774, &quot;z&quot;: -1.900000000000011, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.9468060128462694, &quot;z&quot;: -1.8000000000000114, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.9354090706031004, &quot;z&quot;: -1.7000000000000117, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.9216685544064731, &quot;z&quot;: -1.600000000000012, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.9051482536448687, &quot;z&quot;: -1.5000000000000124, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.8853516482022653, &quot;z&quot;: -1.4000000000000128, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.8617231593133098, &quot;z&quot;: -1.3000000000000131, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.8336546070121593, &quot;z&quot;: -1.2000000000000135, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.8004990217606347, &quot;z&quot;: -1.1000000000000139, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.7615941559557708, &quot;z&quot;: -1.0000000000000142, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.7162978701990315, &quot;z&quot;: -0.9000000000000146, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.6640367702678572, &quot;z&quot;: -0.8000000000000149, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.6043677771171733, &quot;z&quot;: -0.7000000000000153, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.5370495669980464, &quot;z&quot;: -0.6000000000000156, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.46211715726002234, &quot;z&quot;: -0.500000000000016, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.3799489622552389, &quot;z&quot;: -0.40000000000001634, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.29131261245160617, &quot;z&quot;: -0.3000000000000167, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.19737532022492038, &quot;z&quot;: -0.20000000000001705, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -0.09966799462497306, &quot;z&quot;: -0.10000000000001741, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: -1.7763568394002505e-14, &quot;z&quot;: -1.7763568394002505e-14, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.09966799462493789, &quot;z&quot;: 0.09999999999998188, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.19737532022488624, &quot;z&quot;: 0.19999999999998153, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.2913126124515737, &quot;z&quot;: 0.29999999999998117, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.3799489622552084, &quot;z&quot;: 0.3999999999999808, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.4621171572599943, &quot;z&quot;: 0.49999999999998046, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.5370495669980211, &quot;z&quot;: 0.5999999999999801, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.6043677771171506, &quot;z&quot;: 0.6999999999999797, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.6640367702678375, &quot;z&quot;: 0.7999999999999794, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.7162978701990141, &quot;z&quot;: 0.899999999999979, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.7615941559557559, &quot;z&quot;: 0.9999999999999787, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.8004990217606219, &quot;z&quot;: 1.0999999999999783, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.8336546070121486, &quot;z&quot;: 1.199999999999978, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.8617231593133006, &quot;z&quot;: 1.2999999999999776, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.8853516482022576, &quot;z&quot;: 1.3999999999999773, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.9051482536448623, &quot;z&quot;: 1.499999999999977, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.9216685544064678, &quot;z&quot;: 1.5999999999999766, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.935409070603096, &quot;z&quot;: 1.6999999999999762, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.9468060128462658, &quot;z&quot;: 1.7999999999999758, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.956237458127737, &quot;z&quot;: 1.8999999999999755, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.9640275800758151, &quot;z&quot;: 1.9999999999999751, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.9704519366134524, &quot;z&quot;: 2.0999999999999748, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.9757431300314503, &quot;z&quot;: 2.1999999999999744, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.9800963962661904, &quot;z&quot;: 2.299999999999974, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.9836748576936793, &quot;z&quot;: 2.3999999999999737, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.9866142981514295, &quot;z&quot;: 2.4999999999999734, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.9890274022010986, &quot;z&quot;: 2.599999999999973, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.9910074536781172, &quot;z&quot;: 2.6999999999999726, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.9926315202011277, &quot;z&quot;: 2.7999999999999723, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.9939631673505828, &quot;z&quot;: 2.899999999999972, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.9950547536867301, &quot;z&quot;: 2.9999999999999716, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.9959493592219, &quot;z&quot;: 3.0999999999999712, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.996682397839651, &quot;z&quot;: 3.199999999999971, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.997282960099142, &quot;z&quot;: 3.2999999999999705, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.9977749279342792, &quot;z&quot;: 3.39999999999997, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.9981778976111986, &quot;z&quot;: 3.49999999999997, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.9985079423323265, &quot;z&quot;: 3.5999999999999694, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.9987782412811311, &quot;z&quot;: 3.699999999999969, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.9989995977858408, &quot;z&quot;: 3.7999999999999687, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.9991808656700278, &quot;z&quot;: 3.8999999999999684, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.999329299739067, &quot;z&quot;: 3.999999999999968, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.9994508436877974, &quot;z&quot;: 4.099999999999968, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.9995503664595334, &quot;z&quot;: 4.199999999999967, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.9996318561900731, &quot;z&quot;: 4.299999999999967, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.9996985792838805, &quot;z&quot;: 4.399999999999967, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.9997532108480275, &quot;z&quot;: 4.499999999999966, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.9997979416121845, &quot;z&quot;: 4.599999999999966, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.9998345655542966, &quot;z&quot;: 4.6999999999999655, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.9998645517007605, &quot;z&quot;: 4.799999999999965, &quot;z1&quot;: 0, &quot;t1&quot;: -1}, {&quot;t&quot;: 0.9998891029505544, &quot;z&quot;: 4.899999999999965, &quot;z1&quot;: 0, &quot;t1&quot;: -1}]}}, {&quot;mode&quot;: &quot;vega-lite&quot;});
&lt;/script&gt;

&lt;p&gt;The main reason to use this “squashing” function is to speed up learning convergence. Basically, symmetric functions are believed to converge faster. For a more complete explanation see &lt;em&gt;Appendix A&lt;/em&gt; in LeCun et all (1998).&lt;/p&gt;

&lt;p&gt;Nowadays, a more popular non-linear function is the &lt;strong&gt;ReLU&lt;/strong&gt; or rectified linear unit. This is what AlexNet uses. The ReLU simply returns the positive side of the input function or $0$:&lt;/p&gt;

\[r (x) = max(x,0)\]

&lt;p&gt;The shape of the ReLU is a straight line in the negative side of the number line and linear function on the positive side:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;5.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;5.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;z1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;r1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tanh&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mark_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;threshold&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mark_rule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;red&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;z1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;r1&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tanh&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;threshold&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;properties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Chart 2&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div id=&quot;altair-viz-929287aa82c8496887646430e25a2c2e&quot;&gt;&lt;/div&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
  (function(spec, embedOpt){
    const outputDiv = document.getElementById(&quot;altair-viz-929287aa82c8496887646430e25a2c2e&quot;);
    const paths = {
      &quot;vega&quot;: &quot;https://cdn.jsdelivr.net/npm//vega@5?noext&quot;,
      &quot;vega-lib&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-lib?noext&quot;,
      &quot;vega-lite&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-lite@4.0.2?noext&quot;,
      &quot;vega-embed&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-embed@6?noext&quot;,
    };

    function loadScript(lib) {
      return new Promise(function(resolve, reject) {
        var s = document.createElement(&apos;script&apos;);
        s.src = paths[lib];
        s.async = true;
        s.onload = () =&gt; resolve(paths[lib]);
        s.onerror = () =&gt; reject(`Error loading script: ${paths[lib]}`);
        document.getElementsByTagName(&quot;head&quot;)[0].appendChild(s);
      });
    }

    function showError(err) {
      outputDiv.innerHTML = `&lt;div class=&quot;error&quot; style=&quot;color:red;&quot;&gt;${err}&lt;/div&gt;`;
      throw err;
    }

    function displayChart(vegaEmbed) {
      vegaEmbed(outputDiv, spec, embedOpt)
        .catch(err =&gt; showError(`Javascript Error: ${err.message}&lt;br&gt;This usually means there&apos;s a typo in your chart specification. See the javascript console for the full traceback.`));
    }

    if(typeof define === &quot;function&quot; &amp;&amp; define.amd) {
      requirejs.config({paths});
      require([&quot;vega-embed&quot;], displayChart, err =&gt; showError(`Error loading script: ${err.message}`));
    } else if (typeof vegaEmbed === &quot;function&quot;) {
      displayChart(vegaEmbed);
    } else {
      loadScript(&quot;vega&quot;)
        .then(() =&gt; loadScript(&quot;vega-lite&quot;))
        .then(() =&gt; loadScript(&quot;vega-embed&quot;))
        .catch(showError)
        .then(() =&gt; displayChart(vegaEmbed));
    }
  })({&quot;config&quot;: {&quot;view&quot;: {&quot;continuousWidth&quot;: 400, &quot;continuousHeight&quot;: 300}}, &quot;layer&quot;: [{&quot;mark&quot;: &quot;line&quot;, &quot;encoding&quot;: {&quot;x&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;z&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;r&quot;}}}, {&quot;mark&quot;: {&quot;type&quot;: &quot;rule&quot;, &quot;color&quot;: &quot;red&quot;}, &quot;encoding&quot;: {&quot;x&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;z1&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;r1&quot;}}}], &quot;data&quot;: {&quot;name&quot;: &quot;data-61d1aa4674fe2b4cb14b2351f7bb5df7&quot;}, &quot;title&quot;: &quot;Chart 2&quot;, &quot;$schema&quot;: &quot;https://vega.github.io/schema/vega-lite/v4.0.2.json&quot;, &quot;datasets&quot;: {&quot;data-61d1aa4674fe2b4cb14b2351f7bb5df7&quot;: [{&quot;r&quot;: 0.0, &quot;z&quot;: -5.0, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -4.9, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -4.800000000000001, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -4.700000000000001, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -4.600000000000001, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -4.500000000000002, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -4.400000000000002, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -4.3000000000000025, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -4.200000000000003, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -4.100000000000003, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -4.0000000000000036, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -3.900000000000004, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -3.8000000000000043, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -3.7000000000000046, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -3.600000000000005, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -3.5000000000000053, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -3.4000000000000057, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -3.300000000000006, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -3.2000000000000064, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -3.1000000000000068, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -3.000000000000007, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -2.9000000000000075, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -2.800000000000008, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -2.700000000000008, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -2.6000000000000085, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -2.500000000000009, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -2.4000000000000092, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -2.3000000000000096, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -2.20000000000001, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -2.1000000000000103, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -2.0000000000000107, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -1.900000000000011, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -1.8000000000000114, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -1.7000000000000117, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -1.600000000000012, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -1.5000000000000124, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -1.4000000000000128, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -1.3000000000000131, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -1.2000000000000135, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -1.1000000000000139, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -1.0000000000000142, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -0.9000000000000146, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -0.8000000000000149, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -0.7000000000000153, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -0.6000000000000156, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -0.500000000000016, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -0.40000000000001634, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -0.3000000000000167, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -0.20000000000001705, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -0.10000000000001741, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.0, &quot;z&quot;: -1.7763568394002505e-14, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.09999999999998188, &quot;z&quot;: 0.09999999999998188, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.19999999999998153, &quot;z&quot;: 0.19999999999998153, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.29999999999998117, &quot;z&quot;: 0.29999999999998117, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.3999999999999808, &quot;z&quot;: 0.3999999999999808, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.49999999999998046, &quot;z&quot;: 0.49999999999998046, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.5999999999999801, &quot;z&quot;: 0.5999999999999801, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.6999999999999797, &quot;z&quot;: 0.6999999999999797, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.7999999999999794, &quot;z&quot;: 0.7999999999999794, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.899999999999979, &quot;z&quot;: 0.899999999999979, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 0.9999999999999787, &quot;z&quot;: 0.9999999999999787, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 1.0999999999999783, &quot;z&quot;: 1.0999999999999783, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 1.199999999999978, &quot;z&quot;: 1.199999999999978, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 1.2999999999999776, &quot;z&quot;: 1.2999999999999776, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 1.3999999999999773, &quot;z&quot;: 1.3999999999999773, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 1.499999999999977, &quot;z&quot;: 1.499999999999977, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 1.5999999999999766, &quot;z&quot;: 1.5999999999999766, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 1.6999999999999762, &quot;z&quot;: 1.6999999999999762, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 1.7999999999999758, &quot;z&quot;: 1.7999999999999758, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 1.8999999999999755, &quot;z&quot;: 1.8999999999999755, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 1.9999999999999751, &quot;z&quot;: 1.9999999999999751, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 2.0999999999999748, &quot;z&quot;: 2.0999999999999748, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 2.1999999999999744, &quot;z&quot;: 2.1999999999999744, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 2.299999999999974, &quot;z&quot;: 2.299999999999974, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 2.3999999999999737, &quot;z&quot;: 2.3999999999999737, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 2.4999999999999734, &quot;z&quot;: 2.4999999999999734, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 2.599999999999973, &quot;z&quot;: 2.599999999999973, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 2.6999999999999726, &quot;z&quot;: 2.6999999999999726, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 2.7999999999999723, &quot;z&quot;: 2.7999999999999723, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 2.899999999999972, &quot;z&quot;: 2.899999999999972, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 2.9999999999999716, &quot;z&quot;: 2.9999999999999716, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 3.0999999999999712, &quot;z&quot;: 3.0999999999999712, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 3.199999999999971, &quot;z&quot;: 3.199999999999971, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 3.2999999999999705, &quot;z&quot;: 3.2999999999999705, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 3.39999999999997, &quot;z&quot;: 3.39999999999997, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 3.49999999999997, &quot;z&quot;: 3.49999999999997, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 3.5999999999999694, &quot;z&quot;: 3.5999999999999694, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 3.699999999999969, &quot;z&quot;: 3.699999999999969, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 3.7999999999999687, &quot;z&quot;: 3.7999999999999687, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 3.8999999999999684, &quot;z&quot;: 3.8999999999999684, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 3.999999999999968, &quot;z&quot;: 3.999999999999968, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 4.099999999999968, &quot;z&quot;: 4.099999999999968, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 4.199999999999967, &quot;z&quot;: 4.199999999999967, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 4.299999999999967, &quot;z&quot;: 4.299999999999967, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 4.399999999999967, &quot;z&quot;: 4.399999999999967, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 4.499999999999966, &quot;z&quot;: 4.499999999999966, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 4.599999999999966, &quot;z&quot;: 4.599999999999966, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 4.6999999999999655, &quot;z&quot;: 4.6999999999999655, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 4.799999999999965, &quot;z&quot;: 4.799999999999965, &quot;z1&quot;: 0, &quot;r1&quot;: -1}, {&quot;r&quot;: 4.899999999999965, &quot;z&quot;: 4.899999999999965, &quot;z1&quot;: 0, &quot;r1&quot;: -1}]}}, {&quot;mode&quot;: &quot;vega-lite&quot;});
&lt;/script&gt;

&lt;h3 id=&quot;linear-function&quot;&gt;Linear function&lt;/h3&gt;

&lt;p&gt;After the second pooling layer, LeNet-5 “flattens” the 2-dimensional matrix into a vector. At this point flattening is not a problem because the features have been extracted in previous layers. Then computes a linear function $Z$, this is the dot product between the weights and the flattened vector and passes the result trough the sigmoid. We have seen this function before &lt;a href=&quot;https://pabloinsente.github.io/the-adaline&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://pabloinsente.github.io/the-multilayer-perceptron&quot;&gt;here&lt;/a&gt;. It is defined as:&lt;/p&gt;

\[z(f_{mn}) = b + \sum x_n w_{mn}\]

&lt;p&gt;In matrix notation:&lt;/p&gt;

\[\bf{z} = W^T \times \bf{x}\]

&lt;h3 id=&quot;output-function&quot;&gt;Output function&lt;/h3&gt;

&lt;p&gt;In previous blog-posts, we worked with either binary or real-valued outputs. Now we have a multi-class problem. LeNet-5 approaches this by implementing a &lt;strong&gt;Euclidean radial basis (RBF) function&lt;/strong&gt;. Each output unit computes:&lt;/p&gt;

\[\hat{y_i} = \sum_j (a_j - w_{ij})^2\]

&lt;p&gt;This equals to compute the Euclidean distance between the input vector $\bf{a}$ (output of the sigmoid) and the parameter vector $\bf{w}$. To understand why this works we need to delve into probability theory, which I’ll skip for now since it requires introducing several new mathematical ideas. In brief, the larger the distance between the input vector $\bf{x}$ and the parameter vector $\bf{w}$, the larger the RBF value output, and the more likely the unit to activate.&lt;/p&gt;

&lt;p&gt;In modern neural networks, like AlexNet, multi-class problems are usually approached with a &lt;strong&gt;softmax function&lt;/strong&gt; defined as:&lt;/p&gt;

\[\sigma(a)_i = \frac{e^{a_i}}{\sum_{j=1}^K e^{a_j}} \text {  for i} =1,...,K\text{  and}= (z_1,...,z_K)\]

&lt;p&gt;Let’s unpack this: first, we apply the exponential function to each element of the vector $\bf{a}$, and then, we normalize by dividing by the sum of all exponentials values. The normalization constrains the sum of $\sigma(a)_i$ to 1. Now that we normalized to 1, each element of the output vector &lt;strong&gt;can be interpreted as an activation probability&lt;/strong&gt;. This is convinient since now we can rank the outputs by its probability. For instance, in a 10-class problem, the unit with the highest value is the most likely value for a given input pattern.&lt;/p&gt;

&lt;h3 id=&quot;cost-function&quot;&gt;Cost function&lt;/h3&gt;

&lt;p&gt;In previous blog-posts, we used the Mean Squared Error (MSE) as a measure of “goodness” (or “badness”) of the network predictions. This is not possible here since we are working on a multi-class problem and the RBD function. The LeNet-5 implements the &lt;strong&gt;Maximum a Posteriori&lt;/strong&gt; &lt;a href=&quot;https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation&quot;&gt;(MAP)&lt;/a&gt; criterion as a cost function, defined as:&lt;/p&gt;

\[E(W) = \frac{1}{P}\sum_{p=1}^X(\hat{y}_{D^{p}}(X^p, W) + \log(e^{-j} + \sum_i e^{-\hat{y}(X^p, W)}))\]

&lt;p&gt;Explaining the cost function goes beyond what I want to cover here, and requires familiarity with Bayesian estimation. In brief, it maximizes the posterior probability of the correct class $D_p$ and selects the most likely class. If you are curious about the mechanics of MAP &lt;a href=&quot;https://www.youtube.com/watch?v=kkhdIriddSI&quot;&gt;here is a video&lt;/a&gt; with a detailed explanation.&lt;/p&gt;

&lt;p&gt;We mentioned the softmax as an alternative output function. The cost function used to train a network with a softmax output typically is the &lt;strong&gt;cross-entropy loss&lt;/strong&gt; defined as:&lt;/p&gt;

\[E_i = -\sum_i y_ i log (p_i)\]

&lt;p&gt;Where $y_i$ is the true label for $ith$ output unit, and $p_i$ is the softmax probability value for the $ith$ output unit. In short, cross-entropy computes the distance between the &lt;strong&gt;model output distribution&lt;/strong&gt; and the &lt;strong&gt;real output distribution&lt;/strong&gt;. A video with a extended explanation of the &lt;strong&gt;cross-entropy loss&lt;/strong&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=bLb_Kp5Q9cw&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;learning-procedure-backpropagation&quot;&gt;Learning procedure: backpropagation&lt;/h3&gt;

&lt;p&gt;As in the multi-layer perceptron, the gradients for the error with respect to the weights are computed with &lt;strong&gt;backpropagation&lt;/strong&gt;. I gave an extended explanation for backpropagation for the multi-layer perceptron case &lt;a href=&quot;https://pabloinsente.github.io/the-multilayer-perceptron&quot;&gt;here&lt;/a&gt;. For now, let’s just remember that the general form for backpropagation is:&lt;/p&gt;

\[\frac{\partial E}{\partial{W^{L-n}}} = \frac{\partial E}{\partial{f^L}} \frac{\partial{f^L}}{\partial{f^{L-1}}} ... \frac{\partial{f^{L-n}}}{\partial{W^{L-n}}}\]

&lt;p&gt;This is, sequentially applying the chain-rule of calculus layer by layer, until we reach $W$ in the layer $L-n$.&lt;/p&gt;

&lt;p&gt;There is one important adjustment to mention regarding convolutional networks. Remember that each feature map (convolution) &lt;strong&gt;shares weights&lt;/strong&gt;. To account for this, LeCun et all (1998) recommend to first compute each error derivative as if the network were a standard multi-layer perceptron without weight sharing, and then add the derivatives of all connections that share the same parameters.&lt;/p&gt;

&lt;p&gt;For the full derivation of backpropagation for convolutional networks see &lt;a href=&quot;http://www.iro.umontreal.ca/~lisa/bib/pub_subject/language/pointeurs/convolution.pdf&quot;&gt;Goodfellow (2010)&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;a-note-about-generalization-in-neural-networks&quot;&gt;A note about generalization in neural networks&lt;/h2&gt;

&lt;p&gt;Before implementing LeNet-5 and AlexNet I want to introduce an issue I have ignored so far: model &lt;strong&gt;generalization&lt;/strong&gt;. If you are familiar with these issues feel free to skip to the next section. I’ve intentionally ignored these issues to fully focus on understanding the ideas behind the models.&lt;/p&gt;

&lt;p&gt;Fundamentally, a neural network is an equation, a function. A very complicated one but it’s just that: a bunch of functions wrapping each other with some number of adjustable parameters (the weights). The assumption of any machine learning model is that there is an equation “out there” describing the relationship between &lt;strong&gt;features&lt;/strong&gt; and &lt;strong&gt;outputs&lt;/strong&gt;, more or less in the same fashion that $e=mc^2$ describes the relationship between &lt;em&gt;energy&lt;/em&gt; (i.e., &lt;em&gt;the output&lt;/em&gt;) as a function of mass (i.e., &lt;em&gt;feature 1&lt;/em&gt;) and the speed of light squared (i.e., &lt;em&gt;feature 2&lt;/em&gt;). And, that such equation can be &lt;em&gt;learned&lt;/em&gt; or &lt;em&gt;approximated&lt;/em&gt; by using a combination of an architecture and a training algorithm. Once you learn/approximate the equation, you can feed feature values (like mass and speed of light) to the network and make very precise predictions about the output (energy), like in physics.&lt;/p&gt;

&lt;p&gt;Now, the issue with machine learning is that problems most people are interested to solve, like image recognition or language understanding, are so complex that no simple formula can be (or has been) found. Otherwise, we would not bother at all with machine learning. Therefore, we &lt;em&gt;flipped the problem&lt;/em&gt;: we build systems that learn (approximate) the solution (function) for us by means of &lt;strong&gt;extracting patterns from exemplars&lt;/strong&gt;. For instance, in face recognition, there is no known equation describing the relationship between the pixels in a face-image and subject identity. So we try to learn one. The problem with this strategy is that our models are &lt;strong&gt;constrained to the information contained in the data&lt;/strong&gt;. Let’s say you train a “children vs adults” classifier. In an ideal world, you would want to take a large random sample of the entire human species to train your model, so your model can extract “patterns” (i.e., statistical information) present in all human sub-groups. In practice, you can’t do that. What you can do is to take freely available face-images from the internet. It is easy to see the problem with such a strategy: face-images on the internet are &lt;strong&gt;highly biased&lt;/strong&gt;. For instance, the internet contains way more photos of white people from developed countries than from, let’s say, Aboriginal Australians. The same goes for gender, age, and any other variation imaginable: the data is biased. If you train your model to perfection with that kind of dataset, your model may be very good at classifying pictures from the former group, but not from the latter. Another way to say this is that your model &lt;strong&gt;does not generalize well to out-of-sample exemplars&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Machine learning practitioners are fully aware of this problem. The traditional strategy to address it is to split the sample into two groups: &lt;strong&gt;training&lt;/strong&gt; and &lt;strong&gt;testing&lt;/strong&gt;. The training sample is used to train the model and learn the mathematical function relating features to targets. The testing sample is used &lt;em&gt;only&lt;/em&gt; to test the model and obtain a measure of &lt;strong&gt;generalization&lt;/strong&gt;. When the training and testing accuracy are really close, we say the model predictions generalize well. Otherwise, we say the model is &lt;strong&gt;overfitting&lt;/strong&gt; the data. This is, the learned equation “fits” the data so well, that now is unable to make good predictions for out-of-sample exemplars. We also have the opposite phenomena: &lt;strong&gt;underfitting&lt;/strong&gt;. A model is underfitted when it is so simple that has poor accuracy in the training set, and mediocre to poor in the testing set.&lt;/p&gt;

&lt;p&gt;Overfitting is usually approached with a mix of techniques in neural networks: data augmentation, dropout, weight decay, early stopping, and others. Overfitting is often interpreted as a sign of excessive &lt;strong&gt;model complexity&lt;/strong&gt;, so all these techniques try to simplify the model. Underfitting is usually approached with more training, more complex architectures, or more data. The goal is the opposite, to get increase model complexity so it can fit the data. In practice, the goal is to obtain a &lt;strong&gt;balanced&lt;/strong&gt; model: neither overfitted nor underfitted, just “good-enough” fitted.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 13&lt;/strong&gt; summarizes underfitting and overfitting in the two dimensional case. A extended treatment of issue for neural networks can be found &lt;a href=&quot;https://www.deeplearningbook.org/contents/regularization.html&quot;&gt;here&lt;/a&gt;, &lt;a href=&quot;https://d2l.ai/chapter_multilayer-perceptrons/underfit-overfit.html&quot;&gt;here&lt;/a&gt;, and &lt;a href=&quot;https://arxiv.org/pdf/1710.10686.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 13: overfitting, underfitting, and good-fitting&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-8/regularization.png&quot; width=&quot;60%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From &lt;strong&gt;Figure 13&lt;/strong&gt; is clear than when we move from the training data to the testing data, the “good-fitting” curve generalizes better than the other two.&lt;/p&gt;

&lt;p&gt;Is important to remark that even if you split your data into training-set and testing-set, and get excellent test-sample accuracy, that won’t guarantee that your model would generalize well into the real world, for the same reasons I mentioned at the beginning: your model is constrained to the statistical information contained in your data. We can only hope that our sample data is a good enough representation of the real-world data. &lt;strong&gt;No amount of regularization, cross-validation, etc. will fix poorly sampled data&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;code-implementation&quot;&gt;Code implementation&lt;/h2&gt;

&lt;p&gt;In previous blog-posts, I implemented the networks and backpropagation from scratch using Numpy and Python. I won’t do that this time because it’s going to take several hundreds of lines of code. It also requires relatively advanced knowledge of Python just to read the code, which is not assumed here.&lt;/p&gt;

&lt;p&gt;Fortunately, modern deep learning libraries make relatively easy to build convolutional networks. I’ll implement LeNet-5 and AlexNet using &lt;a href=&quot;https://keras.io/&quot;&gt;Keras&lt;/a&gt;. For LeNet-5 we will use the MNIST dataset as in the original paper. Unfortunately, the ImageNet dataset is too large (1.2 million high-resolution images) to utilize in a free-to-use CPU cloud environment. It may take days to train, and I’m not even sure it would fit in memory. As a replacement, we will use the &lt;a href=&quot;https://www.cs.toronto.edu/~kriz/cifar.html&quot;&gt;CIFAR-10 small image dataset&lt;/a&gt; to test AlexNet.&lt;/p&gt;

&lt;h3 id=&quot;mnist-classification-with-lenet-5&quot;&gt;MNIST classification with LeNet-5&lt;/h3&gt;

&lt;p&gt;There are more tutorials about MNIST image classification with Keras on the internet that I can count. Yet very few implement LeNet-5, and most assume you know Keras already. I’ll base LeNet-5 implementation on the generic CovNet &lt;a href=&quot;https://keras.io/examples/mnist_cnn/&quot;&gt;example provided by the official Keras documentation&lt;/a&gt;. Implementing LeNet-5 in its original form requires a lot of custom code, particularly because the RBD activation function and the MAP cost function are hardly used anymore, hence they are not part of the standard set of functions in modern deep learning libraries. Therefore, I’ll use the softmax activation and cross-entropy loss as replacements. The goal here is dual: to modify a standard CovNet implementation to match it &lt;em&gt;as closely as possible&lt;/em&gt; to the original LeNet-5, and to provide explanations at each step of the implementation.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keras.datasets&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mnist&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keras.models&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keras.layers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Flatten&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keras.layers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Conv2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AveragePooling2D&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;backend&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;altair&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;reading-data-from-keras&quot;&gt;Reading data from Keras&lt;/h3&gt;

&lt;p&gt;Keras comes with several popular datasets, like the MNIST database of handwritten digits and others that you can check &lt;a href=&quot;https://keras.io/datasets/&quot;&gt;here&lt;/a&gt;. This dataset comprises 60,000 28x28 grayscale images for the training set and 10,000 for the testing set. MNIST is loaded as a (number-sample, 28, 28) multidimensional array.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mnist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;load_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# unpack data into training and test
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;x_train shape:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;x_test shape:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;x_train shape:(60000, 28, 28)
x_test shape:(10000, 28, 28)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Let’s visualize a few examples images from the dataset.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cmap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;get_cmap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;gray&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-8/mnist-digits.png&quot; width=&quot;60%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;reshaping-data&quot;&gt;Reshaping data&lt;/h3&gt;

&lt;p&gt;Convolutional layers in Keras expect inputs with shape (num-samples, width, height, RGB-channels). So far our data has (num-samples, width, height), so we need to add the last dimension. The next chunk of code verify that data-shape and reformat accordingly.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;img_rows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_cols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;channels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# wight, height, colors
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;image_data_format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;channels_first&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_rows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_cols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_rows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_cols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;input_shape&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_rows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_cols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_rows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_cols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_rows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_cols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;input_shape&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img_rows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_cols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;x_train shape:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;x_test shape:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;x_train shape:(60000, 28, 28, 1)
x_test shape:(10000, 28, 28, 1)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;normalizing-data&quot;&gt;Normalizing data&lt;/h3&gt;

&lt;p&gt;Grayscale values range from 0 (white) to 255 (black). Neural networks tend to converge (learn) faster when data is normalized. Here, by dividing our data by 255 we get pixel values ranging from 0 (white) to 1 (black).&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;x_train range *before* normalization (max - min): &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ptp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;x_test range *before* normalization (max - min): &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ptp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;x_train range *before* normalization (max - min): 255
x_test range *before* normalization (max - min): 255
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;x_train range *after* normalization (max - min): &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ptp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;x_test range *after* normalization (max - min): &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ptp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;x_train range *after* normalization (max - min): 1.0
x_test range *after* normalization (max - min): 1.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;one-hot-encoding-target-vector&quot;&gt;One-hot encoding target vector&lt;/h3&gt;

&lt;p&gt;When we move from binary to multi-class classification, we need to change the shape of the target, in this case, from a single vector of shape (num-samples,) to a matrix of shape (num-samples, num-classes). In machine learning this coding scheme is called &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html&quot;&gt;one-hot encoding&lt;/a&gt;. In statistics is usually called &lt;a href=&quot;https://en.wikiversity.org/wiki/Dummy_variable_(statistics)&quot;&gt;dummy coding&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;y_train shape *before* one-hot encoding: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;y_test shape *before* one-hot encoding: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;y_train shape *before* one-hot encoding: (60000,)
y_test shape *before* one-hot encoding: (10000,)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Numbers 0-9 
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utils&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;to_categorical&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utils&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;to_categorical&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;y_train shape *after* one-hot encoding: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;y_test shape *after* one-hot encoding: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;y_train shape *after* one-hot encoding: (60000, 10)
y_test shape *after* one-hot encoding: (10000, 10)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;lenet-5-model-architecture-in-keras&quot;&gt;LeNet-5 model architecture in Keras&lt;/h3&gt;

&lt;p&gt;Here is where we ensemble the LeNet-5 architecture as shown (as closely as possible) in &lt;strong&gt;Figure 7&lt;/strong&gt;. The comments explain each step in the model definition.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Define a network as a linear stack of layers
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; 

&lt;span class=&quot;c1&quot;&gt;# Add 1st convolutional layer with:
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - features maps: 6
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - kernel shape: 5 x 5 
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - activation function post-convolution: hyperbolic tanget (tanh)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Conv2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filters&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;input_shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Add 1st pooling layer with kernel shape: 2 x 2 
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;AveragePooling2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pool_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Add 2st convolutional layer with:
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - features maps: 16
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - kernel shape: 5 x 5 
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - activation function post-convolution: hyperbolic tanget (tanh)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Conv2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filters&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Add 2st pooling layer with kernel shape: 2 x 2 
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;AveragePooling2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pool_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Flatten the feature maps           
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Add 1st fully-connected layer with sigmoid activation function
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;120&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Add 2st fully-connected layer with sigmoid activation function
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;84&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Add output layer with softmax activation with 10 output classes         
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The model summary shows that our architecture yields 62,006 trainable parameters.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Model: &quot;sequential_1&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 24, 24, 6)         156       
_________________________________________________________________
average_pooling2d_1 (Average (None, 12, 12, 6)         0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 8, 8, 16)          2416      
_________________________________________________________________
average_pooling2d_2 (Average (None, 4, 4, 16)          0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 256)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 120)               30840     
_________________________________________________________________
dense_2 (Dense)              (None, 84)                10164     
_________________________________________________________________
dense_3 (Dense)              (None, 10)                850       
=================================================================
Total params: 44,426
Trainable params: 44,426
Non-trainable params: 0
_________________________________________________________________
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;lenet-5-application-mnist-classification&quot;&gt;LeNet-5 Application: MNIST classification&lt;/h2&gt;

&lt;p&gt;Keras requires to compile the model before training. Here is where we add the cost function, the optimizer (learning algorithm, i.e., &lt;a href=&quot;https://arxiv.org/abs/1212.5701&quot;&gt;Adadelta&lt;/a&gt; , that is a variation of backpropagation), and the metrics to be saved.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Compile model with:
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - cost function: categorical cross-entropy
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - optimizer: Adadelta (variation of backpropagation)
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - metrics recorded: accuracy
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;losses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;categorical_crossentropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Adadelta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;accuracy&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now we are ready to train and evaluate LeNet-5 in the MNIST dataset.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# number of passes of the entire dataset
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# model iterations before a gradient upgrade
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;history&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                   &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                   &lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                   &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                   &lt;span class=&quot;n&quot;&gt;validation_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Train on 60000 samples, validate on 10000 samples
Epoch 1/12
60000/60000 [==============================] - 7s 118us/step - loss: 0.7811 - accuracy: 0.7862 - val_loss: 0.2485 - val_accuracy: 0.9314
Epoch 2/12
60000/60000 [==============================] - 7s 109us/step - loss: 0.1895 - accuracy: 0.9456 - val_loss: 0.1292 - val_accuracy: 0.9623
Epoch 3/12
60000/60000 [==============================] - 8s 138us/step - loss: 0.1141 - accuracy: 0.9675 - val_loss: 0.0864 - val_accuracy: 0.9750
Epoch 4/12
60000/60000 [==============================] - 9s 147us/step - loss: 0.0843 - accuracy: 0.9749 - val_loss: 0.0699 - val_accuracy: 0.9788
Epoch 5/12
60000/60000 [==============================] - 9s 142us/step - loss: 0.0684 - accuracy: 0.9797 - val_loss: 0.0582 - val_accuracy: 0.9809
Epoch 6/12
60000/60000 [==============================] - 9s 144us/step - loss: 0.0584 - accuracy: 0.9825 - val_loss: 0.0526 - val_accuracy: 0.9829
Epoch 7/12
60000/60000 [==============================] - 9s 143us/step - loss: 0.0514 - accuracy: 0.9846 - val_loss: 0.0459 - val_accuracy: 0.9853
Epoch 8/12
60000/60000 [==============================] - 9s 147us/step - loss: 0.0454 - accuracy: 0.9865 - val_loss: 0.0430 - val_accuracy: 0.9849
Epoch 9/12
60000/60000 [==============================] - 9s 142us/step - loss: 0.0418 - accuracy: 0.9876 - val_loss: 0.0437 - val_accuracy: 0.9851
Epoch 10/12
60000/60000 [==============================] - 8s 141us/step - loss: 0.0381 - accuracy: 0.9890 - val_loss: 0.0380 - val_accuracy: 0.9869
Epoch 11/12
60000/60000 [==============================] - 9s 142us/step - loss: 0.0351 - accuracy: 0.9894 - val_loss: 0.0414 - val_accuracy: 0.9862
Epoch 12/12
60000/60000 [==============================] - 9s 142us/step - loss: 0.0326 - accuracy: 0.9904 - val_loss: 0.0358 - val_accuracy: 0.9871
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Chart 4&lt;/strong&gt; shows the error (cost, loss) curve (red) and accuracy curve (accuracy) for each iteration.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;history&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;history&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;accuracy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;history&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;history&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;accuracy&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;df2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;accuracy&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;time-step&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))})&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;base&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mark_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;blue&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;time-step&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;accuracy&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mark_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;red&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;time-step&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;properties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Chart 4&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div id=&quot;altair-viz-b1c43f2bc4db40068f3049346360d945&quot;&gt;&lt;/div&gt;
&lt;script type=&quot;text/javascript&quot;&gt;
  (function(spec, embedOpt){
    const outputDiv = document.getElementById(&quot;altair-viz-b1c43f2bc4db40068f3049346360d945&quot;);
    const paths = {
      &quot;vega&quot;: &quot;https://cdn.jsdelivr.net/npm//vega@5?noext&quot;,
      &quot;vega-lib&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-lib?noext&quot;,
      &quot;vega-lite&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-lite@4.0.2?noext&quot;,
      &quot;vega-embed&quot;: &quot;https://cdn.jsdelivr.net/npm//vega-embed@6?noext&quot;,
    };

    function loadScript(lib) {
      return new Promise(function(resolve, reject) {
        var s = document.createElement(&apos;script&apos;);
        s.src = paths[lib];
        s.async = true;
        s.onload = () =&gt; resolve(paths[lib]);
        s.onerror = () =&gt; reject(`Error loading script: ${paths[lib]}`);
        document.getElementsByTagName(&quot;head&quot;)[0].appendChild(s);
      });
    }

    function showError(err) {
      outputDiv.innerHTML = `&lt;div class=&quot;error&quot; style=&quot;color:red;&quot;&gt;${err}&lt;/div&gt;`;
      throw err;
    }

    function displayChart(vegaEmbed) {
      vegaEmbed(outputDiv, spec, embedOpt)
        .catch(err =&gt; showError(`Javascript Error: ${err.message}&lt;br&gt;This usually means there&apos;s a typo in your chart specification. See the javascript console for the full traceback.`));
    }

    if(typeof define === &quot;function&quot; &amp;&amp; define.amd) {
      requirejs.config({paths});
      require([&quot;vega-embed&quot;], displayChart, err =&gt; showError(`Error loading script: ${err.message}`));
    } else if (typeof vegaEmbed === &quot;function&quot;) {
      displayChart(vegaEmbed);
    } else {
      loadScript(&quot;vega&quot;)
        .then(() =&gt; loadScript(&quot;vega-lite&quot;))
        .then(() =&gt; loadScript(&quot;vega-embed&quot;))
        .catch(showError)
        .then(() =&gt; displayChart(vegaEmbed));
    }
  })({&quot;config&quot;: {&quot;view&quot;: {&quot;continuousWidth&quot;: 400, &quot;continuousHeight&quot;: 300}}, &quot;layer&quot;: [{&quot;mark&quot;: {&quot;type&quot;: &quot;line&quot;, &quot;color&quot;: &quot;blue&quot;}, &quot;encoding&quot;: {&quot;x&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;time-step&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;accuracy&quot;}}}, {&quot;mark&quot;: {&quot;type&quot;: &quot;line&quot;, &quot;color&quot;: &quot;red&quot;}, &quot;encoding&quot;: {&quot;x&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;time-step&quot;}, &quot;y&quot;: {&quot;type&quot;: &quot;quantitative&quot;, &quot;field&quot;: &quot;loss&quot;}}}], &quot;data&quot;: {&quot;name&quot;: &quot;data-61663d6c03bfc2e36615b4753703363d&quot;}, &quot;title&quot;: &quot;Chart 4&quot;, &quot;$schema&quot;: &quot;https://vega.github.io/schema/vega-lite/v4.0.2.json&quot;, &quot;datasets&quot;: {&quot;data-61663d6c03bfc2e36615b4753703363d&quot;: [{&quot;accuracy&quot;: 0.7861833572387695, &quot;loss&quot;: 0.7811281068325042, &quot;time-step&quot;: 0}, {&quot;accuracy&quot;: 0.9456333518028259, &quot;loss&quot;: 0.18953756535847982, &quot;time-step&quot;: 1}, {&quot;accuracy&quot;: 0.9675333499908447, &quot;loss&quot;: 0.11406081169048946, &quot;time-step&quot;: 2}, {&quot;accuracy&quot;: 0.9749166369438171, &quot;loss&quot;: 0.08432972807884216, &quot;time-step&quot;: 3}, {&quot;accuracy&quot;: 0.9797166585922241, &quot;loss&quot;: 0.06836639445026715, &quot;time-step&quot;: 4}, {&quot;accuracy&quot;: 0.9824666380882263, &quot;loss&quot;: 0.05844920764565468, &quot;time-step&quot;: 5}, {&quot;accuracy&quot;: 0.9846166372299194, &quot;loss&quot;: 0.051383760850628214, &quot;time-step&quot;: 6}, {&quot;accuracy&quot;: 0.986466646194458, &quot;loss&quot;: 0.045413614031672475, &quot;time-step&quot;: 7}, {&quot;accuracy&quot;: 0.987583339214325, &quot;loss&quot;: 0.04184290882945061, &quot;time-step&quot;: 8}, {&quot;accuracy&quot;: 0.9890166521072388, &quot;loss&quot;: 0.03814259369770686, &quot;time-step&quot;: 9}, {&quot;accuracy&quot;: 0.9893500208854675, &quot;loss&quot;: 0.035076645305256046, &quot;time-step&quot;: 10}, {&quot;accuracy&quot;: 0.9903500080108643, &quot;loss&quot;: 0.03259256738523642, &quot;time-step&quot;: 11}]}}, {&quot;mode&quot;: &quot;vega-lite&quot;});
&lt;/script&gt;

&lt;p&gt;Our adapted version of LeNet-5 reached a training accuracy of ~99% in just 12 iterations. Let’s test LeNet-5 generalization in the 10,000 samples of the test data.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;evaluate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Test loss score: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Test accuracy score:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Test loss score: 0.035752620117459444
Test accuracy score:0.9871000051498413
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We obtained a test accuracy score of ~99%. Pretty good! Almost identical to the 99.05% of the original LeNet-5 in 1998.&lt;/p&gt;

&lt;h3 id=&quot;cifar-10-classification-with-alexnet&quot;&gt;CIFAR-10 classification with AlexNet&lt;/h3&gt;

&lt;p&gt;Implement AlexNet is more straightforward than implementing LeNet-5 because the architecture elements are in line with modern neural networks. We will follow the same steps we followed with LeNet-5 to define and test AlexNet.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keras.datasets&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cifar10&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keras.models&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keras.layers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dropout&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keras.layers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Conv2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MaxPooling2D&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keras.layers.normalization&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BatchNormalization&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keras.models&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_model&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;altair&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;reading-data-from-keras-1&quot;&gt;Reading data from Keras&lt;/h3&gt;

&lt;p&gt;The CIFAR-10 dataset comprises 50,000 32x32 color images for the training set and 10,000 for the testing set. CIFAR-10 is loaded as a (num-samples, 32, 32, 3) multidimensional array.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cifar10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;load_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# unpack data into training and test
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;x_train shape:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;x_test shape:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;x_train shape:(50000, 32, 32, 3)
x_test shape:(10000, 32, 32, 3)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-8/cifar10.png&quot; width=&quot;60%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;reshaping-data-1&quot;&gt;Reshaping data&lt;/h3&gt;

&lt;p&gt;This time no reshaping is needed as the data comes in the right format (num-samples, width, height, RBG channels).&lt;/p&gt;

&lt;h3 id=&quot;normalize-data&quot;&gt;Normalize data&lt;/h3&gt;

&lt;p&gt;Let’s normalize the data to 0-1 range again.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;x_train range *before* normalization (max - min): &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ptp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;x_test range *before* normalization (max - min): &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ptp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;x_train range *before* normalization (max - min): 255
x_test range *before* normalization (max - min): 255
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;x_train range *after* normalization (max - min): &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ptp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;x_test range *after* normalization (max - min): &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ptp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;x_train range *after* normalization (max - min): 1.0
x_test range *after* normalization (max - min): 1.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;one-hot-encoding-target-vector-1&quot;&gt;One-hot encoding target vector&lt;/h3&gt;

&lt;p&gt;We need to reshape the target vector in the same manner we did with MNIST.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;y_train shape *before* one-hot encoding: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;y_test shape *before* one-hot encoding: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;y_train shape *before* one-hot encoding: (50000, 1)
y_test shape *before* one-hot encoding: (10000, 1)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 10 object classes
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utils&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;to_categorical&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utils&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;to_categorical&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;y_train shape *after* one-hot encoding: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;y_test shape *after* one-hot encoding: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;y_train shape *after* one-hot encoding: (50000, 10)
y_test shape *after* one-hot encoding: (10000, 10)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;alexnet-model-architecture-in-keras&quot;&gt;AlexNet model architecture in Keras&lt;/h3&gt;

&lt;p&gt;Here is where we ensemble AlexNet architecture as shown (as closely as possible) in &lt;strong&gt;Figure 7&lt;/strong&gt;. The comments explain each step in the model definition.&lt;/p&gt;

&lt;p&gt;Since we are usin CIFAR-10 32x32 images instead of the 224x224 ImageNet images, “padding” will be necessary in several layers so dimensions match. Normally we will use kernels with different dimensions for CIFAR-10 but I’m opting for padding to recreate AlexNet as closely as possible.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;img_rows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_cols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;channels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# wight, height, colors
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_shape&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img_rows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_cols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Define a network as a linear stack of layers
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; 

&lt;span class=&quot;c1&quot;&gt;# Add 1st convolutional layer with:
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - features maps (filters): 96
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - kernel shape: 11 x 11 
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - activation function post-convolution: rectifier linear unit (relu)
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - stride size: 4 x 4
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Conv2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filters&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;96&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;strides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;input_shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Batch normalisation in between layers
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;BatchNormalization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Add 1st pooling layer with 
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - kernel shape: 3 x 3
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - stride size: 2 x 2
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;MaxPooling2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pool_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;strides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Add 2nd convolutional layer with:
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - features maps: 256
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - kernel shape: 5 x 5 
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - activation function post-convolution: rectifier linear unit (relu)
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - stride size: 1 x 1
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Conv2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filters&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;strides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;same&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Batch normalisation in between layers
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;BatchNormalization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Add 1st pooling layer with 
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - kernel shape: 3 x 3
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - stride size: 2 x 2
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;MaxPooling2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pool_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;strides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;same&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Add 3rd convolutional layer with:
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - features maps: 384
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - kernel shape: 3 x 3 
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - activation function post-convolution: rectifier linear unit (relu)
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - stride size: 1 x 1
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Conv2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filters&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;384&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;strides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;same&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Add 4th convolutional layer with:
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - features maps: 384
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - kernel shape: 3 x 3 
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - activation function post-convolution: rectifier linear unit (relu)
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - stride size: 1 x 1
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Conv2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filters&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;384&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;strides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;same&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Add 5th convolutional layer with:
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - features maps: 384
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - kernel shape: 3 x 3 
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - activation function post-convolution: rectifier linear unit (relu)
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - stride size: 1 x 1
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Conv2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filters&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;384&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;strides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                 &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;same&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Add 3th pooling layer with 
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - kernel shape: 3 x 3
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - stride size: 2 x 2
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;MaxPooling2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pool_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;strides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;same&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Flatten feature maps           
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Add 1st fully-connected layer with relu activation function
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4096&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Add dropout to help with generalization
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Add 2st fully-connected layer with relu activation function
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4096&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Add dropout to help with generalization
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Add output layer with softmax activation with 10 output classes         
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The model summary below shows that out network design yields ~22.5 million trainable parameters, which is massive compared to LeNet-5.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Model: &quot;sequential_2&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_3 (Conv2D)            (None, 6, 6, 96)          34944     
_________________________________________________________________
batch_normalization_1 (Batch (None, 6, 6, 96)          384       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 2, 2, 96)          0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 2, 2, 256)         614656    
_________________________________________________________________
batch_normalization_2 (Batch (None, 2, 2, 256)         1024      
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 1, 1, 256)         0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 1, 1, 384)         885120    
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 1, 1, 384)         1327488   
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 1, 1, 384)         1327488   
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 1, 1, 384)         0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 384)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 4096)              1576960   
_________________________________________________________________
dropout_1 (Dropout)          (None, 4096)              0         
_________________________________________________________________
dense_5 (Dense)              (None, 4096)              16781312  
_________________________________________________________________
dropout_2 (Dropout)          (None, 4096)              0         
_________________________________________________________________
dense_6 (Dense)              (None, 10)                40970     
=================================================================
Total params: 22,590,346
Trainable params: 22,589,642
Non-trainable params: 704
_________________________________________________________________
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;alexnet-application-cifar-10-classification&quot;&gt;AlexNet Application: CIFAR-10 classification&lt;/h2&gt;

&lt;p&gt;Which such a large number of parameters training our model to a meaningful accuracy level in a typical CPU may take from hours to days. For instance, my &lt;a href=&quot;https://ark.intel.com/content/www/us/en/ark/products/122589/intel-core-i7-8550u-processor-8m-cache-up-to-4-00-ghz.html&quot;&gt;Intel i7-8550U&lt;/a&gt; takes around 14 minutes to run 1 epoch. Running 50 epochs would take around 12 hours. There are several alternatives to train large models:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Locally&lt;/strong&gt;: if you have a computer with one or multiple &lt;a href=&quot;https://lambdalabs.com/blog/choosing-a-gpu-for-deep-learning/&quot;&gt;state-of-the-art GPUs&lt;/a&gt;, you could use that and the training of our model should be pretty fast.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;High throughput computing (HTC)&lt;/strong&gt;: if you have access to a grid of computers (and know how to use it), you could take our code, repackage it, and use that. Most major universities have HTC or HPC clusters available for students. HTC computing is ideal for training neural networks since trained can be parallelized in multiple cores, significantly speeding up training time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Remotely (cloud)&lt;/strong&gt;: you can use either someone else server via an SSH connection or cloud services as &lt;a href=&quot;https://aws.amazon.com/&quot;&gt;AWS&lt;/a&gt;, &lt;a href=&quot;https://cloud.google.com/&quot;&gt;Google Cloud&lt;/a&gt;, and &lt;a href=&quot;https://azure.microsoft.com/en-us/&quot;&gt;Microsoft Azure&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Google Colab&lt;/strong&gt;: &lt;a href=&quot;https://colab.research.google.com/&quot;&gt;Google Colab&lt;/a&gt; is a remote interactive computing environment (basically a Jupyter Notebook in a google computer) typically used to train machine learning models. It grants you free access to 1 GPU with a time limit (12 or 24 hours).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Transfer learning&lt;/strong&gt;: recall that training a network essentially is to learn weight values. If someone else trained AlexNet in CIFAR-10, such a person can save the weights learned by the network, and we could re-use them for our porpuses. For popular models and datasets, most deep learning libraries provide pre-trained models.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For a demo like ours, Google Colab is the best option. I copy-pasted AlexNet into a Colab notebook that you can &lt;a href=&quot;https://colab.research.google.com/drive/1CJhNrMVEKLOTlq-eYwAENZjlJbGdGe4o#forceEdit=true&amp;amp;sandboxMode=true&quot;&gt;access here&lt;/a&gt;. I also saved the model such that we can reuse the trained parameters to make predictions in our test set. If you wanna go through the process of training AlexNet yourself, you can use the Colab Notebook and run the cells (in “Playground mode”). When I trained AlexNet with CIFAR-10 each epoch took a bit more than 1 minute to run, so training the model for 50 epochs should take about 1 hour.&lt;/p&gt;

&lt;p&gt;Uncomment and run the cells below only if you have access to adequate hardware (otherwise it will run for hours).&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Compile model with:
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - cost function: categorical cross-entropy
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - optimizer: Adadelta (variation of backpropagation)
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# - metrics recorded: accuracy
# model.compile(loss=keras.losses.categorical_crossentropy,
#               optimizer=keras.optimizers.Adadelta(),
#               metrics=[&apos;accuracy&apos;])
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# epochs = 50 # number of passes of the entire dataset
# batch_size = 32 # model iterations before a gradient upgrade
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#
# model.fit(x_train, y_train,
#           batch_size=batch_size,
#           epochs=epochs,
#           verbose=1,
#           validation_data=(x_test, y_test))
#
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# score = model.evaluate(x_test, y_test, verbose=0)
# print(f&apos;Test loss score: {score[0]}&apos;)
# print(f&apos;Test accuracy score:{ score[1]}&apos;)
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;From my 50 epochs Google Colab run with  I obtained a training accuracy score of ~77%. &lt;strong&gt;Chart 4&lt;/strong&gt; shows the loss (red) and accuracy (blue) trajectory over iterations.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-8/alexnet-training.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s load the trained model to use on the test set. We first need to &lt;strong&gt;download the model&lt;/strong&gt; I put on Google Drive by running the cell below. This may take a couple of minutes depending on your internet speed.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;err&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gdown&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mMiujUTQQkQgU0CjAwVQN69GUSmHjxNo&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alexnet&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cifar10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h5&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Downloading...
From: https://drive.google.com/uc?id=1mMiujUTQQkQgU0CjAwVQN69GUSmHjxNo
To: /home/pablo/Desktop/projects/nn-mod-cog/notebooks/alexnet-cifar10.h5
271MB [06:14, 723kB/s]  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The model will be saved locally as “alexnet-cifar10.h5”. Keras can easily import h5 files with the load_model method.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# returns previously trained AlexNet with CIFAR-10
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alexnet&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;load_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;alexnet-cifar10.h5&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now we can compute the test score accuracy as we did before.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alexnet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;evaluate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Test loss score: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Test accuracy score:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Test loss score: 2.4020407371520998
Test accuracy score:0.5548999905586243
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We obtain &lt;strong&gt;~55% of test accuracy&lt;/strong&gt; which is significantly below the training accuracy. This is an example of &lt;strong&gt;overfitting&lt;/strong&gt; or &lt;strong&gt;overtraining&lt;/strong&gt;. There are multiple ways to address overfitting which I won’t cover here (more normalization, dropout, early stopping, etc). It may be also the case we need a &lt;strong&gt;different architecture&lt;/strong&gt; or simply &lt;strong&gt;more training time&lt;/strong&gt;. More training is a counterintuitive suggestion: “How is that more training would help if “too much training” was causing the problem in the first place?” Turns out that sometimes neural networks shown a phenomenon known as &lt;a href=&quot;https://openai.com/blog/deep-double-descent/&quot;&gt;“double descent”&lt;/a&gt;: performance improves, then get worse, and then &lt;em&gt;improves again&lt;/em&gt;. if you want to learn more about double descent see &lt;a href=&quot;https://openai.com/blog/deep-double-descent/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Is recommended to delete the h5 file once you are done by running:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;err&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rm&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alexnet&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cifar10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h5&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;limitations&quot;&gt;Limitations&lt;/h2&gt;

&lt;h3 id=&quot;covnets-learn-fragile-representations&quot;&gt;CovNets learn “fragile”(?) representations&lt;/h3&gt;

&lt;p&gt;There is a funny phenomenon that brings elation to neural network critics: convolutional networks can be easily “fooled” by minuscule alterations to input images. Change a couple of pixels here and there to the picture of a “panda”, and the network will predict “gibbon” with embarrassingly high confidence. From a cognitive science perspective, this is admittedly concerning, since such alterations are barely noticeable for humans, as illustrated by Goodfellow, Shlens, and Szegedy (2015) in &lt;strong&gt;Figure 14&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 14&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post-8/panda.png&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;An important detail is that the perturbations (the image in the middle), although look random, are not random at all: &lt;strong&gt;they are carefully “designed” to “fool” the network&lt;/strong&gt;. This carefully designed alterations are known as &lt;strong&gt;adversarial attacks&lt;/strong&gt; or &lt;strong&gt;adversarial examples&lt;/strong&gt;. By “designed” I don’t mean handcrafted by humans but by another type of network known as Generative Adversarial Network (GAN). GANs are trained to generate images just “different enough” to fool your classifier network.&lt;/p&gt;

&lt;p&gt;Many explanations have been proposed to explain why adversarial examples exist, for instance:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;HA! Neural networks are rubbish&lt;/li&gt;
  &lt;li&gt;Neural networks pay attention to &lt;a href=&quot;https://openreview.net/forum?id=Bygh9j09KX&quot;&gt;meaningless fetures like texture&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Networks are &lt;a href=&quot;https://arxiv.org/pdf/1412.6572.pdf&quot;&gt;too linear&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;They are a consequence of the &lt;a href=&quot;https://arxiv.org/abs/1801.02774&quot;&gt;high-dimensional of the input-space&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;They are a consequence of the &lt;a href=&quot;https://arxiv.org/abs/1608.07690&quot;&gt;finite-sample phenomena&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Actually, adversarial examples &lt;a href=&quot;https://arxiv.org/abs/1905.02175&quot;&gt;are not bugs, but features&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The first five explanations are a problem for the idea that convolutional networks are a good conceptual model of human perception. They all indicate that something important is missing in our models that deviate from human perceptual processes, challenging their utility. Some point out to this kind of problem to suggest that there is something &lt;a href=&quot;https://arxiv.org/abs/1801.00631&quot;&gt;so fundamentally different between human and neural nets perception&lt;/a&gt; that we should either abandon or significantly changed neural nets to be useful models.&lt;/p&gt;

&lt;p&gt;The last explanation challenges the first five. It is proposing that there is nothing wrong with neural networks. In brief, the idea is that what human perceives as “meaningless noise”, in fact, is a relatively robust pattern which is useful to learn to make predictions. In other words, networks “see” things humans are missing, which are &lt;em&gt;features&lt;/em&gt; as useful as “tails” and “legs” for prediction &lt;em&gt;in the training dataset&lt;/em&gt;. True, this does not fully address the differences between machine and human perception, but it does contradict the idea neural networks are paying attention to useless features: they do pay attention to useful features, as humans do.&lt;/p&gt;

&lt;h3 id=&quot;convolutional-networks-contain-many-unrealistic-elements&quot;&gt;Convolutional networks contain many “unrealistic” elements&lt;/h3&gt;

&lt;p&gt;Convolutional networks incorporate some &lt;strong&gt;design elements that have no basis in human vision or perception&lt;/strong&gt;. Of course, this is only a problem if you are trying to emulate human vision and perception as closely as possible. Among them:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The human eye is mostly a &lt;a href=&quot;https://en.wikipedia.org/wiki/Visual_acuity&quot;&gt;low-resolution sensor&lt;/a&gt; whereas most convolutional nets are trained with large high-resolution images.&lt;/li&gt;
  &lt;li&gt;Human vision works by “sampling” or attending to &lt;a href=&quot;https://en.wikipedia.org/wiki/Saccade&quot;&gt;a relatively small section of a scene at the time&lt;/a&gt;. Convolutional nets process the whole image at once.&lt;/li&gt;
  &lt;li&gt;Human vision does not work in isolation, it is &lt;a href=&quot;https://en.wikipedia.org/wiki/Multisensory_integration&quot;&gt;integrated with other senses&lt;/a&gt;, including feedback from higher-order cognitive functions. Most convolutional nets do one thing at the time in isolation: image recognition, image segmentation, etc.&lt;/li&gt;
  &lt;li&gt;It is not clear that the brain implements convolution and pooling (or backpropagation for that matter, although &lt;a href=&quot;https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(19)30012-9&quot;&gt;recent research have proposed&lt;/a&gt; how backpropagation may work in the brain).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All the previously mentioned issues are actives areas of research.&lt;/p&gt;

&lt;h3 id=&quot;massive-data-and-computational-requirements&quot;&gt;Massive data and computational requirements&lt;/h3&gt;

&lt;p&gt;As I mentioned &lt;a href=&quot;https://pabloinsente.github.io/the-multilayer-perceptron&quot;&gt;in previous blog-posts&lt;/a&gt;, neural networks, in general, require enormous amounts of data to train effectively, which becomes even more obvious in the case of convolutional networks and images. I won’t extend myself about this, just remember that processing capacity, past learning experience, and the richness of training data may partially explain the difference between human and neural networks’ speed of learning.&lt;/p&gt;

&lt;p&gt;Regarding computational requirements, several techniques are available to reduce computing time which is mostly derived from training kernel weights with backpropagation: random kernel initialization, design kernels by hand, and learn the kernels with unsupervised learning techniques. See &lt;a href=&quot;https://www.deeplearningbook.org/contents/convnets.html&quot;&gt;Section 9.9&lt;/a&gt; from Goodfello, Bengio, &amp;amp; Courville (2016) for a review of such strategies.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;Limitations aside, convolutional networks are among the best examples of connecting cognitive neuroscience with artificial neural networks. Convolutional nets &lt;strong&gt;core design principle&lt;/strong&gt; comes from classic neuroscience research: hierarchically organized layers of simple cells and complex cells acting together to build complex representations of objects. Perhaps the most interesting aspect of convolutional nets regarding human cognition, is they are by far the most successful model in terms of emulating human &lt;strong&gt;performance&lt;/strong&gt; (emphasis in performance, not processes) in perceptual task. Further, the interplay between human vision and perception and neural nets is an active area of research that is trying to address the many limitations of this approach.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;Fukushima, K. (1980). Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological Cybernetics, 36(4), 193–202.&lt;/p&gt;

&lt;p&gt;Goodfellow, I. J. (2010). Technical report: Multidimensional, downsampled convolution for autoencoders. Technical report, Université de Montréal. 357.&lt;/p&gt;

&lt;p&gt;Goodfellow, I., Bengio, Y., &amp;amp; Courville, A. (2016). 7. Regularization for Deep Learning. In Deep Learning. MIT Press. https://www.deeplearningbook.org/contents/regularization.html&lt;/p&gt;

&lt;p&gt;Goodfellow, I., Bengio, Y., &amp;amp; Courville, A. (2016). 9. Convolutional Networks. In Deep Learning. MIT Press. https://www.deeplearningbook.org/contents/convnets.html&lt;/p&gt;

&lt;p&gt;Goodfellow, I. J., Shlens, J., &amp;amp; Szegedy, C. (2014). Explaining and harnessing adversarial examples. ArXiv Preprint ArXiv:1412.6572.&lt;/p&gt;

&lt;p&gt;Hubel, D. H., &amp;amp; Wiesel, T. N. (1962). Receptive fields, binocular interaction and functional architecture in the cat’s visual cortex. The Journal of Physiology, 160(1), 106–154.&lt;/p&gt;

&lt;p&gt;Ilyas, A., Santurkar, S., Tsipras, D., Engstrom, L., Tran, B., &amp;amp; Madry, A. (2019). Adversarial examples are not bugs, they are features. Advances in Neural Information Processing Systems, 125–136.&lt;/p&gt;

&lt;p&gt;Krizhevsky, A., Sutskever, I., &amp;amp; Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. 1097–1105.&lt;/p&gt;

&lt;p&gt;LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., &amp;amp; Jackel, L. D. (1989). Backpropagation applied to handwritten zip code recognition. Neural Computation, 1(4), 541–551.&lt;/p&gt;

&lt;p&gt;LeCun, Y., Bottou, L., Bengio, Y., &amp;amp; Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278–2324.&lt;/p&gt;

&lt;p&gt;Lindsay, G. (2020). Convolutional Neural Networks as a Model of the Visual System: Past, Present, and Future. Journal of Cognitive Neuroscience, 1–15.&lt;/p&gt;

&lt;p&gt;Rosenblatt, F. (1958). The perceptron: A probabilistic model for information storage and organization in the brain. Psychological Review, 65(6), 386–408. https://doi.org/10.1037/h0042519&lt;/p&gt;

&lt;p&gt;Zhang, A., Lipton, Z. C., Li, M., &amp;amp; Smola, A. J. (2020). 6. Convolutional Neural Networks. In Dive into Deep Learning. https://d2l.ai/chapter_convolutional-neural-networks/index.html&lt;/p&gt;

&lt;h2 id=&quot;useful-on-line-resources&quot;&gt;Useful on-line resources&lt;/h2&gt;

&lt;p&gt;The internet is plenty of free great resources about convolutional networks. I  used Yann LeCun’s interviews for the historical section.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Yann LeCun: Deep Learning, Convolutional Neural Networks, and Self-Supervised Learning. &lt;a href=&quot;https://www.youtube.com/watch?v=SGSOCuByo24&amp;amp;t=16s&quot;&gt;YouTube Video&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;deeplearning.ai’s Heroes of Deep Learning: Yann LeCun. &lt;a href=&quot;https://www.youtube.com/watch?v=JS12eb1cTLE&quot;&gt;YouTube Video&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Yann LeCun’s Lecture: Convolutional neural networks. &lt;a href=&quot;https://www.youtube.com/watch?v=FW5gFiJb-ig&amp;amp;t=2s&quot;&gt;YouTube Video&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Practicum by Alfredo Canziani &amp;amp; Mark Goldstein: Natural signals properties and CNNs. &lt;a href=&quot;https://www.youtube.com/watch?v=kwPWpVverkw&quot;&gt;YouTube Video&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Practicum by Alfredo Canziani &amp;amp; Mark Goldstein: Listening to convolutions. &lt;a href=&quot;https://www.youtube.com/watch?v=OrBEon3VlQg&quot;&gt;YouTube Video&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;MIT 6.S191 (2019): Convolutional Neural Networks. &lt;a href=&quot;https://www.youtube.com/watch?v=H-HVZJ7kGI0&quot;&gt;YouTube Video&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 02 Apr 2020 00:00:00 +0800</pubDate>
        <link>//the-convolutional-network</link>
        <link href="/the-convolutional-network"/>
        <guid isPermaLink="true">/the-convolutional-network</guid>
      </item>
    
  </channel>
</rss>
